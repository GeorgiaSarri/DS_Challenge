{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "- [Import Libraries](#libs)\n",
    "- [Read/Preprocess Text](#prep)\n",
    "- [Greek pretrained skipgram embeddings](#embeds)\n",
    "- [CNN](#cnn)\n",
    "- [Feature Engineering](#fe)\n",
    " - [tf-idf](#tfidf)\n",
    " - [lsa/lsi with tsvd](#tsvd)\n",
    " - [bm25](#bm25)\n",
    "- [MLP/XGB/SVM on generated features](#mlp)\n",
    "- [Graph Kernel-Weisfeiler-Lehman](#gk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries <a id='libs'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 1.12.0\n",
      "Keras Version: 2.2.4\n",
      "\n",
      "Python 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "Gensim 3.7.3\n",
      "Seaborn 0.9.0\n",
      "Pandas 0.24.1\n",
      "Scikit-Learn 0.20.2\n",
      "Your CPU is: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\n",
      "Your CPU has 8 cores.\n",
      "Total memory: 15.9GiB\n",
      "svmem(total=17034620928, available=7013130240, percent=58.8, used=10021490688, free=7013130240)\n",
      "Installed GPUs:\n",
      "/device:GPU:0 - device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download el_core_news_sm\n",
    "# !python -m spacy download el_core_news_md\n",
    "from __future__ import print_function\n",
    "print(__doc__)\n",
    "import re\n",
    "\n",
    "import el_core_news_md\n",
    "nlp = el_core_news_md.load()\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.max_length = 93621305\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv \n",
    "import gc\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import Dropout,Dense,Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import multiprocessing\n",
    "import platform\n",
    "import psutil\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from random import randint\n",
    "from gensim.models import Word2Vec\n",
    "sys.path.insert(0,'C:/Users/User/Desktop/DS_challenge/')\n",
    "from GraphEmbedding.ge import DeepWalk\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
    "from grakel import Graph\n",
    "from grakel import GraphKernel\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import sentence_polarity\n",
    "# nltk.download('sentence_polarity') \n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Gensim {gensim.__version__}\")\n",
    "print(f\"Seaborn {sns.__version__}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "p = platform.processor()\n",
    "mem = psutil.virtual_memory()\n",
    "\n",
    "print(f'Your CPU is: {p}')\n",
    "print(f'Your CPU has {multiprocessing.cpu_count()} cores.')\n",
    "print(f'Total memory: {sizeof_fmt(mem.total)}')\n",
    "print(mem)\n",
    "\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "gpuList = [x for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(\"Installed GPUs:\")\n",
    "for x in gpuList:\n",
    "    print(\"{} - {}\".format(x.name,x.physical_device_desc))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Preprocess <a id='prep'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r'C:\\Users\\User\\Desktop\\DS_challenge\\data\\\\'\n",
    "TRAIN_FILE = dir_path + \"train.csv\"\n",
    "TEST_FILE = dir_path + \"test.csv\"\n",
    "GRAPH_FILE = dir_path + \"edgelist.txt\"\n",
    "domains_path = 'C:/Users/User/Desktop/DS_challenge/data/data_science_challenge_2019/domains/'\n",
    "\n",
    "class_labels = {'athlitismos':0, 'diaskedasi-psyxagogia':1, 'eidiseis-mme':2, 'katastimata-agores':3, 'pliroforiki-diadiktyo':4}\n",
    "\n",
    "train_hosts = list()\n",
    "train_labels = list()\n",
    "with open(TRAIN_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        train_hosts.append(l[0])\n",
    "        train_labels.append(l[1][:-1])\n",
    "\n",
    "y_train_class = []\n",
    "y_train_dum = np.zeros((len(train_hosts), len(class_labels)))\n",
    "for i, train_label in enumerate(train_labels):\n",
    "    y_train_dum[i,class_labels[train_label]] = 1\n",
    "    y_train_class.append(class_labels[train_label])\n",
    "\n",
    "test_hosts = list()\n",
    "with open(TEST_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        test_hosts.append(l[0])\n",
    "\n",
    "text = dict()\n",
    "filenames = os.listdir(domains_path)\n",
    "for filename in filenames:\n",
    "    if filename[-4:] == '.zip':\n",
    "        z = zipfile.ZipFile(domains_path + filename)\n",
    "        contents = z.namelist()\n",
    "        #text[filename[:-4]] = [] \n",
    "        text_list = []\n",
    "        for c in contents:\n",
    "            f = z.open(c)\n",
    "            data = f.read()\n",
    "            text_list.append(data.decode('utf16')) #for windows: latin1\n",
    "            f.close()\n",
    "        text[filename[:-4]] = text_list\n",
    "            \n",
    "# Read webpages of the training set\n",
    "train_data = list()\n",
    "for host in train_hosts:\n",
    "    if host in text:\n",
    "        train_data.append(text[host])\n",
    "    else:\n",
    "        train_data.append('')\n",
    "\n",
    "# Read webpages of the test set\n",
    "test_data = list()\n",
    "for host in test_hosts:\n",
    "    if host in text:\n",
    "        test_data.append(text[host])\n",
    "    else:\n",
    "        test_data.append('')\n",
    "        \n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess all the texts.\n",
    "- remove special symbols letters\n",
    "- transform to lower case\n",
    "- remove links etc.\n",
    "- lemmatize the tokens as an option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, lemma=True):\n",
    "    string = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"[^άέήίόύώα-ωa-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if lemma==True:\n",
    "        string = nlp(string)\n",
    "        string = [token.lemma_ for token in string ] #if ('\\n' not in token.lemma_)]\n",
    "    else:\n",
    "        string = string.strip().split()        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this preprocessing function to each document and we store the results to pickle objects so we do not have to rerun it each time.\n",
    "<br>\n",
    "We create 3 different types of tokenized documents.\n",
    "- 5000 lemmatized tokens equally sampled from or urls of a website(in order to use it in CNNs)\n",
    "- full text lemmatized tokens\n",
    "- full text without lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:20<00:00, 38.35it/s]\n",
      "100%|██████████| 200/200 [00:06<00:00, 30.44it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "serialized_docs_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/'\n",
    "\n",
    "max_size = 5000\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    num_urls = len(train_data[i])       \n",
    "    \n",
    "    doc_clean = []\n",
    "    doc_concat = []\n",
    "    if num_urls != 0:\n",
    "        num_tokens_per_url = int(max_size/num_urls)\n",
    "        \n",
    "        for url in train_data[i]:\n",
    "            doc_clean.append(clean_str(url, lemma=False))\n",
    "            \n",
    "        remain_tokens = 0\n",
    "        max_doc_len_id = -1\n",
    "        max_length = 0\n",
    "        for idx, d in enumerate(doc_clean):  \n",
    "            if len(d)>max_length:\n",
    "                max_length = len(d)\n",
    "                max_doc_len_id = idx\n",
    "                \n",
    "            if len(d) > (num_tokens_per_url + remain_tokens):\n",
    "                n_tok = remain_tokens + num_tokens_per_url\n",
    "                doc_concat += d[:n_tok]\n",
    "                remain_tokens = 0\n",
    "            else:\n",
    "                remain_tokens += (num_tokens_per_url - len(d))\n",
    "                doc_concat += d\n",
    "        try:\n",
    "            more_tokens = (max_size - len(doc_concat))\n",
    "            doc_concat += doc_clean[max_doc_len_id][-more_tokens:]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    # All tokens\n",
    "    doc_dict = {\n",
    "        'y': y_train_dum[i],\n",
    "        'x_list': doc_concat\n",
    "    } \n",
    "    pickle_out = open(serialized_docs_dir + 'train_mix/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    pickle.dump(doc_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "for i in tqdm(range(len(test_data))):\n",
    "    num_urls = len(test_data[i])       \n",
    "    \n",
    "    doc_clean = []\n",
    "    doc_concat = []\n",
    "    if num_urls != 0:\n",
    "        num_tokens_per_url = int(max_size/num_urls)\n",
    "        \n",
    "        for url in test_data[i]:\n",
    "            doc_clean.append(clean_str(url, lemma=False))\n",
    "        \n",
    "        remain_tokens = 0\n",
    "        max_doc_len_id = -1\n",
    "        max_length = 0\n",
    "        for idx, d in enumerate(doc_clean):  \n",
    "            if len(d)>max_length:\n",
    "                max_length = len(d)\n",
    "                max_doc_len_id = idx\n",
    "                \n",
    "            if (len(d)) > (num_tokens_per_url + remain_tokens):\n",
    "                n_tok = remain_tokens + num_tokens_per_url\n",
    "                doc_concat += d[:n_tok]\n",
    "                remain_tokens = 0\n",
    "            else:\n",
    "                remain_tokens += (num_tokens_per_url - len(d))\n",
    "                doc_concat += d\n",
    "        try:\n",
    "            more_tokens = (max_size - len(doc_concat))\n",
    "            doc_concat += doc_clean[max_doc_len_id][-more_tokens:]            \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    # All tokens\n",
    "    doc_dict = {        \n",
    "        'x_list': doc_concat\n",
    "    } \n",
    "    pickle_out = open(serialized_docs_dir + 'test_mix/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    pickle.dump(doc_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "for i in tqdm(range(len(train_data))):    \n",
    "    doc_concat = []\n",
    "    for url in train_data[i]:\n",
    "        doc_concat += clean_str(url, lemma=False)\n",
    "    #d_lemma = clean_str(train_data[i], lemma=True)  \n",
    "    \n",
    "    # All tokens\n",
    "    doc_dict = {\n",
    "        'y': y_train_dum[i],\n",
    "        'x_list': doc_concat\n",
    "    } \n",
    "    pickle_out = open(serialized_docs_dir + 'train_all/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    pickle.dump(doc_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    # All tokens lemma\n",
    "    #doc_dict_lemma = {\n",
    "    #    'y': y_train[i],\n",
    "    #    'x_list': d_lemma\n",
    "    #} \n",
    "    #pickle_out = open(serialized_docs_dir + 'train_all_lemma/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    #pickle.dump(doc_dict_lemma, pickle_out)\n",
    "    #pickle_out.close()\n",
    "    \n",
    "    # First 5000 tokens no lemma\n",
    "    #if len(d_lemma) > 5000:\n",
    "    #    doc_dict_lemma_5000 = {\n",
    "    #        'y': y_train[i],\n",
    "    #        'x_list': d_lemma[:5000]\n",
    "    #    } \n",
    "    #else:\n",
    "    #    doc_dict_lemma_5000 = {\n",
    "    #        'y': y_train[i],\n",
    "    #        'x_list': d_lemma\n",
    "    #    }    \n",
    "    \n",
    "    #pickle_out = open(serialized_docs_dir + 'train/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    #pickle.dump(doc_dict_lemma_5000, pickle_out)\n",
    "    #pickle_out.close()\n",
    "\n",
    "# Same for the test documents\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    doc_concat = []\n",
    "    for url in test_data[i]:\n",
    "        doc_concat += clean_str(url, lemma=False)\n",
    "    #d_lemma = clean_str(train_data[i], lemma=True)  \n",
    "    \n",
    "    # All tokens\n",
    "    doc_dict = {        \n",
    "        'x_list': doc_concat\n",
    "    } \n",
    "    \n",
    "    pickle_out = open(serialized_docs_dir + 'test_all/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    pickle.dump(doc_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    #doc_dict_lemma = {\n",
    "    #    'x_list': d_lemma\n",
    "    #} \n",
    "#     pickle_out = open(serialized_docs_dir + 'test_all_lemma/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "#     pickle.dump(doc_dict_lemma, pickle_out)\n",
    "#     pickle_out.close()\n",
    "    \n",
    "#     if len(d_lemma) > 5000:\n",
    "#         doc_dict_lemma_5000 = {            \n",
    "#             'x_list': d_lemma[:5000]\n",
    "#         } \n",
    "#     else:\n",
    "#         doc_dict_lemma_5000 = {            \n",
    "#             'x_list': d_lemma\n",
    "#         }    \n",
    "    \n",
    "#     pickle_out = open(serialized_docs_dir + 'test/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "#     pickle.dump(doc_dict_lemma_5000, pickle_out)\n",
    "#     pickle_out.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load serialized proccessed documents that we create in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_docs_train_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/train_mix/'\n",
    "filenames = os.listdir(serialized_docs_train_dir)\n",
    "\n",
    "train_data = []\n",
    "#y_train = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_train_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        train_data.append(doc['x_list'])               \n",
    "        \n",
    "serialized_docs_test_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/test_mix/'\n",
    "filenames = os.listdir(serialized_docs_test_dir)\n",
    "\n",
    "test_data = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_test_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        test_data.append(doc['x_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 160580\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(train_docs, test_docs):\n",
    "    vocab = dict()\n",
    "\n",
    "    for doc in train_docs:\n",
    "        for word in doc:            \n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "                \n",
    "    for doc in test_docs:\n",
    "        for word in doc:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "\n",
    "    return vocab\n",
    " \n",
    "vocab = get_vocab(train_data, test_data)\n",
    "print(\"Vocab size\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings <a id='embeds'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query only the embeddings of the words that are in our vocabulary and store them to pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in pretrained set: 150810\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def load_embeddings(fname, vocab):\n",
    "    embeddings = np.zeros((len(vocab)+1, 300))\n",
    "    \n",
    "    c = 0\n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "    for word in vocab:\n",
    "        if word in model:\n",
    "            c += 1\n",
    "            embeddings[vocab[word]] = model[word]\n",
    "        else:\n",
    "            embeddings[vocab[word]] = np.random.uniform(-0.25, 0.25, 300)\n",
    "    print(\"Words in pretrained set:\", c)\n",
    "    return embeddings\n",
    "\n",
    "path_to_embeddings = r'C:\\Users\\User\\Desktop\\DS_challenge\\practical_session\\grcorpus_def.vec'\n",
    "embeddings = load_embeddings(path_to_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_emb_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/cnn_emb_tokens/'\n",
    "# Save Embeddings\n",
    "'''\n",
    "pickle_out = open(serialized_emb_dir + 'embeddings',\"wb\")\n",
    "pickle.dump(embeddings, pickle_out)\n",
    "pickle_out.close()\n",
    "'''\n",
    "# Load Embedding\n",
    "with open(serialized_emb_dir + 'embeddings', 'rb') as file:\n",
    "    embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match tokens with vocaculary keys/indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix:  (800, 5000)\n",
      "Size of testing matrix:  (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "max_size = 5000\n",
    "X_train_CNN = np.zeros((800, max_size), dtype=np.int64)\n",
    "\n",
    "for i, d in enumerate(train_data):\n",
    "    for j, term in enumerate(d):    \n",
    "        try:\n",
    "            X_train_CNN[i,j] = vocab[term]\n",
    "        except:\n",
    "            pass\n",
    "       \n",
    "        \n",
    "print(\"Size of training matrix: \", X_train_CNN.shape)\n",
    "\n",
    "X_test_CNN = np.zeros((200, max_size), dtype=np.int64)\n",
    "for i, d in enumerate(test_data):\n",
    "    for j, term in enumerate(d):\n",
    "        try:\n",
    "            X_test_CNN[i,j] = vocab[term]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print(\"Size of testing matrix: \", X_test_CNN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network (CNN) <a id='cnn'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_indicies = list(range(0, 800))\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_indicies, y_train_class, train_size=0.87, random_state=43, stratify=y_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra_CNN = np.take(X_train_CNN, X_tra, axis=0)\n",
    "X_val_CNN = np.take(X_train_CNN, X_val, axis=0)\n",
    "\n",
    "y_tra_CNN = np.take(y_train_dum ,X_tra, axis=0)\n",
    "y_val_CNN = np.take(y_train_dum ,X_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 5000, 300)    48174300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 4998, 100)    90100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 4997, 100)    120100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            1005        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 48,385,505\n",
      "Trainable params: 211,205\n",
      "Non-trainable params: 48,174,300\n",
      "__________________________________________________________________________________________________\n",
      "Train on 696 samples, validate on 104 samples\n",
      "Epoch 1/50\n",
      "696/696 [==============================] - ETA: 50s - loss: 2.2820 - acc: 0.26 - ETA: 25s - loss: 2.2303 - acc: 0.24 - ETA: 16s - loss: 2.2151 - acc: 0.22 - ETA: 11s - loss: 2.2280 - acc: 0.21 - ETA: 8s - loss: 2.1518 - acc: 0.2125 - ETA: 6s - loss: 2.0733 - acc: 0.221 - ETA: 4s - loss: 2.0499 - acc: 0.214 - ETA: 3s - loss: 1.9987 - acc: 0.228 - ETA: 1s - loss: 1.9972 - acc: 0.232 - ETA: 0s - loss: 1.9961 - acc: 0.225 - 12s 18ms/step - loss: 1.9675 - acc: 0.2284 - val_loss: 1.4033 - val_acc: 0.3654\n",
      "Epoch 2/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 1.4797 - acc: 0.359 - ETA: 4s - loss: 1.5667 - acc: 0.304 - ETA: 4s - loss: 1.5756 - acc: 0.312 - ETA: 3s - loss: 1.5315 - acc: 0.335 - ETA: 3s - loss: 1.5318 - acc: 0.340 - ETA: 2s - loss: 1.5340 - acc: 0.333 - ETA: 2s - loss: 1.5638 - acc: 0.341 - ETA: 1s - loss: 1.5634 - acc: 0.349 - ETA: 0s - loss: 1.5517 - acc: 0.354 - ETA: 0s - loss: 1.5283 - acc: 0.362 - 6s 9ms/step - loss: 1.5094 - acc: 0.3678 - val_loss: 1.1975 - val_acc: 0.6058\n",
      "Epoch 3/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 1.3640 - acc: 0.406 - ETA: 4s - loss: 1.3067 - acc: 0.421 - ETA: 4s - loss: 1.3037 - acc: 0.427 - ETA: 3s - loss: 1.2795 - acc: 0.437 - ETA: 3s - loss: 1.2653 - acc: 0.453 - ETA: 2s - loss: 1.2653 - acc: 0.440 - ETA: 2s - loss: 1.2520 - acc: 0.446 - ETA: 1s - loss: 1.2577 - acc: 0.449 - ETA: 0s - loss: 1.2660 - acc: 0.449 - ETA: 0s - loss: 1.2656 - acc: 0.459 - 6s 9ms/step - loss: 1.2745 - acc: 0.4540 - val_loss: 1.1245 - val_acc: 0.6442\n",
      "Epoch 4/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 1.1211 - acc: 0.500 - ETA: 4s - loss: 1.1471 - acc: 0.500 - ETA: 4s - loss: 1.1641 - acc: 0.500 - ETA: 3s - loss: 1.1217 - acc: 0.527 - ETA: 3s - loss: 1.1264 - acc: 0.525 - ETA: 2s - loss: 1.1038 - acc: 0.549 - ETA: 2s - loss: 1.1132 - acc: 0.542 - ETA: 1s - loss: 1.0799 - acc: 0.552 - ETA: 0s - loss: 1.0831 - acc: 0.555 - ETA: 0s - loss: 1.0861 - acc: 0.557 - 6s 9ms/step - loss: 1.0891 - acc: 0.5618 - val_loss: 1.0692 - val_acc: 0.6442\n",
      "Epoch 5/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.9403 - acc: 0.656 - ETA: 4s - loss: 0.9695 - acc: 0.656 - ETA: 4s - loss: 0.9900 - acc: 0.635 - ETA: 3s - loss: 0.9578 - acc: 0.632 - ETA: 3s - loss: 0.9387 - acc: 0.656 - ETA: 2s - loss: 0.9363 - acc: 0.645 - ETA: 2s - loss: 0.9345 - acc: 0.640 - ETA: 1s - loss: 0.9390 - acc: 0.638 - ETA: 0s - loss: 0.9295 - acc: 0.645 - ETA: 0s - loss: 0.9328 - acc: 0.650 - 6s 9ms/step - loss: 0.9411 - acc: 0.6480 - val_loss: 1.0570 - val_acc: 0.6442\n",
      "Epoch 6/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.9529 - acc: 0.656 - ETA: 4s - loss: 1.0043 - acc: 0.656 - ETA: 4s - loss: 0.9202 - acc: 0.697 - ETA: 3s - loss: 0.8996 - acc: 0.707 - ETA: 3s - loss: 0.9032 - acc: 0.690 - ETA: 2s - loss: 0.9068 - acc: 0.682 - ETA: 2s - loss: 0.8855 - acc: 0.694 - ETA: 1s - loss: 0.8815 - acc: 0.697 - ETA: 0s - loss: 0.8805 - acc: 0.694 - ETA: 0s - loss: 0.8874 - acc: 0.685 - 6s 9ms/step - loss: 0.8827 - acc: 0.6853 - val_loss: 1.0361 - val_acc: 0.6635\n",
      "Epoch 7/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.8143 - acc: 0.687 - ETA: 4s - loss: 0.8844 - acc: 0.664 - ETA: 4s - loss: 0.8420 - acc: 0.703 - ETA: 3s - loss: 0.8093 - acc: 0.726 - ETA: 3s - loss: 0.8051 - acc: 0.731 - ETA: 2s - loss: 0.8070 - acc: 0.718 - ETA: 2s - loss: 0.7983 - acc: 0.723 - ETA: 1s - loss: 0.8030 - acc: 0.718 - ETA: 1s - loss: 0.7822 - acc: 0.734 - ETA: 0s - loss: 0.7857 - acc: 0.728 - 6s 9ms/step - loss: 0.7888 - acc: 0.7198 - val_loss: 1.0151 - val_acc: 0.6538\n",
      "Epoch 8/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.6235 - acc: 0.781 - ETA: 5s - loss: 0.6670 - acc: 0.765 - ETA: 4s - loss: 0.7173 - acc: 0.750 - ETA: 3s - loss: 0.7201 - acc: 0.761 - ETA: 3s - loss: 0.7283 - acc: 0.759 - ETA: 2s - loss: 0.7189 - acc: 0.770 - ETA: 2s - loss: 0.7269 - acc: 0.770 - ETA: 1s - loss: 0.7214 - acc: 0.761 - ETA: 1s - loss: 0.7055 - acc: 0.770 - ETA: 0s - loss: 0.7078 - acc: 0.768 - 7s 10ms/step - loss: 0.7164 - acc: 0.7687 - val_loss: 1.0104 - val_acc: 0.6346\n",
      "Epoch 9/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.7260 - acc: 0.718 - ETA: 5s - loss: 0.6834 - acc: 0.781 - ETA: 4s - loss: 0.6574 - acc: 0.822 - ETA: 4s - loss: 0.6693 - acc: 0.816 - ETA: 3s - loss: 0.6869 - acc: 0.796 - ETA: 2s - loss: 0.6813 - acc: 0.807 - ETA: 2s - loss: 0.6857 - acc: 0.796 - ETA: 1s - loss: 0.6781 - acc: 0.798 - ETA: 1s - loss: 0.6850 - acc: 0.789 - ETA: 0s - loss: 0.6817 - acc: 0.785 - 7s 10ms/step - loss: 0.6809 - acc: 0.7830 - val_loss: 0.9912 - val_acc: 0.6635\n",
      "Epoch 10/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.6602 - acc: 0.781 - ETA: 5s - loss: 0.6244 - acc: 0.789 - ETA: 4s - loss: 0.6382 - acc: 0.796 - ETA: 4s - loss: 0.6330 - acc: 0.796 - ETA: 3s - loss: 0.6180 - acc: 0.809 - ETA: 3s - loss: 0.6192 - acc: 0.812 - ETA: 2s - loss: 0.6114 - acc: 0.812 - ETA: 1s - loss: 0.6072 - acc: 0.822 - ETA: 1s - loss: 0.6267 - acc: 0.817 - ETA: 0s - loss: 0.6215 - acc: 0.818 - 7s 11ms/step - loss: 0.6183 - acc: 0.8204 - val_loss: 0.9791 - val_acc: 0.6731\n",
      "Epoch 11/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.5178 - acc: 0.890 - ETA: 5s - loss: 0.5400 - acc: 0.867 - ETA: 5s - loss: 0.5200 - acc: 0.864 - ETA: 4s - loss: 0.5527 - acc: 0.843 - ETA: 3s - loss: 0.5671 - acc: 0.840 - ETA: 3s - loss: 0.5621 - acc: 0.841 - ETA: 2s - loss: 0.5802 - acc: 0.832 - ETA: 1s - loss: 0.5869 - acc: 0.830 - ETA: 1s - loss: 0.5948 - acc: 0.824 - ETA: 0s - loss: 0.5933 - acc: 0.823 - 8s 11ms/step - loss: 0.5912 - acc: 0.8218 - val_loss: 0.9894 - val_acc: 0.6538\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - ETA: 6s - loss: 0.5040 - acc: 0.906 - ETA: 6s - loss: 0.5020 - acc: 0.867 - ETA: 5s - loss: 0.4837 - acc: 0.885 - ETA: 4s - loss: 0.5109 - acc: 0.863 - ETA: 3s - loss: 0.5126 - acc: 0.856 - ETA: 3s - loss: 0.5289 - acc: 0.849 - ETA: 2s - loss: 0.5382 - acc: 0.841 - ETA: 1s - loss: 0.5421 - acc: 0.841 - ETA: 1s - loss: 0.5446 - acc: 0.847 - ETA: 0s - loss: 0.5472 - acc: 0.845 - 8s 11ms/step - loss: 0.5448 - acc: 0.8448 - val_loss: 0.9658 - val_acc: 0.6538\n",
      "Epoch 13/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.4934 - acc: 0.859 - ETA: 6s - loss: 0.5451 - acc: 0.843 - ETA: 5s - loss: 0.5662 - acc: 0.817 - ETA: 4s - loss: 0.5262 - acc: 0.839 - ETA: 4s - loss: 0.5332 - acc: 0.831 - ETA: 3s - loss: 0.5454 - acc: 0.828 - ETA: 2s - loss: 0.5470 - acc: 0.830 - ETA: 2s - loss: 0.5494 - acc: 0.830 - ETA: 1s - loss: 0.5448 - acc: 0.833 - ETA: 0s - loss: 0.5390 - acc: 0.837 - 8s 12ms/step - loss: 0.5414 - acc: 0.8405 - val_loss: 0.9745 - val_acc: 0.6827\n",
      "Epoch 14/50\n",
      "696/696 [==============================] - ETA: 7s - loss: 0.5269 - acc: 0.843 - ETA: 6s - loss: 0.4672 - acc: 0.867 - ETA: 5s - loss: 0.5011 - acc: 0.859 - ETA: 5s - loss: 0.4935 - acc: 0.855 - ETA: 4s - loss: 0.5179 - acc: 0.850 - ETA: 3s - loss: 0.5325 - acc: 0.851 - ETA: 2s - loss: 0.5308 - acc: 0.846 - ETA: 2s - loss: 0.5256 - acc: 0.855 - ETA: 1s - loss: 0.5300 - acc: 0.845 - ETA: 0s - loss: 0.5190 - acc: 0.853 - 8s 12ms/step - loss: 0.5158 - acc: 0.8534 - val_loss: 0.9587 - val_acc: 0.6635\n",
      "Epoch 15/50\n",
      "696/696 [==============================] - ETA: 7s - loss: 0.4847 - acc: 0.843 - ETA: 6s - loss: 0.4569 - acc: 0.882 - ETA: 6s - loss: 0.4447 - acc: 0.890 - ETA: 5s - loss: 0.4462 - acc: 0.902 - ETA: 4s - loss: 0.4742 - acc: 0.881 - ETA: 3s - loss: 0.4849 - acc: 0.862 - ETA: 2s - loss: 0.4971 - acc: 0.857 - ETA: 2s - loss: 0.4926 - acc: 0.857 - ETA: 1s - loss: 0.4930 - acc: 0.849 - ETA: 0s - loss: 0.4907 - acc: 0.850 - 9s 13ms/step - loss: 0.4941 - acc: 0.8534 - val_loss: 0.9646 - val_acc: 0.6346\n",
      "Epoch 16/50\n",
      "696/696 [==============================] - ETA: 7s - loss: 0.4117 - acc: 0.906 - ETA: 6s - loss: 0.4272 - acc: 0.898 - ETA: 6s - loss: 0.4860 - acc: 0.854 - ETA: 5s - loss: 0.4567 - acc: 0.875 - ETA: 4s - loss: 0.4584 - acc: 0.875 - ETA: 3s - loss: 0.4684 - acc: 0.867 - ETA: 2s - loss: 0.4657 - acc: 0.870 - ETA: 2s - loss: 0.4490 - acc: 0.877 - ETA: 1s - loss: 0.4397 - acc: 0.881 - ETA: 0s - loss: 0.4524 - acc: 0.873 - 9s 13ms/step - loss: 0.4599 - acc: 0.8678 - val_loss: 0.9606 - val_acc: 0.6827\n",
      "Epoch 17/50\n",
      "696/696 [==============================] - ETA: 8s - loss: 0.3284 - acc: 0.921 - ETA: 7s - loss: 0.4185 - acc: 0.867 - ETA: 6s - loss: 0.4155 - acc: 0.869 - ETA: 5s - loss: 0.4118 - acc: 0.886 - ETA: 4s - loss: 0.4206 - acc: 0.881 - ETA: 4s - loss: 0.4406 - acc: 0.877 - ETA: 3s - loss: 0.4259 - acc: 0.881 - ETA: 2s - loss: 0.4341 - acc: 0.875 - ETA: 1s - loss: 0.4326 - acc: 0.873 - ETA: 0s - loss: 0.4528 - acc: 0.860 - 10s 14ms/step - loss: 0.4526 - acc: 0.8635 - val_loss: 0.9454 - val_acc: 0.6538\n",
      "Epoch 18/50\n",
      "696/696 [==============================] - ETA: 8s - loss: 0.3716 - acc: 0.890 - ETA: 7s - loss: 0.4358 - acc: 0.890 - ETA: 6s - loss: 0.4425 - acc: 0.895 - ETA: 5s - loss: 0.4322 - acc: 0.886 - ETA: 4s - loss: 0.4264 - acc: 0.881 - ETA: 4s - loss: 0.4246 - acc: 0.880 - ETA: 3s - loss: 0.4269 - acc: 0.877 - ETA: 2s - loss: 0.4146 - acc: 0.882 - ETA: 1s - loss: 0.4259 - acc: 0.875 - ETA: 0s - loss: 0.4285 - acc: 0.873 - 10s 14ms/step - loss: 0.4281 - acc: 0.8736 - val_loss: 0.9586 - val_acc: 0.6731\n",
      "Epoch 19/50\n",
      "696/696 [==============================] - ETA: 8s - loss: 0.3167 - acc: 0.953 - ETA: 7s - loss: 0.3897 - acc: 0.890 - ETA: 6s - loss: 0.3744 - acc: 0.895 - ETA: 5s - loss: 0.4088 - acc: 0.886 - ETA: 5s - loss: 0.4079 - acc: 0.890 - ETA: 4s - loss: 0.4138 - acc: 0.880 - ETA: 3s - loss: 0.4210 - acc: 0.870 - ETA: 2s - loss: 0.4333 - acc: 0.865 - ETA: 1s - loss: 0.4347 - acc: 0.869 - ETA: 0s - loss: 0.4399 - acc: 0.865 - 10s 14ms/step - loss: 0.4322 - acc: 0.8693 - val_loss: 0.9542 - val_acc: 0.6538\n",
      "Epoch 20/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.4345 - acc: 0.859 - ETA: 8s - loss: 0.3696 - acc: 0.890 - ETA: 7s - loss: 0.3712 - acc: 0.901 - ETA: 6s - loss: 0.3795 - acc: 0.894 - ETA: 5s - loss: 0.3963 - acc: 0.890 - ETA: 4s - loss: 0.4009 - acc: 0.880 - ETA: 3s - loss: 0.4091 - acc: 0.875 - ETA: 2s - loss: 0.4059 - acc: 0.875 - ETA: 1s - loss: 0.4046 - acc: 0.873 - ETA: 0s - loss: 0.4019 - acc: 0.871 - 11s 16ms/step - loss: 0.4044 - acc: 0.8693 - val_loss: 0.9588 - val_acc: 0.6538\n",
      "Epoch 21/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.4307 - acc: 0.859 - ETA: 8s - loss: 0.4388 - acc: 0.859 - ETA: 7s - loss: 0.3881 - acc: 0.864 - ETA: 6s - loss: 0.3960 - acc: 0.855 - ETA: 5s - loss: 0.4051 - acc: 0.859 - ETA: 4s - loss: 0.3926 - acc: 0.867 - ETA: 3s - loss: 0.3956 - acc: 0.866 - ETA: 2s - loss: 0.4036 - acc: 0.867 - ETA: 1s - loss: 0.4030 - acc: 0.875 - ETA: 0s - loss: 0.3997 - acc: 0.875 - 11s 16ms/step - loss: 0.4005 - acc: 0.8693 - val_loss: 0.9435 - val_acc: 0.6635\n",
      "Epoch 22/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.4274 - acc: 0.906 - ETA: 8s - loss: 0.4323 - acc: 0.859 - ETA: 7s - loss: 0.4185 - acc: 0.854 - ETA: 6s - loss: 0.4190 - acc: 0.867 - ETA: 5s - loss: 0.4247 - acc: 0.868 - ETA: 4s - loss: 0.4234 - acc: 0.864 - ETA: 3s - loss: 0.4324 - acc: 0.857 - ETA: 2s - loss: 0.4169 - acc: 0.863 - ETA: 1s - loss: 0.3951 - acc: 0.871 - ETA: 0s - loss: 0.3830 - acc: 0.876 - 11s 16ms/step - loss: 0.3790 - acc: 0.8750 - val_loss: 0.9559 - val_acc: 0.6442\n",
      "Epoch 23/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.2806 - acc: 0.953 - ETA: 8s - loss: 0.3516 - acc: 0.898 - ETA: 7s - loss: 0.3304 - acc: 0.895 - ETA: 6s - loss: 0.3532 - acc: 0.886 - ETA: 5s - loss: 0.3693 - acc: 0.881 - ETA: 4s - loss: 0.3639 - acc: 0.877 - ETA: 3s - loss: 0.3907 - acc: 0.870 - ETA: 2s - loss: 0.3868 - acc: 0.871 - ETA: 1s - loss: 0.3966 - acc: 0.864 - ETA: 0s - loss: 0.3927 - acc: 0.865 - 11s 16ms/step - loss: 0.3901 - acc: 0.8693 - val_loss: 0.9404 - val_acc: 0.6731\n",
      "Epoch 24/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.5423 - acc: 0.781 - ETA: 8s - loss: 0.4380 - acc: 0.835 - ETA: 7s - loss: 0.4425 - acc: 0.838 - ETA: 6s - loss: 0.4042 - acc: 0.863 - ETA: 5s - loss: 0.3860 - acc: 0.871 - ETA: 4s - loss: 0.3780 - acc: 0.882 - ETA: 3s - loss: 0.3719 - acc: 0.886 - ETA: 2s - loss: 0.3762 - acc: 0.878 - ETA: 1s - loss: 0.3858 - acc: 0.873 - ETA: 0s - loss: 0.3698 - acc: 0.881 - 11s 16ms/step - loss: 0.3675 - acc: 0.8807 - val_loss: 0.9325 - val_acc: 0.6635\n",
      "Epoch 25/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.3893 - acc: 0.906 - ETA: 8s - loss: 0.3157 - acc: 0.906 - ETA: 7s - loss: 0.3471 - acc: 0.895 - ETA: 6s - loss: 0.3474 - acc: 0.886 - ETA: 5s - loss: 0.3506 - acc: 0.884 - ETA: 4s - loss: 0.3386 - acc: 0.888 - ETA: 3s - loss: 0.3442 - acc: 0.892 - ETA: 2s - loss: 0.3421 - acc: 0.886 - ETA: 1s - loss: 0.3593 - acc: 0.878 - ETA: 0s - loss: 0.3656 - acc: 0.878 - 11s 16ms/step - loss: 0.3602 - acc: 0.8793 - val_loss: 0.9437 - val_acc: 0.6731\n",
      "Epoch 26/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.3478 - acc: 0.875 - ETA: 8s - loss: 0.3737 - acc: 0.859 - ETA: 7s - loss: 0.3618 - acc: 0.875 - ETA: 6s - loss: 0.3638 - acc: 0.878 - ETA: 5s - loss: 0.3704 - acc: 0.878 - ETA: 4s - loss: 0.3668 - acc: 0.880 - ETA: 3s - loss: 0.3553 - acc: 0.888 - ETA: 2s - loss: 0.3457 - acc: 0.888 - ETA: 1s - loss: 0.3538 - acc: 0.881 - ETA: 0s - loss: 0.3603 - acc: 0.878 - 13s 18ms/step - loss: 0.3573 - acc: 0.8793 - val_loss: 0.9340 - val_acc: 0.6731\n",
      "Epoch 27/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.4360 - acc: 0.90 - ETA: 12s - loss: 0.4141 - acc: 0.88 - ETA: 10s - loss: 0.3960 - acc: 0.87 - ETA: 9s - loss: 0.3646 - acc: 0.8828 - ETA: 8s - loss: 0.3405 - acc: 0.890 - ETA: 6s - loss: 0.3242 - acc: 0.898 - ETA: 5s - loss: 0.3233 - acc: 0.895 - ETA: 3s - loss: 0.3305 - acc: 0.892 - ETA: 2s - loss: 0.3282 - acc: 0.892 - ETA: 1s - loss: 0.3307 - acc: 0.887 - 16s 23ms/step - loss: 0.3436 - acc: 0.8793 - val_loss: 0.9628 - val_acc: 0.6538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3346 - acc: 0.87 - ETA: 12s - loss: 0.3763 - acc: 0.84 - ETA: 10s - loss: 0.3841 - acc: 0.84 - ETA: 9s - loss: 0.3807 - acc: 0.8594 - ETA: 8s - loss: 0.3643 - acc: 0.871 - ETA: 6s - loss: 0.3490 - acc: 0.877 - ETA: 5s - loss: 0.3462 - acc: 0.875 - ETA: 3s - loss: 0.3509 - acc: 0.869 - ETA: 2s - loss: 0.3429 - acc: 0.873 - ETA: 1s - loss: 0.3360 - acc: 0.875 - 16s 23ms/step - loss: 0.3428 - acc: 0.8736 - val_loss: 0.9446 - val_acc: 0.6538\n",
      "Epoch 29/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3346 - acc: 0.87 - ETA: 12s - loss: 0.3572 - acc: 0.88 - ETA: 10s - loss: 0.3662 - acc: 0.86 - ETA: 9s - loss: 0.3506 - acc: 0.8789 - ETA: 8s - loss: 0.3481 - acc: 0.871 - ETA: 6s - loss: 0.3489 - acc: 0.872 - ETA: 5s - loss: 0.3431 - acc: 0.877 - ETA: 3s - loss: 0.3395 - acc: 0.873 - ETA: 2s - loss: 0.3439 - acc: 0.876 - ETA: 1s - loss: 0.3301 - acc: 0.882 - 16s 23ms/step - loss: 0.3357 - acc: 0.8807 - val_loss: 0.9415 - val_acc: 0.6731\n",
      "Epoch 30/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2729 - acc: 0.89 - ETA: 12s - loss: 0.3166 - acc: 0.87 - ETA: 10s - loss: 0.3202 - acc: 0.87 - ETA: 9s - loss: 0.3239 - acc: 0.8711 - ETA: 8s - loss: 0.3398 - acc: 0.865 - ETA: 6s - loss: 0.3389 - acc: 0.867 - ETA: 5s - loss: 0.3392 - acc: 0.875 - ETA: 3s - loss: 0.3311 - acc: 0.880 - ETA: 2s - loss: 0.3383 - acc: 0.878 - ETA: 1s - loss: 0.3386 - acc: 0.875 - 16s 23ms/step - loss: 0.3352 - acc: 0.8779 - val_loss: 0.9376 - val_acc: 0.6731\n",
      "Epoch 31/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3025 - acc: 0.92 - ETA: 12s - loss: 0.3119 - acc: 0.89 - ETA: 10s - loss: 0.3133 - acc: 0.89 - ETA: 9s - loss: 0.3011 - acc: 0.8906 - ETA: 8s - loss: 0.3123 - acc: 0.884 - ETA: 6s - loss: 0.3159 - acc: 0.885 - ETA: 5s - loss: 0.3355 - acc: 0.875 - ETA: 3s - loss: 0.3424 - acc: 0.873 - ETA: 2s - loss: 0.3250 - acc: 0.880 - ETA: 1s - loss: 0.3298 - acc: 0.878 - 16s 23ms/step - loss: 0.3199 - acc: 0.8822 - val_loss: 0.9328 - val_acc: 0.6731\n",
      "Epoch 32/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2908 - acc: 0.90 - ETA: 12s - loss: 0.2981 - acc: 0.89 - ETA: 10s - loss: 0.2931 - acc: 0.90 - ETA: 9s - loss: 0.2884 - acc: 0.8984 - ETA: 8s - loss: 0.3201 - acc: 0.875 - ETA: 6s - loss: 0.3015 - acc: 0.882 - ETA: 5s - loss: 0.3240 - acc: 0.875 - ETA: 3s - loss: 0.3117 - acc: 0.884 - ETA: 2s - loss: 0.3121 - acc: 0.883 - ETA: 1s - loss: 0.3246 - acc: 0.879 - 16s 23ms/step - loss: 0.3266 - acc: 0.8779 - val_loss: 0.9513 - val_acc: 0.6442\n",
      "Epoch 33/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3973 - acc: 0.84 - ETA: 12s - loss: 0.3435 - acc: 0.85 - ETA: 10s - loss: 0.3334 - acc: 0.87 - ETA: 9s - loss: 0.3246 - acc: 0.8711 - ETA: 8s - loss: 0.3132 - acc: 0.878 - ETA: 6s - loss: 0.3032 - acc: 0.880 - ETA: 5s - loss: 0.3029 - acc: 0.883 - ETA: 3s - loss: 0.3059 - acc: 0.884 - ETA: 2s - loss: 0.3039 - acc: 0.888 - ETA: 1s - loss: 0.3225 - acc: 0.881 - 16s 23ms/step - loss: 0.3236 - acc: 0.8779 - val_loss: 0.9367 - val_acc: 0.6635\n",
      "Epoch 34/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3339 - acc: 0.87 - ETA: 12s - loss: 0.3559 - acc: 0.84 - ETA: 10s - loss: 0.3193 - acc: 0.87 - ETA: 9s - loss: 0.3236 - acc: 0.8711 - ETA: 8s - loss: 0.3010 - acc: 0.887 - ETA: 6s - loss: 0.3030 - acc: 0.885 - ETA: 5s - loss: 0.2934 - acc: 0.888 - ETA: 3s - loss: 0.3023 - acc: 0.882 - ETA: 2s - loss: 0.3108 - acc: 0.881 - ETA: 1s - loss: 0.3166 - acc: 0.882 - 16s 23ms/step - loss: 0.3157 - acc: 0.8807 - val_loss: 0.9382 - val_acc: 0.6538\n",
      "Epoch 35/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3046 - acc: 0.87 - ETA: 12s - loss: 0.3415 - acc: 0.86 - ETA: 10s - loss: 0.3440 - acc: 0.85 - ETA: 9s - loss: 0.3028 - acc: 0.8750 - ETA: 8s - loss: 0.3039 - acc: 0.881 - ETA: 6s - loss: 0.3038 - acc: 0.885 - ETA: 5s - loss: 0.3046 - acc: 0.886 - ETA: 3s - loss: 0.3038 - acc: 0.890 - ETA: 2s - loss: 0.3004 - acc: 0.892 - ETA: 1s - loss: 0.2963 - acc: 0.893 - 16s 23ms/step - loss: 0.3125 - acc: 0.8807 - val_loss: 0.9598 - val_acc: 0.6346\n",
      "Epoch 36/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3660 - acc: 0.85 - ETA: 12s - loss: 0.3374 - acc: 0.88 - ETA: 10s - loss: 0.3765 - acc: 0.85 - ETA: 9s - loss: 0.3677 - acc: 0.8555 - ETA: 8s - loss: 0.3283 - acc: 0.875 - ETA: 6s - loss: 0.3192 - acc: 0.882 - ETA: 5s - loss: 0.2990 - acc: 0.890 - ETA: 3s - loss: 0.3134 - acc: 0.877 - ETA: 2s - loss: 0.3163 - acc: 0.876 - ETA: 1s - loss: 0.3010 - acc: 0.882 - 16s 23ms/step - loss: 0.3068 - acc: 0.8822 - val_loss: 0.9460 - val_acc: 0.6635\n",
      "Epoch 37/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2712 - acc: 0.89 - ETA: 12s - loss: 0.2789 - acc: 0.89 - ETA: 10s - loss: 0.2871 - acc: 0.88 - ETA: 9s - loss: 0.2887 - acc: 0.8867 - ETA: 8s - loss: 0.2924 - acc: 0.890 - ETA: 6s - loss: 0.3142 - acc: 0.880 - ETA: 5s - loss: 0.2958 - acc: 0.888 - ETA: 3s - loss: 0.3104 - acc: 0.877 - ETA: 2s - loss: 0.3255 - acc: 0.866 - ETA: 1s - loss: 0.3225 - acc: 0.873 - 16s 23ms/step - loss: 0.3105 - acc: 0.8793 - val_loss: 0.9368 - val_acc: 0.6731\n",
      "Epoch 38/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2230 - acc: 0.90 - ETA: 12s - loss: 0.2624 - acc: 0.89 - ETA: 10s - loss: 0.2448 - acc: 0.91 - ETA: 9s - loss: 0.2833 - acc: 0.8867 - ETA: 8s - loss: 0.2842 - acc: 0.884 - ETA: 6s - loss: 0.2869 - acc: 0.888 - ETA: 5s - loss: 0.2895 - acc: 0.888 - ETA: 3s - loss: 0.2892 - acc: 0.890 - ETA: 2s - loss: 0.2990 - acc: 0.887 - ETA: 1s - loss: 0.3036 - acc: 0.882 - 16s 23ms/step - loss: 0.3109 - acc: 0.8793 - val_loss: 0.9575 - val_acc: 0.6731\n",
      "Epoch 39/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3346 - acc: 0.87 - ETA: 12s - loss: 0.3360 - acc: 0.86 - ETA: 11s - loss: 0.3478 - acc: 0.86 - ETA: 9s - loss: 0.3275 - acc: 0.8789 - ETA: 8s - loss: 0.3168 - acc: 0.881 - ETA: 6s - loss: 0.3284 - acc: 0.875 - ETA: 5s - loss: 0.3304 - acc: 0.870 - ETA: 3s - loss: 0.3185 - acc: 0.878 - ETA: 2s - loss: 0.3089 - acc: 0.881 - ETA: 1s - loss: 0.3122 - acc: 0.878 - 16s 23ms/step - loss: 0.3067 - acc: 0.8793 - val_loss: 0.9463 - val_acc: 0.6538\n",
      "Epoch 40/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2558 - acc: 0.92 - ETA: 12s - loss: 0.3041 - acc: 0.88 - ETA: 10s - loss: 0.2967 - acc: 0.89 - ETA: 9s - loss: 0.2812 - acc: 0.8945 - ETA: 8s - loss: 0.2730 - acc: 0.890 - ETA: 6s - loss: 0.2854 - acc: 0.880 - ETA: 5s - loss: 0.2863 - acc: 0.881 - ETA: 3s - loss: 0.2982 - acc: 0.877 - ETA: 2s - loss: 0.2914 - acc: 0.881 - ETA: 1s - loss: 0.2931 - acc: 0.882 - 16s 23ms/step - loss: 0.3064 - acc: 0.8779 - val_loss: 0.9407 - val_acc: 0.6635\n",
      "Epoch 41/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2749 - acc: 0.89 - ETA: 12s - loss: 0.2613 - acc: 0.90 - ETA: 10s - loss: 0.2862 - acc: 0.89 - ETA: 9s - loss: 0.3195 - acc: 0.8711 - ETA: 8s - loss: 0.3016 - acc: 0.875 - ETA: 6s - loss: 0.2916 - acc: 0.880 - ETA: 5s - loss: 0.2976 - acc: 0.875 - ETA: 3s - loss: 0.2999 - acc: 0.877 - ETA: 2s - loss: 0.2893 - acc: 0.883 - ETA: 1s - loss: 0.2912 - acc: 0.884 - 16s 23ms/step - loss: 0.3002 - acc: 0.8822 - val_loss: 0.9506 - val_acc: 0.6731\n",
      "Epoch 42/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.1740 - acc: 0.95 - ETA: 12s - loss: 0.2381 - acc: 0.91 - ETA: 10s - loss: 0.2556 - acc: 0.89 - ETA: 9s - loss: 0.2909 - acc: 0.8867 - ETA: 8s - loss: 0.2845 - acc: 0.890 - ETA: 6s - loss: 0.2742 - acc: 0.895 - ETA: 5s - loss: 0.2767 - acc: 0.892 - ETA: 3s - loss: 0.2804 - acc: 0.888 - ETA: 2s - loss: 0.2785 - acc: 0.887 - ETA: 1s - loss: 0.2921 - acc: 0.884 - 16s 23ms/step - loss: 0.2982 - acc: 0.8807 - val_loss: 0.9438 - val_acc: 0.6635\n",
      "Epoch 43/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3170 - acc: 0.85 - ETA: 12s - loss: 0.2974 - acc: 0.86 - ETA: 10s - loss: 0.3042 - acc: 0.87 - ETA: 9s - loss: 0.3240 - acc: 0.8750 - ETA: 8s - loss: 0.3120 - acc: 0.875 - ETA: 6s - loss: 0.3123 - acc: 0.869 - ETA: 5s - loss: 0.3087 - acc: 0.870 - ETA: 3s - loss: 0.2978 - acc: 0.875 - ETA: 2s - loss: 0.2986 - acc: 0.876 - ETA: 1s - loss: 0.2968 - acc: 0.882 - 193s 278ms/step - loss: 0.3005 - acc: 0.8793 - val_loss: 0.9406 - val_acc: 0.6538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.3107 - acc: 0.843 - ETA: 4s - loss: 0.3147 - acc: 0.867 - ETA: 4s - loss: 0.3142 - acc: 0.875 - ETA: 3s - loss: 0.3024 - acc: 0.886 - ETA: 2s - loss: 0.2942 - acc: 0.884 - ETA: 2s - loss: 0.2870 - acc: 0.888 - ETA: 1s - loss: 0.2970 - acc: 0.879 - ETA: 1s - loss: 0.3071 - acc: 0.875 - ETA: 0s - loss: 0.3064 - acc: 0.875 - ETA: 0s - loss: 0.2970 - acc: 0.879 - 6s 8ms/step - loss: 0.2971 - acc: 0.8822 - val_loss: 0.9656 - val_acc: 0.6058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15fcc4eda20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(lr=0.001) # , beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# optimizer = Adam(lr=0.008)\n",
    "# reducer_lr = ReduceLROnPlateau(factor = 0.00005, patience = 1, min_lr = 1e-6, verbose = 1)\n",
    "\n",
    "my_input = Input(shape=(max_size, ))\n",
    "\n",
    "embedding = Embedding(input_dim=len(vocab)+1,\n",
    "                      output_dim=300,\n",
    "                      weights=[embeddings],\n",
    "                      trainable=False,\n",
    "                      input_length=max_size\n",
    "                      ) (my_input)\n",
    "\n",
    "conv1 = Conv1D(filters=100,\n",
    "               kernel_size=3,\n",
    "               activation = 'relu',\n",
    "#                kernel_regularizer=l2(0.0001),\n",
    "#                bias_regularizer=l2(0.0001)\n",
    "               ) (embedding)\n",
    "\n",
    "conv2 = Conv1D(filters=100,\n",
    "               kernel_size=4,\n",
    "               activation = 'relu',\n",
    "#                kernel_regularizer=l2(0.0001),\n",
    "#                bias_regularizer=l2(0.0001)\n",
    "               ) (embedding)\n",
    "\n",
    "pool1 = GlobalMaxPooling1D()(conv1)\n",
    "pool2 = GlobalMaxPooling1D()(conv2)\n",
    "\n",
    "concat = Concatenate()([pool1, pool2])\n",
    "drop = Dropout(rate=0.5) (concat)\n",
    "my_output = Dense(units=5, activation='softmax')(drop)\n",
    "\n",
    "model_CNN = Model(my_input, my_output)\n",
    "model_CNN.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_CNN.summary()\n",
    "\n",
    "# go through epochs as long as accuracy on validation set increases\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=20,\n",
    "                               mode='min')\n",
    "\n",
    "# make sure that the model corresponding to the best epoch is saved\n",
    "checkpointer = ModelCheckpoint(filepath='cnn_text_categorization.hdf5',\n",
    "                               monitor='val_loss',\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "\n",
    "model_CNN.fit(X_tra_CNN,\n",
    "          y_tra_CNN,\n",
    "          epochs=50,\n",
    "          batch_size=64,\n",
    "          validation_data=(X_val_CNN, y_val_CNN),\n",
    "#           validation_split=0.05,\n",
    "          callbacks=[early_stopping, checkpointer]#, reducer_lr]\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "del model_CNN\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_CNN = model_CNN.predict(X_tra_CNN)\n",
    "y_pred_val_CNN = model_CNN.predict(X_val_CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN text log loss:\n",
      "0.2684406981762581\n",
      "0.9230987612827448\n"
     ]
    }
   ],
   "source": [
    "print('CNN text log loss:')\n",
    "print(log_loss(y_tra_CNN,y_pred_train_CNN))\n",
    "print(log_loss(y_val_CNN, y_pred_val_CNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering<a id='fe'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load all preprocced document tokens from serialized pickle objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "serialized_docs_train_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/train_all/'\n",
    "filenames = os.listdir(serialized_docs_train_dir)\n",
    "\n",
    "train_data = []\n",
    "y_train_tfidf = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_train_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        train_data.append(doc['x_list'])               \n",
    "        y_train_tfidf.append(doc['y'])\n",
    "        \n",
    "serialized_docs_test_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/test_all/'\n",
    "filenames = os.listdir(serialized_docs_test_dir)\n",
    "\n",
    "test_data = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_test_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        test_data.append(doc['x_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the tokens so they can be give to tf-idf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []\n",
    "y_train_doc = []\n",
    "\n",
    "for idx, doc in enumerate(train_data):\n",
    "    if len(doc) != 0:\n",
    "        train_text.append(' '.join(doc))\n",
    "        y_train_doc.append(y_train_tfidf[idx])\n",
    "y_train_doc = np.array(y_train_doc)\n",
    "\n",
    "test_text = []\n",
    "no_text_idx = []\n",
    "text_idx = []\n",
    "for idx, doc in enumerate(test_data):\n",
    "    if len(doc) != 0:          \n",
    "        test_text.append(' '.join(doc))\n",
    "        text_idx.append(idx)\n",
    "    else:        \n",
    "        no_text_idx.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF<a id='tfidf'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = np.concatenate((train_text, test_text))\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    decode_error='ignore',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.7,\n",
    "    min_df=0.02,\n",
    "    lowercase=False,\n",
    "    max_features=5000)\n",
    "word_vectorizer.fit(all_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    decode_error='ignore',\n",
    "    analyzer='char',\n",
    "    max_df=0.5,\n",
    "    min_df=0.2,\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=3000,\n",
    "    lowercase=False)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA/LSI with Truncated SVD(for LSA) on tf-idfs<a id='tsvd'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000, random_state=4321)\n",
    "train_features_svd = svd.fit_transform(train_features)\n",
    "test_features_svd = svd.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25<a id='bm25'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "all_text = np.concatenate((train_data, test_data))\n",
    "result = get_bm25_weights(all_text, n_jobs=-1)\n",
    "\n",
    "x_train = result[:800]\n",
    "x_test = result[800:]\n",
    "\n",
    "X_train_bm25 = np.zeros((800, 1000))\n",
    "for i in range(800):\n",
    "    X_train_bm25[i] = x_train[i]\n",
    "\n",
    "X_test_bm25 = np.zeros((200, 1000))\n",
    "for i in range(200):\n",
    "    X_test_bm25[i] = x_test[i]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_bm25_scaled = min_max_scaler.fit_transform(X_train_bm25)\n",
    "X_train_bm25 = pd.DataFrame(X_train_bm25_scaled)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_test_bm25_scaled = min_max_scaler.fit_transform(X_test_bm25)\n",
    "X_test_bm25 = pd.DataFrame(X_test_bm25_scaled)\n",
    "\n",
    "classes = {0:'athlitismos', 1:'diaskedasi-psyxagogia', 2:'eidiseis-mme',\n",
    "       3:'katastimata-agores', 4:'pliroforiki-diadiktyo'}\n",
    "\n",
    "y_train_cl = [classes[y_train_class[i]] for i in range(800)]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(X_train_bm25, y_train_cl, train_size=0.7,\n",
    "                                              random_state=245354, stratify=y_train_cl)\n",
    "\n",
    "# C = 4\n",
    "# gamma = 0.1\n",
    "\n",
    "# svc = SVC(probability=True, C=C, kernel='rbf', gamma=gamma, random_state=7)\n",
    "# svc.fit(X_tra, y_tra)\n",
    "# y_pred_train = svc.predict_proba(X_tra)\n",
    "# y_pred_val = svc.predict_proba(X_val)\n",
    "\n",
    "mcw = 10\n",
    "md = 100\n",
    "g = 0\n",
    "\n",
    "xgb = XGBClassifier(min_child_weight = mcw, gamma = g, max_depth = md)\n",
    "xgb.fit(X_tra, y_tra)\n",
    "y_pred_train = xgb.predict_proba(X_tra)\n",
    "y_pred_val = xgb.predict_proba(X_val)\n",
    "\n",
    "\n",
    "# if ((abs(svc.score(X_tra, y_tra) - svc.score(X_val, y_val))) <= 0.06 ) \\\n",
    "#     & (log_loss(y_tra, y_pred_train)<0.97):\n",
    "print('C = {} , gamma = {} '.format(C,gamma))\n",
    "print(svc.score(X_tra, y_tra))\n",
    "print(svc.score(X_val, y_val))\n",
    "print('Loss')\n",
    "print(log_loss(y_tra, y_pred_train))\n",
    "print(log_loss(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LSA_1000_train\n",
    "# path_train_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/train_LSA'\n",
    "# with open(path_train_tfidf, 'rb') as file:\n",
    "#         train_features_svd = pickle.load(file)\n",
    "        \n",
    "# #LSA_100_test    \n",
    "# path_test_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/test_LSA'\n",
    "# with open(path_test_tfidf, 'rb') as file:\n",
    "#         test_features_svd = pickle.load(file)\n",
    "        \n",
    "#tfidf_8000_train\n",
    "path_train_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/train_tf_idf'\n",
    "with open(path_train_tfidf, 'rb') as file:\n",
    "        train_features = pickle.load(file)\n",
    "        \n",
    "#tfidf_8000_test\n",
    "path_test_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/test_tf_idf'\n",
    "with open(path_test_tfidf, 'rb') as file:\n",
    "        test_features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4a129a571980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_train_doc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_indicies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m676\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_tra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tra_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_indicies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train_doc' is not defined"
     ]
    }
   ],
   "source": [
    "y = [np.argmax(t) for t in y_train_doc]\n",
    "x_indicies = list(range(0, 676))\n",
    "\n",
    "X_tra, X_val, y_tra_class, y_val_class = train_test_split(x_indicies, y, train_size=0.80, random_state=2, stratify=y)\n",
    "\n",
    "# y_tra = np.take(y_train_doc, X_tra, axis=0)\n",
    "# y_val = np.take(y_train_doc, X_val, axis=0) #X_val[70:]\n",
    "\n",
    "# # tmp_v = X_val[:70]\n",
    "# # temp_x = train_features.tocsr()[tmp_v,:]\n",
    "# # temp_y = np.take(y_train_doc, tmp_v, axis=0)\n",
    "\n",
    "# X_tra = train_features.tocsr()[X_tra,:]\n",
    "# X_val = train_features.tocsr()[X_val,:] #[X_val[70:],:]\n",
    "\n",
    "#TSVD tf-idf\n",
    "y_tra = np.take(y_train_doc, X_tra, axis=0)\n",
    "y_val = np.take(y_train_doc, X_val, axis=0)\n",
    "\n",
    "# tmp_v = X_val[:70]\n",
    "# temp_x = np.take(train_features_svd, tmp_v, axis=0)\n",
    "# temp_y = np.take(y_train_doc, tmp_v, axis=0)\n",
    "\n",
    "X_tra = np.take(train_features_svd, X_tra, axis=0)\n",
    "X_val = np.take(train_features_svd, X_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ll\n",
      "0.8506856713731347\n",
      "1.0800381862364583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from statistics import mean \n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seeds = [random.randint(1, 20000) for _ in range(2)]\n",
    "res_train = []\n",
    "res_val = []\n",
    "x_indicies = list(range(0, 676))\n",
    "\n",
    "for idx in range(len(seeds)):  \n",
    "    \n",
    "    X_tra, X_val, y_tra_class, y_val_class = train_test_split(x_indicies, y, train_size=0.90, random_state=33, stratify=y)\n",
    "    \n",
    "    #TSVD tf-idf\n",
    "    y_tra_l = np.take(y_train_doc, X_tra, axis=0)\n",
    "    y_val_l = np.take(y_train_doc, X_val, axis=0)\n",
    "\n",
    "    X_tra = np.take(train_features_svd, X_tra, axis=0)\n",
    "    X_val = np.take(train_features_svd, X_val, axis=0)\n",
    "\n",
    "    svc = SVC(probability=True, C=1.1, kernel='rbf', gamma=0.035, random_state=5) #C=1.3, gamma=0.4\n",
    "    svc.fit(X_tra, y_tra_class)\n",
    "\n",
    "    y_pred_train = svc.predict_proba(X_tra)\n",
    "    y_pred_val = svc.predict_proba(X_val)\n",
    "    \n",
    "    res_train.append(log_loss(y_tra_l, y_pred_train))\n",
    "    res_val.append(log_loss(y_val_l, y_pred_val))\n",
    "\n",
    "print('Mean ll')\n",
    "print(mean(res_train))\n",
    "print(mean(res_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN Mlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(\n",
    "    input_size,\n",
    "    optimizer,    \n",
    "    classes=5,  \n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    hidden_layers=1,\n",
    "    units = 600,\n",
    "    dropout_rate = 0.5,\n",
    "    funnel = True,\n",
    "    hidden_activation='relu',\n",
    "    output_activation='softmax'\n",
    "):\n",
    "  \n",
    "    # Define the seed1for numpy and Tensorflow to have reproducible experiments.\n",
    "    np.random.seed(1402) \n",
    "    set_random_seed(2)\n",
    "       \n",
    "    input = Input(\n",
    "        shape=(input_size,),\n",
    "        name='Input'\n",
    "    )\n",
    "    x = input\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "    print(x.shape)\n",
    "    # Define the hidden layers.\n",
    "    for i in range(hidden_layers):\n",
    "        if funnel:\n",
    "            layer_units=units // (i+1)\n",
    "        else: \n",
    "            layer_units=units\n",
    "        x = Dense(\n",
    "           units=layer_units,\n",
    "           kernel_initializer='glorot_uniform',\n",
    "           activation=hidden_activation,\n",
    "           name='Hidden-{0:d}'.format(i + 1)\n",
    "        )(x)\n",
    "     #   x = BatchNormalization()(x)\n",
    "     #   x = BatchNormalization(x)\n",
    "        #Dropout\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "        \n",
    "    # Define the output layer.    \n",
    "    output = Dense(\n",
    "        units=classes,\n",
    "        kernel_initializer='uniform',\n",
    "        activation=output_activation,\n",
    "        name='Output'\n",
    "    )(x)\n",
    "    # Define the model and train it.\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "      \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_crossentropy', 'accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 676)\n",
      "Train on 540 samples, validate on 136 samples\n",
      "Epoch 1/200\n",
      "540/540 [==============================] - ETA: 3s - loss: 1.6038 - categorical_crossentropy: 1.6038 - acc: 0.23 - 1s 2ms/step - loss: 1.6074 - categorical_crossentropy: 1.6074 - acc: 0.2389 - val_loss: 1.6049 - val_categorical_crossentropy: 1.6049 - val_acc: 0.3603\n",
      "Epoch 2/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.6082 - categorical_crossentropy: 1.6082 - acc: 0.24 - 0s 131us/step - loss: 1.6080 - categorical_crossentropy: 1.6080 - acc: 0.2352 - val_loss: 1.6004 - val_categorical_crossentropy: 1.6004 - val_acc: 0.3529\n",
      "Epoch 3/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.6030 - categorical_crossentropy: 1.6030 - acc: 0.21 - 0s 157us/step - loss: 1.5986 - categorical_crossentropy: 1.5986 - acc: 0.2556 - val_loss: 1.5961 - val_categorical_crossentropy: 1.5961 - val_acc: 0.3676\n",
      "Epoch 4/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5983 - categorical_crossentropy: 1.5983 - acc: 0.24 - ETA: 0s - loss: 1.5963 - categorical_crossentropy: 1.5963 - acc: 0.27 - 0s 225us/step - loss: 1.5962 - categorical_crossentropy: 1.5962 - acc: 0.2741 - val_loss: 1.5916 - val_categorical_crossentropy: 1.5916 - val_acc: 0.3603\n",
      "Epoch 5/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5962 - categorical_crossentropy: 1.5962 - acc: 0.28 - ETA: 0s - loss: 1.5899 - categorical_crossentropy: 1.5899 - acc: 0.30 - 0s 222us/step - loss: 1.5896 - categorical_crossentropy: 1.5896 - acc: 0.3056 - val_loss: 1.5870 - val_categorical_crossentropy: 1.5870 - val_acc: 0.3676\n",
      "Epoch 6/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5962 - categorical_crossentropy: 1.5962 - acc: 0.25 - ETA: 0s - loss: 1.5872 - categorical_crossentropy: 1.5872 - acc: 0.30 - 0s 277us/step - loss: 1.5877 - categorical_crossentropy: 1.5877 - acc: 0.3130 - val_loss: 1.5822 - val_categorical_crossentropy: 1.5822 - val_acc: 0.3824\n",
      "Epoch 7/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5798 - categorical_crossentropy: 1.5798 - acc: 0.32 - ETA: 0s - loss: 1.5793 - categorical_crossentropy: 1.5793 - acc: 0.33 - 0s 235us/step - loss: 1.5796 - categorical_crossentropy: 1.5796 - acc: 0.3315 - val_loss: 1.5771 - val_categorical_crossentropy: 1.5771 - val_acc: 0.3750\n",
      "Epoch 8/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5674 - categorical_crossentropy: 1.5674 - acc: 0.42 - ETA: 0s - loss: 1.5704 - categorical_crossentropy: 1.5704 - acc: 0.36 - 0s 249us/step - loss: 1.5725 - categorical_crossentropy: 1.5725 - acc: 0.3611 - val_loss: 1.5715 - val_categorical_crossentropy: 1.5715 - val_acc: 0.3824\n",
      "Epoch 9/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5667 - categorical_crossentropy: 1.5667 - acc: 0.35 - ETA: 0s - loss: 1.5660 - categorical_crossentropy: 1.5660 - acc: 0.39 - 0s 225us/step - loss: 1.5663 - categorical_crossentropy: 1.5663 - acc: 0.3889 - val_loss: 1.5654 - val_categorical_crossentropy: 1.5654 - val_acc: 0.3824\n",
      "Epoch 10/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5629 - categorical_crossentropy: 1.5629 - acc: 0.32 - ETA: 0s - loss: 1.5640 - categorical_crossentropy: 1.5640 - acc: 0.37 - 0s 215us/step - loss: 1.5639 - categorical_crossentropy: 1.5639 - acc: 0.3796 - val_loss: 1.5588 - val_categorical_crossentropy: 1.5588 - val_acc: 0.3824\n",
      "Epoch 11/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5581 - categorical_crossentropy: 1.5581 - acc: 0.38 - ETA: 0s - loss: 1.5531 - categorical_crossentropy: 1.5531 - acc: 0.40 - 0s 244us/step - loss: 1.5542 - categorical_crossentropy: 1.5542 - acc: 0.3963 - val_loss: 1.5516 - val_categorical_crossentropy: 1.5516 - val_acc: 0.4044\n",
      "Epoch 12/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5504 - categorical_crossentropy: 1.5504 - acc: 0.35 - ETA: 0s - loss: 1.5482 - categorical_crossentropy: 1.5482 - acc: 0.38 - 0s 218us/step - loss: 1.5482 - categorical_crossentropy: 1.5482 - acc: 0.3778 - val_loss: 1.5439 - val_categorical_crossentropy: 1.5439 - val_acc: 0.4265\n",
      "Epoch 13/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5455 - categorical_crossentropy: 1.5455 - acc: 0.45 - ETA: 0s - loss: 1.5420 - categorical_crossentropy: 1.5420 - acc: 0.45 - 0s 231us/step - loss: 1.5426 - categorical_crossentropy: 1.5426 - acc: 0.4537 - val_loss: 1.5353 - val_categorical_crossentropy: 1.5353 - val_acc: 0.4779\n",
      "Epoch 14/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5232 - categorical_crossentropy: 1.5232 - acc: 0.50 - ETA: 0s - loss: 1.5317 - categorical_crossentropy: 1.5317 - acc: 0.48 - 0s 227us/step - loss: 1.5331 - categorical_crossentropy: 1.5331 - acc: 0.4796 - val_loss: 1.5262 - val_categorical_crossentropy: 1.5262 - val_acc: 0.5221\n",
      "Epoch 15/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5368 - categorical_crossentropy: 1.5368 - acc: 0.46 - ETA: 0s - loss: 1.5205 - categorical_crossentropy: 1.5205 - acc: 0.52 - 0s 211us/step - loss: 1.5198 - categorical_crossentropy: 1.5198 - acc: 0.5204 - val_loss: 1.5162 - val_categorical_crossentropy: 1.5162 - val_acc: 0.5588\n",
      "Epoch 16/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5130 - categorical_crossentropy: 1.5130 - acc: 0.53 - ETA: 0s - loss: 1.5132 - categorical_crossentropy: 1.5132 - acc: 0.49 - 0s 210us/step - loss: 1.5146 - categorical_crossentropy: 1.5146 - acc: 0.4907 - val_loss: 1.5050 - val_categorical_crossentropy: 1.5050 - val_acc: 0.5809\n",
      "Epoch 17/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5030 - categorical_crossentropy: 1.5030 - acc: 0.47 - ETA: 0s - loss: 1.4968 - categorical_crossentropy: 1.4968 - acc: 0.52 - 0s 215us/step - loss: 1.4970 - categorical_crossentropy: 1.4970 - acc: 0.5278 - val_loss: 1.4930 - val_categorical_crossentropy: 1.4930 - val_acc: 0.5809\n",
      "Epoch 18/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4949 - categorical_crossentropy: 1.4949 - acc: 0.50 - ETA: 0s - loss: 1.4825 - categorical_crossentropy: 1.4825 - acc: 0.56 - 0s 211us/step - loss: 1.4825 - categorical_crossentropy: 1.4825 - acc: 0.5630 - val_loss: 1.4799 - val_categorical_crossentropy: 1.4799 - val_acc: 0.5809\n",
      "Epoch 19/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4841 - categorical_crossentropy: 1.4841 - acc: 0.55 - ETA: 0s - loss: 1.4733 - categorical_crossentropy: 1.4733 - acc: 0.58 - 0s 262us/step - loss: 1.4721 - categorical_crossentropy: 1.4721 - acc: 0.5889 - val_loss: 1.4651 - val_categorical_crossentropy: 1.4651 - val_acc: 0.5882\n",
      "Epoch 20/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4745 - categorical_crossentropy: 1.4745 - acc: 0.55 - ETA: 0s - loss: 1.4542 - categorical_crossentropy: 1.4542 - acc: 0.57 - 0s 222us/step - loss: 1.4529 - categorical_crossentropy: 1.4529 - acc: 0.5722 - val_loss: 1.4491 - val_categorical_crossentropy: 1.4491 - val_acc: 0.5956\n",
      "Epoch 21/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4356 - categorical_crossentropy: 1.4356 - acc: 0.60 - ETA: 0s - loss: 1.4414 - categorical_crossentropy: 1.4414 - acc: 0.59 - 0s 286us/step - loss: 1.4394 - categorical_crossentropy: 1.4394 - acc: 0.6037 - val_loss: 1.4310 - val_categorical_crossentropy: 1.4310 - val_acc: 0.5956\n",
      "Epoch 22/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4272 - categorical_crossentropy: 1.4272 - acc: 0.67 - ETA: 0s - loss: 1.4207 - categorical_crossentropy: 1.4207 - acc: 0.63 - 0s 232us/step - loss: 1.4191 - categorical_crossentropy: 1.4191 - acc: 0.6352 - val_loss: 1.4116 - val_categorical_crossentropy: 1.4116 - val_acc: 0.5956\n",
      "Epoch 23/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4053 - categorical_crossentropy: 1.4053 - acc: 0.57 - ETA: 0s - loss: 1.3868 - categorical_crossentropy: 1.3868 - acc: 0.64 - 0s 229us/step - loss: 1.3885 - categorical_crossentropy: 1.3885 - acc: 0.6463 - val_loss: 1.3908 - val_categorical_crossentropy: 1.3908 - val_acc: 0.6103\n",
      "Epoch 24/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.3938 - categorical_crossentropy: 1.3938 - acc: 0.63 - ETA: 0s - loss: 1.3679 - categorical_crossentropy: 1.3679 - acc: 0.67 - 0s 212us/step - loss: 1.3636 - categorical_crossentropy: 1.3636 - acc: 0.6796 - val_loss: 1.3689 - val_categorical_crossentropy: 1.3689 - val_acc: 0.6103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.3604 - categorical_crossentropy: 1.3604 - acc: 0.64 - ETA: 0s - loss: 1.3413 - categorical_crossentropy: 1.3413 - acc: 0.65 - 0s 211us/step - loss: 1.3419 - categorical_crossentropy: 1.3419 - acc: 0.6463 - val_loss: 1.3451 - val_categorical_crossentropy: 1.3451 - val_acc: 0.6324\n",
      "Epoch 26/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.3307 - categorical_crossentropy: 1.3307 - acc: 0.62 - ETA: 0s - loss: 1.3107 - categorical_crossentropy: 1.3107 - acc: 0.65 - 0s 253us/step - loss: 1.3089 - categorical_crossentropy: 1.3089 - acc: 0.6648 - val_loss: 1.3204 - val_categorical_crossentropy: 1.3204 - val_acc: 0.6176\n",
      "Epoch 27/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.3175 - categorical_crossentropy: 1.3175 - acc: 0.70 - ETA: 0s - loss: 1.2882 - categorical_crossentropy: 1.2882 - acc: 0.66 - 0s 251us/step - loss: 1.2869 - categorical_crossentropy: 1.2869 - acc: 0.6611 - val_loss: 1.2946 - val_categorical_crossentropy: 1.2946 - val_acc: 0.6250\n",
      "Epoch 28/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.2269 - categorical_crossentropy: 1.2269 - acc: 0.69 - ETA: 0s - loss: 1.2364 - categorical_crossentropy: 1.2364 - acc: 0.67 - 0s 249us/step - loss: 1.2481 - categorical_crossentropy: 1.2481 - acc: 0.6611 - val_loss: 1.2677 - val_categorical_crossentropy: 1.2677 - val_acc: 0.6471\n",
      "Epoch 29/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.2274 - categorical_crossentropy: 1.2274 - acc: 0.71 - ETA: 0s - loss: 1.2160 - categorical_crossentropy: 1.2160 - acc: 0.68 - 0s 238us/step - loss: 1.2173 - categorical_crossentropy: 1.2173 - acc: 0.6833 - val_loss: 1.2403 - val_categorical_crossentropy: 1.2403 - val_acc: 0.6250\n",
      "Epoch 30/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1663 - categorical_crossentropy: 1.1663 - acc: 0.68 - ETA: 0s - loss: 1.1577 - categorical_crossentropy: 1.1577 - acc: 0.71 - 0s 268us/step - loss: 1.1662 - categorical_crossentropy: 1.1662 - acc: 0.6907 - val_loss: 1.2127 - val_categorical_crossentropy: 1.2127 - val_acc: 0.6176\n",
      "Epoch 31/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1791 - categorical_crossentropy: 1.1791 - acc: 0.67 - ETA: 0s - loss: 1.1381 - categorical_crossentropy: 1.1381 - acc: 0.70 - 0s 227us/step - loss: 1.1368 - categorical_crossentropy: 1.1368 - acc: 0.7000 - val_loss: 1.1844 - val_categorical_crossentropy: 1.1844 - val_acc: 0.6324\n",
      "Epoch 32/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1138 - categorical_crossentropy: 1.1138 - acc: 0.72 - ETA: 0s - loss: 1.1058 - categorical_crossentropy: 1.1058 - acc: 0.69 - 0s 238us/step - loss: 1.1049 - categorical_crossentropy: 1.1049 - acc: 0.6926 - val_loss: 1.1563 - val_categorical_crossentropy: 1.1563 - val_acc: 0.6691\n",
      "Epoch 33/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1086 - categorical_crossentropy: 1.1086 - acc: 0.69 - ETA: 0s - loss: 1.0814 - categorical_crossentropy: 1.0814 - acc: 0.70 - 0s 233us/step - loss: 1.0789 - categorical_crossentropy: 1.0789 - acc: 0.7111 - val_loss: 1.1291 - val_categorical_crossentropy: 1.1291 - val_acc: 0.6912\n",
      "Epoch 34/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1378 - categorical_crossentropy: 1.1378 - acc: 0.62 - ETA: 0s - loss: 1.0565 - categorical_crossentropy: 1.0565 - acc: 0.67 - 0s 240us/step - loss: 1.0428 - categorical_crossentropy: 1.0428 - acc: 0.7000 - val_loss: 1.1026 - val_categorical_crossentropy: 1.1026 - val_acc: 0.6985\n",
      "Epoch 35/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.9909 - categorical_crossentropy: 0.9909 - acc: 0.73 - ETA: 0s - loss: 0.9927 - categorical_crossentropy: 0.9927 - acc: 0.72 - 0s 268us/step - loss: 0.9924 - categorical_crossentropy: 0.9924 - acc: 0.7241 - val_loss: 1.0767 - val_categorical_crossentropy: 1.0767 - val_acc: 0.6985\n",
      "Epoch 36/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.9621 - categorical_crossentropy: 0.9621 - acc: 0.76 - ETA: 0s - loss: 0.9562 - categorical_crossentropy: 0.9562 - acc: 0.74 - 0s 235us/step - loss: 0.9444 - categorical_crossentropy: 0.9444 - acc: 0.7481 - val_loss: 1.0524 - val_categorical_crossentropy: 1.0524 - val_acc: 0.7059\n",
      "Epoch 37/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.9602 - categorical_crossentropy: 0.9602 - acc: 0.73 - ETA: 0s - loss: 0.9260 - categorical_crossentropy: 0.9260 - acc: 0.76 - 0s 222us/step - loss: 0.9247 - categorical_crossentropy: 0.9247 - acc: 0.7667 - val_loss: 1.0292 - val_categorical_crossentropy: 1.0292 - val_acc: 0.7059\n",
      "Epoch 38/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8959 - categorical_crossentropy: 0.8959 - acc: 0.78 - ETA: 0s - loss: 0.8804 - categorical_crossentropy: 0.8804 - acc: 0.78 - 0s 259us/step - loss: 0.8816 - categorical_crossentropy: 0.8816 - acc: 0.7889 - val_loss: 1.0079 - val_categorical_crossentropy: 1.0079 - val_acc: 0.7059\n",
      "Epoch 39/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8472 - categorical_crossentropy: 0.8472 - acc: 0.78 - ETA: 0s - loss: 0.8420 - categorical_crossentropy: 0.8420 - acc: 0.77 - 0s 255us/step - loss: 0.8444 - categorical_crossentropy: 0.8444 - acc: 0.7704 - val_loss: 0.9878 - val_categorical_crossentropy: 0.9878 - val_acc: 0.7059\n",
      "Epoch 40/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8040 - categorical_crossentropy: 0.8040 - acc: 0.78 - ETA: 0s - loss: 0.8129 - categorical_crossentropy: 0.8129 - acc: 0.79 - 0s 212us/step - loss: 0.8080 - categorical_crossentropy: 0.8080 - acc: 0.7889 - val_loss: 0.9688 - val_categorical_crossentropy: 0.9688 - val_acc: 0.7132\n",
      "Epoch 41/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8002 - categorical_crossentropy: 0.8002 - acc: 0.78 - ETA: 0s - loss: 0.7884 - categorical_crossentropy: 0.7884 - acc: 0.80 - 0s 246us/step - loss: 0.7810 - categorical_crossentropy: 0.7810 - acc: 0.8111 - val_loss: 0.9512 - val_categorical_crossentropy: 0.9512 - val_acc: 0.7206\n",
      "Epoch 42/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.7581 - categorical_crossentropy: 0.7581 - acc: 0.78 - ETA: 0s - loss: 0.7623 - categorical_crossentropy: 0.7623 - acc: 0.81 - 0s 290us/step - loss: 0.7539 - categorical_crossentropy: 0.7539 - acc: 0.8130 - val_loss: 0.9351 - val_categorical_crossentropy: 0.9351 - val_acc: 0.7206\n",
      "Epoch 43/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.7576 - categorical_crossentropy: 0.7576 - acc: 0.83 - ETA: 0s - loss: 0.7239 - categorical_crossentropy: 0.7239 - acc: 0.81 - 0s 214us/step - loss: 0.7251 - categorical_crossentropy: 0.7251 - acc: 0.8056 - val_loss: 0.9210 - val_categorical_crossentropy: 0.9210 - val_acc: 0.7206\n",
      "Epoch 44/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.7209 - categorical_crossentropy: 0.7209 - acc: 0.78 - ETA: 0s - loss: 0.7008 - categorical_crossentropy: 0.7008 - acc: 0.81 - 0s 236us/step - loss: 0.6958 - categorical_crossentropy: 0.6958 - acc: 0.8222 - val_loss: 0.9082 - val_categorical_crossentropy: 0.9082 - val_acc: 0.7279\n",
      "Epoch 45/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6650 - categorical_crossentropy: 0.6650 - acc: 0.83 - ETA: 0s - loss: 0.6517 - categorical_crossentropy: 0.6517 - acc: 0.83 - 0s 255us/step - loss: 0.6797 - categorical_crossentropy: 0.6797 - acc: 0.8241 - val_loss: 0.8955 - val_categorical_crossentropy: 0.8955 - val_acc: 0.7279\n",
      "Epoch 46/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6460 - categorical_crossentropy: 0.6460 - acc: 0.84 - ETA: 0s - loss: 0.6501 - categorical_crossentropy: 0.6501 - acc: 0.82 - 0s 227us/step - loss: 0.6449 - categorical_crossentropy: 0.6449 - acc: 0.8333 - val_loss: 0.8834 - val_categorical_crossentropy: 0.8834 - val_acc: 0.7353\n",
      "Epoch 47/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6121 - categorical_crossentropy: 0.6121 - acc: 0.82 - ETA: 0s - loss: 0.5953 - categorical_crossentropy: 0.5953 - acc: 0.84 - 0s 214us/step - loss: 0.5986 - categorical_crossentropy: 0.5986 - acc: 0.8463 - val_loss: 0.8719 - val_categorical_crossentropy: 0.8719 - val_acc: 0.7279\n",
      "Epoch 48/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6437 - categorical_crossentropy: 0.6437 - acc: 0.84 - ETA: 0s - loss: 0.5847 - categorical_crossentropy: 0.5847 - acc: 0.87 - 0s 212us/step - loss: 0.5935 - categorical_crossentropy: 0.5935 - acc: 0.8704 - val_loss: 0.8613 - val_categorical_crossentropy: 0.8613 - val_acc: 0.7279\n",
      "Epoch 49/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6180 - categorical_crossentropy: 0.6180 - acc: 0.82 - ETA: 0s - loss: 0.5485 - categorical_crossentropy: 0.5485 - acc: 0.86 - 0s 242us/step - loss: 0.5480 - categorical_crossentropy: 0.5480 - acc: 0.8722 - val_loss: 0.8512 - val_categorical_crossentropy: 0.8512 - val_acc: 0.7353\n",
      "Epoch 50/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.5452 - categorical_crossentropy: 0.5452 - acc: 0.85 - ETA: 0s - loss: 0.5304 - categorical_crossentropy: 0.5304 - acc: 0.87 - 0s 205us/step - loss: 0.5281 - categorical_crossentropy: 0.5281 - acc: 0.8796 - val_loss: 0.8430 - val_categorical_crossentropy: 0.8430 - val_acc: 0.7279\n",
      "Epoch 51/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.5222 - categorical_crossentropy: 0.5222 - acc: 0.89 - ETA: 0s - loss: 0.5159 - categorical_crossentropy: 0.5159 - acc: 0.89 - 0s 220us/step - loss: 0.5133 - categorical_crossentropy: 0.5133 - acc: 0.8944 - val_loss: 0.8358 - val_categorical_crossentropy: 0.8358 - val_acc: 0.7206\n",
      "Epoch 52/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4851 - categorical_crossentropy: 0.4851 - acc: 0.92 - ETA: 0s - loss: 0.4879 - categorical_crossentropy: 0.4879 - acc: 0.90 - 0s 273us/step - loss: 0.4904 - categorical_crossentropy: 0.4904 - acc: 0.9019 - val_loss: 0.8286 - val_categorical_crossentropy: 0.8286 - val_acc: 0.7206\n",
      "Epoch 53/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4522 - categorical_crossentropy: 0.4522 - acc: 0.89 - ETA: 0s - loss: 0.4381 - categorical_crossentropy: 0.4381 - acc: 0.91 - 0s 253us/step - loss: 0.4401 - categorical_crossentropy: 0.4401 - acc: 0.9111 - val_loss: 0.8226 - val_categorical_crossentropy: 0.8226 - val_acc: 0.7206\n",
      "Epoch 54/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4576 - categorical_crossentropy: 0.4576 - acc: 0.88 - ETA: 0s - loss: 0.4492 - categorical_crossentropy: 0.4492 - acc: 0.90 - 0s 233us/step - loss: 0.4524 - categorical_crossentropy: 0.4524 - acc: 0.9056 - val_loss: 0.8161 - val_categorical_crossentropy: 0.8161 - val_acc: 0.7279\n",
      "Epoch 55/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4527 - categorical_crossentropy: 0.4527 - acc: 0.91 - ETA: 0s - loss: 0.4227 - categorical_crossentropy: 0.4227 - acc: 0.91 - 0s 225us/step - loss: 0.4288 - categorical_crossentropy: 0.4288 - acc: 0.9167 - val_loss: 0.8108 - val_categorical_crossentropy: 0.8108 - val_acc: 0.7279\n",
      "Epoch 56/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4174 - categorical_crossentropy: 0.4174 - acc: 0.90 - ETA: 0s - loss: 0.3984 - categorical_crossentropy: 0.3984 - acc: 0.93 - 0s 268us/step - loss: 0.3867 - categorical_crossentropy: 0.3867 - acc: 0.9296 - val_loss: 0.8059 - val_categorical_crossentropy: 0.8059 - val_acc: 0.7353\n",
      "Epoch 57/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3480 - categorical_crossentropy: 0.3480 - acc: 0.92 - ETA: 0s - loss: 0.3830 - categorical_crossentropy: 0.3830 - acc: 0.92 - 0s 240us/step - loss: 0.3841 - categorical_crossentropy: 0.3841 - acc: 0.9259 - val_loss: 0.8012 - val_categorical_crossentropy: 0.8012 - val_acc: 0.7426\n",
      "Epoch 58/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3918 - categorical_crossentropy: 0.3918 - acc: 0.93 - ETA: 0s - loss: 0.3607 - categorical_crossentropy: 0.3607 - acc: 0.93 - 0s 227us/step - loss: 0.3522 - categorical_crossentropy: 0.3522 - acc: 0.9407 - val_loss: 0.7961 - val_categorical_crossentropy: 0.7961 - val_acc: 0.7426\n",
      "Epoch 59/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3801 - categorical_crossentropy: 0.3801 - acc: 0.94 - ETA: 0s - loss: 0.3615 - categorical_crossentropy: 0.3615 - acc: 0.93 - 0s 226us/step - loss: 0.3628 - categorical_crossentropy: 0.3628 - acc: 0.9315 - val_loss: 0.7927 - val_categorical_crossentropy: 0.7927 - val_acc: 0.7353\n",
      "Epoch 60/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3477 - categorical_crossentropy: 0.3477 - acc: 0.92 - ETA: 0s - loss: 0.3365 - categorical_crossentropy: 0.3365 - acc: 0.93 - 0s 240us/step - loss: 0.3245 - categorical_crossentropy: 0.3245 - acc: 0.9444 - val_loss: 0.7910 - val_categorical_crossentropy: 0.7910 - val_acc: 0.7426\n",
      "Epoch 61/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3381 - categorical_crossentropy: 0.3381 - acc: 0.92 - ETA: 0s - loss: 0.3239 - categorical_crossentropy: 0.3239 - acc: 0.93 - ETA: 0s - loss: 0.3175 - categorical_crossentropy: 0.3175 - acc: 0.93 - 0s 386us/step - loss: 0.3208 - categorical_crossentropy: 0.3208 - acc: 0.9315 - val_loss: 0.7895 - val_categorical_crossentropy: 0.7895 - val_acc: 0.7353\n",
      "Epoch 62/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2629 - categorical_crossentropy: 0.2629 - acc: 0.96 - ETA: 0s - loss: 0.3089 - categorical_crossentropy: 0.3089 - acc: 0.93 - 0s 259us/step - loss: 0.3070 - categorical_crossentropy: 0.3070 - acc: 0.9352 - val_loss: 0.7865 - val_categorical_crossentropy: 0.7865 - val_acc: 0.7279\n",
      "Epoch 63/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2866 - categorical_crossentropy: 0.2866 - acc: 0.93 - ETA: 0s - loss: 0.2829 - categorical_crossentropy: 0.2829 - acc: 0.94 - 0s 233us/step - loss: 0.2835 - categorical_crossentropy: 0.2835 - acc: 0.9444 - val_loss: 0.7811 - val_categorical_crossentropy: 0.7811 - val_acc: 0.7353\n",
      "Epoch 64/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2913 - categorical_crossentropy: 0.2913 - acc: 0.94 - ETA: 0s - loss: 0.2764 - categorical_crossentropy: 0.2764 - acc: 0.95 - 0s 223us/step - loss: 0.2767 - categorical_crossentropy: 0.2767 - acc: 0.9519 - val_loss: 0.7754 - val_categorical_crossentropy: 0.7754 - val_acc: 0.7353\n",
      "Epoch 65/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2134 - categorical_crossentropy: 0.2134 - acc: 1.00 - ETA: 0s - loss: 0.2378 - categorical_crossentropy: 0.2378 - acc: 0.97 - 0s 251us/step - loss: 0.2458 - categorical_crossentropy: 0.2458 - acc: 0.9741 - val_loss: 0.7720 - val_categorical_crossentropy: 0.7720 - val_acc: 0.7279\n",
      "Epoch 66/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2313 - categorical_crossentropy: 0.2313 - acc: 0.98 - ETA: 0s - loss: 0.2323 - categorical_crossentropy: 0.2323 - acc: 0.96 - 0s 214us/step - loss: 0.2324 - categorical_crossentropy: 0.2324 - acc: 0.9648 - val_loss: 0.7699 - val_categorical_crossentropy: 0.7699 - val_acc: 0.7353\n",
      "Epoch 67/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2378 - categorical_crossentropy: 0.2378 - acc: 0.96 - ETA: 0s - loss: 0.2405 - categorical_crossentropy: 0.2405 - acc: 0.96 - 0s 209us/step - loss: 0.2408 - categorical_crossentropy: 0.2408 - acc: 0.9593 - val_loss: 0.7700 - val_categorical_crossentropy: 0.7700 - val_acc: 0.7353\n",
      "Epoch 68/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1967 - categorical_crossentropy: 0.1967 - acc: 0.96 - ETA: 0s - loss: 0.1968 - categorical_crossentropy: 0.1968 - acc: 0.98 - 0s 223us/step - loss: 0.1964 - categorical_crossentropy: 0.1964 - acc: 0.9815 - val_loss: 0.7720 - val_categorical_crossentropy: 0.7720 - val_acc: 0.7353\n",
      "Epoch 69/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1876 - categorical_crossentropy: 0.1876 - acc: 0.97 - ETA: 0s - loss: 0.2091 - categorical_crossentropy: 0.2091 - acc: 0.96 - 0s 222us/step - loss: 0.2087 - categorical_crossentropy: 0.2087 - acc: 0.9685 - val_loss: 0.7720 - val_categorical_crossentropy: 0.7720 - val_acc: 0.7353\n",
      "Epoch 70/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1971 - categorical_crossentropy: 0.1971 - acc: 0.98 - ETA: 0s - loss: 0.1990 - categorical_crossentropy: 0.1990 - acc: 0.97 - 0s 235us/step - loss: 0.1964 - categorical_crossentropy: 0.1964 - acc: 0.9778 - val_loss: 0.7701 - val_categorical_crossentropy: 0.7701 - val_acc: 0.7279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2033 - categorical_crossentropy: 0.2033 - acc: 0.96 - ETA: 0s - loss: 0.1971 - categorical_crossentropy: 0.1971 - acc: 0.97 - 0s 225us/step - loss: 0.1939 - categorical_crossentropy: 0.1939 - acc: 0.9741 - val_loss: 0.7657 - val_categorical_crossentropy: 0.7657 - val_acc: 0.7353\n",
      "Epoch 72/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1484 - categorical_crossentropy: 0.1484 - acc: 0.99 - ETA: 0s - loss: 0.1810 - categorical_crossentropy: 0.1810 - acc: 0.98 - 0s 251us/step - loss: 0.1847 - categorical_crossentropy: 0.1847 - acc: 0.9796 - val_loss: 0.7628 - val_categorical_crossentropy: 0.7628 - val_acc: 0.7206\n",
      "Epoch 73/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1740 - categorical_crossentropy: 0.1740 - acc: 0.99 - ETA: 0s - loss: 0.1578 - categorical_crossentropy: 0.1578 - acc: 0.98 - 0s 201us/step - loss: 0.1590 - categorical_crossentropy: 0.1590 - acc: 0.9852 - val_loss: 0.7647 - val_categorical_crossentropy: 0.7647 - val_acc: 0.7206\n",
      "Epoch 74/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1411 - categorical_crossentropy: 0.1411 - acc: 0.97 - ETA: 0s - loss: 0.1498 - categorical_crossentropy: 0.1498 - acc: 0.97 - 0s 225us/step - loss: 0.1523 - categorical_crossentropy: 0.1523 - acc: 0.9759 - val_loss: 0.7652 - val_categorical_crossentropy: 0.7652 - val_acc: 0.7206\n",
      "Epoch 75/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1941 - categorical_crossentropy: 0.1941 - acc: 0.96 - ETA: 0s - loss: 0.1692 - categorical_crossentropy: 0.1692 - acc: 0.96 - 0s 283us/step - loss: 0.1664 - categorical_crossentropy: 0.1664 - acc: 0.9685 - val_loss: 0.7630 - val_categorical_crossentropy: 0.7630 - val_acc: 0.7206\n",
      "Epoch 76/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1401 - categorical_crossentropy: 0.1401 - acc: 0.98 - ETA: 0s - loss: 0.1605 - categorical_crossentropy: 0.1605 - acc: 0.96 - 0s 271us/step - loss: 0.1605 - categorical_crossentropy: 0.1605 - acc: 0.9704 - val_loss: 0.7634 - val_categorical_crossentropy: 0.7634 - val_acc: 0.7132\n",
      "Epoch 77/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1512 - categorical_crossentropy: 0.1512 - acc: 0.97 - ETA: 0s - loss: 0.1378 - categorical_crossentropy: 0.1378 - acc: 0.98 - 0s 235us/step - loss: 0.1382 - categorical_crossentropy: 0.1382 - acc: 0.9815 - val_loss: 0.7651 - val_categorical_crossentropy: 0.7651 - val_acc: 0.7132\n",
      "Epoch 78/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1174 - categorical_crossentropy: 0.1174 - acc: 1.00 - ETA: 0s - loss: 0.1280 - categorical_crossentropy: 0.1280 - acc: 0.99 - 0s 227us/step - loss: 0.1301 - categorical_crossentropy: 0.1301 - acc: 0.9926 - val_loss: 0.7681 - val_categorical_crossentropy: 0.7681 - val_acc: 0.7206\n",
      "Epoch 79/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1402 - categorical_crossentropy: 0.1402 - acc: 0.98 - ETA: 0s - loss: 0.1271 - categorical_crossentropy: 0.1271 - acc: 0.99 - 0s 225us/step - loss: 0.1267 - categorical_crossentropy: 0.1267 - acc: 0.9926 - val_loss: 0.7680 - val_categorical_crossentropy: 0.7680 - val_acc: 0.7206\n",
      "Epoch 80/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1049 - categorical_crossentropy: 0.1049 - acc: 0.99 - ETA: 0s - loss: 0.1054 - categorical_crossentropy: 0.1054 - acc: 0.99 - 0s 253us/step - loss: 0.1052 - categorical_crossentropy: 0.1052 - acc: 0.9926 - val_loss: 0.7674 - val_categorical_crossentropy: 0.7674 - val_acc: 0.7206\n",
      "Epoch 81/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1215 - categorical_crossentropy: 0.1215 - acc: 0.99 - ETA: 0s - loss: 0.1110 - categorical_crossentropy: 0.1110 - acc: 0.99 - 0s 250us/step - loss: 0.1131 - categorical_crossentropy: 0.1131 - acc: 0.9926 - val_loss: 0.7666 - val_categorical_crossentropy: 0.7666 - val_acc: 0.7353\n",
      "Epoch 82/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1104 - categorical_crossentropy: 0.1104 - acc: 0.99 - ETA: 0s - loss: 0.1148 - categorical_crossentropy: 0.1148 - acc: 0.98 - 0s 224us/step - loss: 0.1148 - categorical_crossentropy: 0.1148 - acc: 0.9889 - val_loss: 0.7666 - val_categorical_crossentropy: 0.7666 - val_acc: 0.7426\n",
      "Finished training.\n",
      "------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 676)               0         \n",
      "_________________________________________________________________\n",
      "Hidden-1 (Dense)             (None, 1000)              677000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "Hidden-2 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 1,180,005\n",
      "Trainable params: 1,180,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 5\n",
    "epochs = 200\n",
    "optimizer = Adam(lr=0.0002)\n",
    "\n",
    "model_MLP = MLP_model(\n",
    "    optimizer=optimizer,\n",
    "    input_size=X_tra.shape[1],\n",
    "    dropout_rate = 0.7,\n",
    "    hidden_layers=2,\n",
    "    units=1000,\n",
    "    funnel=True\n",
    ")\n",
    "\n",
    "# Keras Callbacks\n",
    "reducer_lr = ReduceLROnPlateau(factor = 0.00002, patience = 10, min_lr = 1e-6, verbose = -1)\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_categorical_crossentropy', mode='min', patience = 10) # Change 4 to 8 in the final run\n",
    "model_file_name = 'tfidf_mlp'\n",
    "check_pointer = keras.callbacks.ModelCheckpoint(model_file_name, monitor='val_categorical_crossentropy', mode='min', verbose = -1, save_best_only = True)  \n",
    "callbacks_list = [early_stopper, reducer_lr, check_pointer]#\n",
    "\n",
    "model_MLP.fit(x=X_tra,\n",
    "              y=y_tra,          \n",
    "              validation_data=(X_val, y_val), \n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              batch_size=batch_size,\n",
    "              callbacks = callbacks_list)\n",
    "\n",
    "# y_test_pred = model_MLP.predict(test_features.tocsr(), batch_size=batch_size)\n",
    "print('Finished training.')\n",
    "print('------------------')    \n",
    "model_MLP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41084"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "del model_MLP\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6817416127205693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "temp_pred = model_MLP.predict(temp_x, batch_size=batch_size)\n",
    "print(log_loss(temp_y, temp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9868421052631579\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_va_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-db35b6ba2228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tra_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_va_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_va_class' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xgb = XGBClassifier(objective ='multi:softprob', learning_rate = 0.01,\n",
    "                     max_depth = 5, alpha = 10, n_estimators = 100,\n",
    "                     eval_metris='mlogloss',\n",
    "                     reg_lambda=0.8\n",
    "                     )\n",
    "\n",
    "xgb.fit(X_tra, y_tra_class)\n",
    "y_pred_train = xgb.predict_proba(X_tra)\n",
    "y_pred_val = xgb.predict_proba(X_val)        \n",
    "\n",
    "print(xgb.score(X_tra, y_tra_class))\n",
    "print(xgb.score(X_val, y_va_class))\n",
    "\n",
    "print('Loss')\n",
    "print(log_loss(y_tra_class, y_pred_train))\n",
    "print(log_loss(y_val_class, y_pred_val))\n",
    "\n",
    "print(classification_report(y_val_class, np.argmax(y_pred, axis=1)))\n",
    "print('Multi-Class Log-loss: ',log_loss(y_val_class, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x214fc8be9b0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "test_classes = {0:'athlitismos', 1:'diaskedasi-psyxagogia', 2:'eidiseis-mme',\n",
    "       3:'katastimata-agores', 4:'pliroforiki-diadiktyo'}\n",
    "results_val = [test_classes[i] for i in np.argmax(temp_pred,axis=1)]\n",
    "true_val = [test_classes[i] for i in np.argmax(temp_y,axis=1)]\n",
    "cm = confusion_matrix(true_val,results_val)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_T = model_MLP.predict(X_tra)\n",
    "y_pred_val_T = model_MLP.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_T = model_MLP.predict(test_features.tocsr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval of most similar document using the Weisfeiler-Lehman subtree kernel.<a id='gk'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GraKeL graphs for train and test documents.\n",
    "We use the tokenized documents that we serialized in the start of the notebook.\n",
    "\n",
    "**We process only nodes that have text!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word co-occurrence networks\n",
      " for train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [00:46, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word co-occurrence networks\n",
      " for test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:15, 12.55it/s]\n"
     ]
    }
   ],
   "source": [
    "y_train_grakel = []\n",
    "y_train_grakel_idx = []\n",
    "print(\"Creating word co-occurrence networks\\n for train data\")\n",
    "word_networks = list()\n",
    "for idx, doc in tqdm(enumerate(train_data)):\n",
    "    node_labels = dict()\n",
    "    tokens_to_ids = dict()\n",
    "    for token in doc:\n",
    "        if token not in tokens_to_ids:\n",
    "            tokens_to_ids[token] = len(tokens_to_ids)\n",
    "            node_labels[tokens_to_ids[token]] = token\n",
    "    \n",
    "    edges = list()\n",
    "    for i in range(len(doc)-1):\n",
    "        edges.append((tokens_to_ids[doc[i]], tokens_to_ids[doc[i+1]]))\n",
    "         \n",
    "    try:        \n",
    "        word_networks.append(Graph(edges, node_labels=node_labels))\n",
    "        y_train_grakel.append(y_train[idx])\n",
    "        y_train_grakel_idx.append(idx)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Creating word co-occurrence networks\\n for test data\")\n",
    "word_networks_test = list()\n",
    "test_ids = []\n",
    "for idx, doc in tqdm(enumerate(test_data)):\n",
    "    node_labels = dict()\n",
    "    tokens_to_ids = dict()\n",
    "    for token in doc:\n",
    "        if token not in tokens_to_ids:\n",
    "            tokens_to_ids[token] = len(tokens_to_ids)\n",
    "            node_labels[tokens_to_ids[token]] = token\n",
    "    \n",
    "    edges = list()\n",
    "    for i in range(len(doc)-1):\n",
    "        edges.append((tokens_to_ids[doc[i]], tokens_to_ids[doc[i+1]]))\n",
    "         \n",
    "    try:        \n",
    "        word_networks_test.append(Graph(edges, node_labels=node_labels))\n",
    "        test_ids.append(idx)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sums the similarities of the top N (20 in our case) closest documents for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct solution:\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    \n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def get_categories_prob(K, y_train_grakel, idx):\n",
    "    K[idx, 0] = 0\n",
    "    similarity_values = np.take(K[:, 0], np.argsort(K[:,0])[-20:]) # 1x10\n",
    "    similarity_category = np.take(y_train_grakel, np.argsort(K[:,0])[-20:], axis=0) # 10x5+\n",
    "    \n",
    "    for idx, similarity in enumerate(similarity_values):\n",
    "        \n",
    "        similarity_category[idx] = similarity_category[idx] * similarity\n",
    "\n",
    "    similarity_category = np.array(np.sum(similarity_category, axis=0))\n",
    "       \n",
    "    category_prob = softmax(similarity_category)\n",
    "    \n",
    "    return similarity_category, category_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gk = WeisfeilerLehman(niter=2, normalize=True, base_kernel=VertexHistogram)\n",
    "#gk = GraphKernel(kernel = [{\"name\": \"weisfeiler_lehman\", \"niter\": 5}, {\"name\": \"subtree_wl\"}], Nystroem=20)\n",
    "\n",
    "# Train documents\n",
    "serialized_docs_train_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/train/'\n",
    "results_train = []\n",
    "for idx, train_graph in tqdm(enumerate(word_networks)):\n",
    "    gk.fit([train_graph])\n",
    "    K = gk.transform(word_networks)    \n",
    "    cat_sum, cat_prob = get_categories_prob(K, y_train_grakel, idx)    \n",
    "        \n",
    "    res = {\n",
    "        'doc_id': y_train_grakel_idx[idx],\n",
    "        'y': y_train_grakel[idx],\n",
    "        'cat_sum': cat_sum,\n",
    "        'cat_prob': cat_prob\n",
    "    }\n",
    "    print()\n",
    "    print(res)\n",
    "        \n",
    "    pickle_out = open(serialized_docs_train_dir + 'dict.pickle_' + str(idx),\"wb\")\n",
    "    pickle.dump(res, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    results_train.append(res)\n",
    "    \n",
    "# Test documents\n",
    "serialized_docs_test_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/test/'\n",
    "results_test = []\n",
    "for idx, test_graph in tqdm(enumerate(word_networks_test)):\n",
    "    gk.fit([test_graph])\n",
    "    K = gk.transform(word_networks)\n",
    "    \n",
    "    cat_sum, cat_prob = get_categories_prob(K, y_train_grakel)\n",
    "    res = {\n",
    "        'doc_id': test_ids[idx],\n",
    "        'true': y_test[test_ids[idx]],\n",
    "        'cat_sum': cat_sum,\n",
    "        'cat_prob': cat_prob\n",
    "    }\n",
    "    \n",
    "    pickle_out = open(serialized_docs_test_dir + 'dict.pickle_' + str(idx),\"wb\")\n",
    "    pickle.dump(res, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    results_test.append(res)\n",
    "\n",
    "# MEMORY OVERLOAD\n",
    "# from grakel import GraphKernel\n",
    "# gk = WeisfeilerLehman(niter=2, normalize=True, base_kernel=VertexHistogram, n_jobs=-1)\n",
    "# #gk = GraphKernel(kernel = [{\"name\": \"weisfeiler_lehman\", \"niter\": 5}, {\"name\": \"subtree_wl\"}], Nystroem=20)\n",
    "\n",
    "\n",
    "# Κ = gk.fit_transform(word_networks)\n",
    "# K = gk.transform(word_networks_test)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the WL results calculates above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "serialized_docs_test_WL_results_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/train/'\n",
    "filenames = os.listdir(serialized_docs_test_WL_results_dir)\n",
    "\n",
    "train_results = []\n",
    "for file in filenames:\n",
    "    with open(serialized_docs_test_WL_results_dir + file, 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        train_results.append(doc)              \n",
    "\n",
    "serialized_docs_test_WL_results_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/test/'\n",
    "filenames = os.listdir(serialized_docs_test_WL_results_dir)\n",
    "\n",
    "test_results = []\n",
    "for file in filenames:\n",
    "    with open(serialized_docs_test_WL_results_dir + file, 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        test_results.append(doc)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = []\n",
    "y_train_class = []\n",
    "y_train_dum = []\n",
    "train_probs = []\n",
    "for doc in train_results:\n",
    "    x_train.append(doc['cat_sum'])\n",
    "    train_probs.append(doc['cat_prob'])\n",
    "    y_train_class.append(np.where(doc['y']==1)[0][0])\n",
    "    y_train_dum.append(doc['y'])\n",
    "    \n",
    "x_test = np.zeros((200,5))\n",
    "x_test_prob = np.zeros((200,5))\n",
    "for doc in test_results:\n",
    "    x_test[doc['doc_id']] = doc['cat_sum']\n",
    "    x_test_prob[doc['doc_id']] = doc['cat_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a75707e88087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'log_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_train_class, train_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6a80df7aa36a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_indicies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m675\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_tra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_indicies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "x_indicies = list(range(0, 675))\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_indicies, y_train_class, train_size=0.90, random_state=3, stratify=y_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra_WL = np.take(x_train, X_tra, axis=0)\n",
    "X_val_WL = np.take(x_train, X_val, axis=0)\n",
    "\n",
    "y_tra_WL = np.take(y_train_dum ,X_tra, axis=0)\n",
    "y_val_WL = np.take(y_train_dum ,X_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0038, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=2, shrinking=True, tol=0.001,\n",
       "  verbose=False)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(probability=True, C=1, kernel='rbf', gamma=0.0038, random_state=2\n",
    "         )\n",
    "svc.fit(X_tra_WL, y_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_WL = svc.predict_proba(X_tra_WL)\n",
    "y_pred_val_WL = svc.predict_proba(X_val_WL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WL res:\n",
      "0.19232843395578017\n",
      "0.18366625487638547\n"
     ]
    }
   ],
   "source": [
    "print('WL res:')\n",
    "print(log_loss(y_tra_WL, y_pred_train_WL))\n",
    "print(log_loss(y_val_WL, y_pred_val_WL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_WL = svc.predict_proba(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
