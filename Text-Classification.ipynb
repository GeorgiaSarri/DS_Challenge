{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "- Read Text\n",
    "- Feature Engineering(KeyWord Extraction, Embeddings)\n",
    "- MLP\n",
    "- CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 1.12.0\n",
      "Keras Version: 2.2.4\n",
      "\n",
      "Python 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "Gensim 3.7.3\n",
      "Seaborn 0.9.0\n",
      "Pandas 0.24.1\n",
      "Scikit-Learn 0.20.2\n",
      "Your CPU is: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\n",
      "Your CPU has 8 cores.\n",
      "Total memory: 15.9GiB\n",
      "svmem(total=17034620928, available=7049601024, percent=58.6, used=9985019904, free=7049601024)\n",
      "Installed GPUs:\n",
      "/device:GPU:0 - device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download el_core_news_sm\n",
    "# !python -m spacy download el_core_news_md\n",
    "from __future__ import print_function\n",
    "print(__doc__)\n",
    "import re\n",
    "\n",
    "import el_core_news_md\n",
    "nlp = el_core_news_md.load()\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.max_length = 93621305\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv \n",
    "import gc\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import Dropout,Dense,Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import multiprocessing\n",
    "import platform\n",
    "import psutil\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from random import randint\n",
    "from gensim.models import Word2Vec\n",
    "sys.path.insert(0,'C:/Users/User/Desktop/DS_challenge/')\n",
    "from GraphEmbedding.ge import DeepWalk\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
    "from grakel import Graph\n",
    "from grakel import GraphKernel\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import sentence_polarity\n",
    "# nltk.download('sentence_polarity') \n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Gensim {gensim.__version__}\")\n",
    "print(f\"Seaborn {sns.__version__}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "p = platform.processor()\n",
    "mem = psutil.virtual_memory()\n",
    "\n",
    "print(f'Your CPU is: {p}')\n",
    "print(f'Your CPU has {multiprocessing.cpu_count()} cores.')\n",
    "print(f'Total memory: {sizeof_fmt(mem.total)}')\n",
    "print(mem)\n",
    "\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "gpuList = [x for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(\"Installed GPUs:\")\n",
    "for x in gpuList:\n",
    "    print(\"{} - {}\".format(x.name,x.physical_device_desc))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntext = dict()\\nfilenames = os.listdir(domains_path)\\nfor filename in filenames:\\n    if filename[-4:] == '.zip':\\n        z = zipfile.ZipFile(domains_path + filename)\\n        contents = z.namelist()\\n        #text[filename[:-4]] = [] \\n        text_list = []\\n        for c in contents:\\n            f = z.open(c)\\n            data = f.read()\\n            text_list.append(data.decode('utf16')) #for windows: latin1\\n            f.close()\\n        text[filename[:-4]] = text_list\\n            \\n# Read webpages of the training set\\ntrain_data = list()\\nfor host in train_hosts:\\n    if host in text:\\n        train_data.append(text[host])\\n    else:\\n        train_data.append('')\\n\\n# Read webpages of the test set\\ntest_data = list()\\nfor host in test_hosts:\\n    if host in text:\\n        test_data.append(text[host])\\n    else:\\n        test_data.append('')\\n        \\ndel text\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = r'C:\\Users\\User\\Desktop\\DS_challenge\\data\\\\'\n",
    "TRAIN_FILE = dir_path + \"train.csv\"\n",
    "TEST_FILE = dir_path + \"test.csv\"\n",
    "GRAPH_FILE = dir_path + \"edgelist.txt\"\n",
    "domains_path = 'C:/Users/User/Desktop/DS_challenge/data/data_science_challenge_2019/domains/'\n",
    "\n",
    "class_labels = {'athlitismos':0, 'diaskedasi-psyxagogia':1, 'eidiseis-mme':2, 'katastimata-agores':3, 'pliroforiki-diadiktyo':4}\n",
    "\n",
    "train_hosts = list()\n",
    "train_labels = list()\n",
    "with open(TRAIN_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        train_hosts.append(l[0])\n",
    "        train_labels.append(l[1][:-1])\n",
    "\n",
    "y_train_class = []\n",
    "y_train_dum = np.zeros((len(train_hosts), len(class_labels)))\n",
    "for i, train_label in enumerate(train_labels):\n",
    "    y_train_dum[i,class_labels[train_label]] = 1\n",
    "    y_train_class.append(class_labels[train_label])\n",
    "\n",
    "test_hosts = list()\n",
    "with open(TEST_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        test_hosts.append(l[0])\n",
    "'''\n",
    "text = dict()\n",
    "filenames = os.listdir(domains_path)\n",
    "for filename in filenames:\n",
    "    if filename[-4:] == '.zip':\n",
    "        z = zipfile.ZipFile(domains_path + filename)\n",
    "        contents = z.namelist()\n",
    "        #text[filename[:-4]] = [] \n",
    "        text_list = []\n",
    "        for c in contents:\n",
    "            f = z.open(c)\n",
    "            data = f.read()\n",
    "            text_list.append(data.decode('utf16')) #for windows: latin1\n",
    "            f.close()\n",
    "        text[filename[:-4]] = text_list\n",
    "            \n",
    "# Read webpages of the training set\n",
    "train_data = list()\n",
    "for host in train_hosts:\n",
    "    if host in text:\n",
    "        train_data.append(text[host])\n",
    "    else:\n",
    "        train_data.append('')\n",
    "\n",
    "# Read webpages of the test set\n",
    "test_data = list()\n",
    "for host in test_hosts:\n",
    "    if host in text:\n",
    "        test_data.append(text[host])\n",
    "    else:\n",
    "        test_data.append('')\n",
    "        \n",
    "del text\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess all the texts.\n",
    "- remove special symbols letters\n",
    "- transform to lower case\n",
    "- remove links etc.\n",
    "- lemmatize the tokens as an option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, lemma=True):\n",
    "    string = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"[^άέήίόύώα-ωa-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if lemma==True:\n",
    "        string = nlp(string)\n",
    "        string = [token.lemma_ for token in string ] #if ('\\n' not in token.lemma_)]\n",
    "    else:\n",
    "        string = string.strip().split()        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this preprocessing function to each document and we store the results to pickle objects so we do not have to rerun it each time.\n",
    "<br>\n",
    "We create 3 different types of tokenized documents.\n",
    "- 5000 lemmatized tokens equally sampled from or urls of a website(in order to use it in CNNs)\n",
    "- full text lemmatized tokens\n",
    "- full text without lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:20<00:00, 38.35it/s]\n",
      "100%|██████████| 200/200 [00:06<00:00, 30.44it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "serialized_docs_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/'\n",
    "\n",
    "max_size = 5000\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    num_urls = len(train_data[i])       \n",
    "    \n",
    "    doc_clean = []\n",
    "    doc_concat = []\n",
    "    if num_urls != 0:\n",
    "        num_tokens_per_url = int(max_size/num_urls)\n",
    "        \n",
    "        for url in train_data[i]:\n",
    "            doc_clean.append(clean_str(url, lemma=False))\n",
    "            \n",
    "        remain_tokens = 0\n",
    "        max_doc_len_id = -1\n",
    "        max_length = 0\n",
    "        for idx, d in enumerate(doc_clean):  \n",
    "            if len(d)>max_length:\n",
    "                max_length = len(d)\n",
    "                max_doc_len_id = idx\n",
    "                \n",
    "            if len(d) > (num_tokens_per_url + remain_tokens):\n",
    "                n_tok = remain_tokens + num_tokens_per_url\n",
    "                doc_concat += d[:n_tok]\n",
    "                remain_tokens = 0\n",
    "            else:\n",
    "                remain_tokens += (num_tokens_per_url - len(d))\n",
    "                doc_concat += d\n",
    "        try:\n",
    "            more_tokens = (max_size - len(doc_concat))\n",
    "            doc_concat += doc_clean[max_doc_len_id][-more_tokens:]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    # All tokens\n",
    "    doc_dict = {\n",
    "        'y': y_train_dum[i],\n",
    "        'x_list': doc_concat\n",
    "    } \n",
    "    pickle_out = open(serialized_docs_dir + 'train_mix/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    pickle.dump(doc_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "for i in tqdm(range(len(test_data))):\n",
    "    num_urls = len(test_data[i])       \n",
    "    \n",
    "    doc_clean = []\n",
    "    doc_concat = []\n",
    "    if num_urls != 0:\n",
    "        num_tokens_per_url = int(max_size/num_urls)\n",
    "        \n",
    "        for url in test_data[i]:\n",
    "            doc_clean.append(clean_str(url, lemma=False))\n",
    "        \n",
    "        remain_tokens = 0\n",
    "        max_doc_len_id = -1\n",
    "        max_length = 0\n",
    "        for idx, d in enumerate(doc_clean):  \n",
    "            if len(d)>max_length:\n",
    "                max_length = len(d)\n",
    "                max_doc_len_id = idx\n",
    "                \n",
    "            if (len(d)) > (num_tokens_per_url + remain_tokens):\n",
    "                n_tok = remain_tokens + num_tokens_per_url\n",
    "                doc_concat += d[:n_tok]\n",
    "                remain_tokens = 0\n",
    "            else:\n",
    "                remain_tokens += (num_tokens_per_url - len(d))\n",
    "                doc_concat += d\n",
    "        try:\n",
    "            more_tokens = (max_size - len(doc_concat))\n",
    "            doc_concat += doc_clean[max_doc_len_id][-more_tokens:]            \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    # All tokens\n",
    "    doc_dict = {        \n",
    "        'x_list': doc_concat\n",
    "    } \n",
    "    pickle_out = open(serialized_docs_dir + 'test_mix/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    pickle.dump(doc_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "for i in tqdm(range(len(train_data))):    \n",
    "    doc_concat = []\n",
    "    for url in train_data[i]:\n",
    "        doc_concat += clean_str(url, lemma=False)\n",
    "    #d_lemma = clean_str(train_data[i], lemma=True)  \n",
    "    \n",
    "    # All tokens\n",
    "    doc_dict = {\n",
    "        'y': y_train_dum[i],\n",
    "        'x_list': doc_concat\n",
    "    } \n",
    "    pickle_out = open(serialized_docs_dir + 'train_all/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    pickle.dump(doc_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    # All tokens lemma\n",
    "    #doc_dict_lemma = {\n",
    "    #    'y': y_train[i],\n",
    "    #    'x_list': d_lemma\n",
    "    #} \n",
    "    #pickle_out = open(serialized_docs_dir + 'train_all_lemma/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    #pickle.dump(doc_dict_lemma, pickle_out)\n",
    "    #pickle_out.close()\n",
    "    \n",
    "    # First 5000 tokens no lemma\n",
    "    #if len(d_lemma) > 5000:\n",
    "    #    doc_dict_lemma_5000 = {\n",
    "    #        'y': y_train[i],\n",
    "    #        'x_list': d_lemma[:5000]\n",
    "    #    } \n",
    "    #else:\n",
    "    #    doc_dict_lemma_5000 = {\n",
    "    #        'y': y_train[i],\n",
    "    #        'x_list': d_lemma\n",
    "    #    }    \n",
    "    \n",
    "    #pickle_out = open(serialized_docs_dir + 'train/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    #pickle.dump(doc_dict_lemma_5000, pickle_out)\n",
    "    #pickle_out.close()\n",
    "\n",
    "# Same for the test documents\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    doc_concat = []\n",
    "    for url in test_data[i]:\n",
    "        doc_concat += clean_str(url, lemma=False)\n",
    "    #d_lemma = clean_str(train_data[i], lemma=True)  \n",
    "    \n",
    "    # All tokens\n",
    "    doc_dict = {        \n",
    "        'x_list': doc_concat\n",
    "    } \n",
    "    \n",
    "    pickle_out = open(serialized_docs_dir + 'test_all/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "    pickle.dump(doc_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    #doc_dict_lemma = {\n",
    "    #    'x_list': d_lemma\n",
    "    #} \n",
    "#     pickle_out = open(serialized_docs_dir + 'test_all_lemma/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "#     pickle.dump(doc_dict_lemma, pickle_out)\n",
    "#     pickle_out.close()\n",
    "    \n",
    "#     if len(d_lemma) > 5000:\n",
    "#         doc_dict_lemma_5000 = {            \n",
    "#             'x_list': d_lemma[:5000]\n",
    "#         } \n",
    "#     else:\n",
    "#         doc_dict_lemma_5000 = {            \n",
    "#             'x_list': d_lemma\n",
    "#         }    \n",
    "    \n",
    "#     pickle_out = open(serialized_docs_dir + 'test/' + 'dict.pickle_' + str(i), \"wb\")\n",
    "#     pickle.dump(doc_dict_lemma_5000, pickle_out)\n",
    "#     pickle_out.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load serialized proccessed documents that we create in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_docs_train_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/train_mix/'\n",
    "filenames = os.listdir(serialized_docs_train_dir)\n",
    "\n",
    "train_data = []\n",
    "#y_train = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_train_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        train_data.append(doc['x_list'])               \n",
    "        \n",
    "serialized_docs_test_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/test_mix/'\n",
    "filenames = os.listdir(serialized_docs_test_dir)\n",
    "\n",
    "test_data = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_test_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        test_data.append(doc['x_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 160580\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(train_docs, test_docs):\n",
    "    vocab = dict()\n",
    "\n",
    "    for doc in train_docs:\n",
    "        for word in doc:            \n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "                \n",
    "    for doc in test_docs:\n",
    "        for word in doc:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "\n",
    "    return vocab\n",
    " \n",
    "vocab = get_vocab(train_data, test_data)\n",
    "print(\"Vocab size\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query only the embeddings of the words that are in our vocabulary and store them to pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in pretrained set: 150810\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def load_embeddings(fname, vocab):\n",
    "    embeddings = np.zeros((len(vocab)+1, 300))\n",
    "    \n",
    "    c = 0\n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "    for word in vocab:\n",
    "        if word in model:\n",
    "            c += 1\n",
    "            embeddings[vocab[word]] = model[word]\n",
    "        else:\n",
    "            embeddings[vocab[word]] = np.random.uniform(-0.25, 0.25, 300)\n",
    "    print(\"Words in pretrained set:\", c)\n",
    "    return embeddings\n",
    "\n",
    "path_to_embeddings = r'C:\\Users\\User\\Desktop\\DS_challenge\\practical_session\\grcorpus_def.vec'\n",
    "embeddings = load_embeddings(path_to_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_emb_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/cnn_emb_tokens/'\n",
    "# Save Embeddings\n",
    "'''\n",
    "pickle_out = open(serialized_emb_dir + 'embeddings',\"wb\")\n",
    "pickle.dump(embeddings, pickle_out)\n",
    "pickle_out.close()\n",
    "'''\n",
    "# Load Embedding\n",
    "with open(serialized_emb_dir + 'embeddings', 'rb') as file:\n",
    "    embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match tokens with vocaculary keys/indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix:  (800, 5000)\n",
      "Size of testing matrix:  (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "max_size = 5000\n",
    "X_train_CNN = np.zeros((800, max_size), dtype=np.int64)\n",
    "\n",
    "for i, d in enumerate(train_data):\n",
    "    for j, term in enumerate(d):    \n",
    "        try:\n",
    "            X_train_CNN[i,j] = vocab[term]\n",
    "        except:\n",
    "            pass\n",
    "       \n",
    "        \n",
    "print(\"Size of training matrix: \", X_train_CNN.shape)\n",
    "\n",
    "X_test_CNN = np.zeros((200, max_size), dtype=np.int64)\n",
    "for i, d in enumerate(test_data):\n",
    "    for j, term in enumerate(d):\n",
    "        try:\n",
    "            X_test_CNN[i,j] = vocab[term]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print(\"Size of testing matrix: \", X_test_CNN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_indicies = list(range(0, 800))\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_indicies, y_train_class, train_size=0.87, random_state=43, stratify=y_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra_CNN = np.take(X_train_CNN, X_tra, axis=0)\n",
    "X_val_CNN = np.take(X_train_CNN, X_val, axis=0)\n",
    "\n",
    "y_tra_CNN = np.take(y_train_dum ,X_tra, axis=0)\n",
    "y_val_CNN = np.take(y_train_dum ,X_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 5000, 300)    48174300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 4998, 100)    90100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 4997, 100)    120100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            1005        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 48,385,505\n",
      "Trainable params: 211,205\n",
      "Non-trainable params: 48,174,300\n",
      "__________________________________________________________________________________________________\n",
      "Train on 696 samples, validate on 104 samples\n",
      "Epoch 1/50\n",
      "696/696 [==============================] - ETA: 1:00 - loss: 2.3726 - acc: 0.234 - ETA: 29s - loss: 2.3854 - acc: 0.195 - ETA: 18s - loss: 2.2342 - acc: 0.18 - ETA: 13s - loss: 2.1372 - acc: 0.19 - ETA: 9s - loss: 2.1483 - acc: 0.2000 - ETA: 7s - loss: 2.1620 - acc: 0.197 - ETA: 5s - loss: 2.1313 - acc: 0.198 - ETA: 3s - loss: 2.0988 - acc: 0.201 - ETA: 2s - loss: 2.0869 - acc: 0.204 - ETA: 0s - loss: 2.0766 - acc: 0.204 - 13s 19ms/step - loss: 2.0631 - acc: 0.2055 - val_loss: 1.3939 - val_acc: 0.4231\n",
      "Epoch 2/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 1.6586 - acc: 0.375 - ETA: 4s - loss: 1.5962 - acc: 0.375 - ETA: 4s - loss: 1.6361 - acc: 0.359 - ETA: 3s - loss: 1.6271 - acc: 0.363 - ETA: 3s - loss: 1.6364 - acc: 0.362 - ETA: 2s - loss: 1.5895 - acc: 0.372 - ETA: 2s - loss: 1.5671 - acc: 0.377 - ETA: 1s - loss: 1.5715 - acc: 0.378 - ETA: 0s - loss: 1.5813 - acc: 0.368 - ETA: 0s - loss: 1.5729 - acc: 0.375 - 6s 9ms/step - loss: 1.5675 - acc: 0.3779 - val_loss: 1.2047 - val_acc: 0.5962\n",
      "Epoch 3/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 1.4095 - acc: 0.468 - ETA: 4s - loss: 1.3378 - acc: 0.476 - ETA: 4s - loss: 1.2800 - acc: 0.474 - ETA: 3s - loss: 1.3028 - acc: 0.464 - ETA: 3s - loss: 1.3274 - acc: 0.465 - ETA: 2s - loss: 1.2946 - acc: 0.466 - ETA: 2s - loss: 1.2966 - acc: 0.462 - ETA: 1s - loss: 1.3027 - acc: 0.455 - ETA: 0s - loss: 1.2847 - acc: 0.465 - ETA: 0s - loss: 1.2901 - acc: 0.462 - 6s 9ms/step - loss: 1.2779 - acc: 0.4698 - val_loss: 1.1394 - val_acc: 0.5962\n",
      "Epoch 4/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 1.0715 - acc: 0.578 - ETA: 4s - loss: 1.0408 - acc: 0.578 - ETA: 4s - loss: 1.0810 - acc: 0.552 - ETA: 3s - loss: 1.1059 - acc: 0.550 - ETA: 3s - loss: 1.1200 - acc: 0.546 - ETA: 2s - loss: 1.1151 - acc: 0.559 - ETA: 2s - loss: 1.0978 - acc: 0.564 - ETA: 1s - loss: 1.0847 - acc: 0.570 - ETA: 0s - loss: 1.0705 - acc: 0.579 - ETA: 0s - loss: 1.0780 - acc: 0.578 - 6s 9ms/step - loss: 1.0836 - acc: 0.5733 - val_loss: 1.0739 - val_acc: 0.6250\n",
      "Epoch 5/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.8548 - acc: 0.687 - ETA: 4s - loss: 0.8674 - acc: 0.671 - ETA: 4s - loss: 0.8946 - acc: 0.645 - ETA: 3s - loss: 0.9145 - acc: 0.640 - ETA: 3s - loss: 0.9172 - acc: 0.646 - ETA: 2s - loss: 0.9434 - acc: 0.627 - ETA: 2s - loss: 0.9503 - acc: 0.629 - ETA: 1s - loss: 0.9580 - acc: 0.628 - ETA: 0s - loss: 0.9535 - acc: 0.628 - ETA: 0s - loss: 0.9514 - acc: 0.629 - 6s 9ms/step - loss: 0.9452 - acc: 0.6293 - val_loss: 1.0302 - val_acc: 0.6635\n",
      "Epoch 6/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.8803 - acc: 0.687 - ETA: 4s - loss: 0.9067 - acc: 0.664 - ETA: 4s - loss: 0.8954 - acc: 0.656 - ETA: 3s - loss: 0.8684 - acc: 0.675 - ETA: 3s - loss: 0.8839 - acc: 0.678 - ETA: 2s - loss: 0.8708 - acc: 0.682 - ETA: 2s - loss: 0.8665 - acc: 0.680 - ETA: 1s - loss: 0.8787 - acc: 0.677 - ETA: 0s - loss: 0.8792 - acc: 0.673 - ETA: 0s - loss: 0.8714 - acc: 0.684 - 6s 9ms/step - loss: 0.8669 - acc: 0.6853 - val_loss: 1.0583 - val_acc: 0.6442\n",
      "Epoch 7/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.7864 - acc: 0.765 - ETA: 4s - loss: 0.7906 - acc: 0.742 - ETA: 4s - loss: 0.7681 - acc: 0.750 - ETA: 3s - loss: 0.7660 - acc: 0.757 - ETA: 3s - loss: 0.7724 - acc: 0.753 - ETA: 2s - loss: 0.7871 - acc: 0.750 - ETA: 2s - loss: 0.7792 - acc: 0.754 - ETA: 1s - loss: 0.7892 - acc: 0.750 - ETA: 1s - loss: 0.7926 - acc: 0.739 - ETA: 0s - loss: 0.7886 - acc: 0.742 - 6s 9ms/step - loss: 0.7875 - acc: 0.7457 - val_loss: 1.0079 - val_acc: 0.6731\n",
      "Epoch 8/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.7427 - acc: 0.765 - ETA: 4s - loss: 0.7097 - acc: 0.757 - ETA: 4s - loss: 0.7359 - acc: 0.734 - ETA: 3s - loss: 0.7311 - acc: 0.753 - ETA: 3s - loss: 0.7163 - acc: 0.768 - ETA: 2s - loss: 0.7051 - acc: 0.765 - ETA: 2s - loss: 0.7060 - acc: 0.761 - ETA: 1s - loss: 0.7111 - acc: 0.755 - ETA: 1s - loss: 0.7307 - acc: 0.753 - ETA: 0s - loss: 0.7394 - acc: 0.746 - 7s 9ms/step - loss: 0.7346 - acc: 0.7500 - val_loss: 0.9903 - val_acc: 0.6731\n",
      "Epoch 9/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.5211 - acc: 0.843 - ETA: 5s - loss: 0.6204 - acc: 0.828 - ETA: 4s - loss: 0.6712 - acc: 0.807 - ETA: 3s - loss: 0.6875 - acc: 0.793 - ETA: 3s - loss: 0.6908 - acc: 0.787 - ETA: 2s - loss: 0.6908 - acc: 0.778 - ETA: 2s - loss: 0.6986 - acc: 0.776 - ETA: 1s - loss: 0.6806 - acc: 0.789 - ETA: 1s - loss: 0.6734 - acc: 0.789 - ETA: 0s - loss: 0.6642 - acc: 0.793 - 7s 10ms/step - loss: 0.6511 - acc: 0.7989 - val_loss: 0.9854 - val_acc: 0.6635\n",
      "Epoch 10/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.6534 - acc: 0.812 - ETA: 5s - loss: 0.6698 - acc: 0.773 - ETA: 4s - loss: 0.6406 - acc: 0.796 - ETA: 4s - loss: 0.6442 - acc: 0.804 - ETA: 3s - loss: 0.6205 - acc: 0.825 - ETA: 2s - loss: 0.6230 - acc: 0.828 - ETA: 2s - loss: 0.6130 - acc: 0.832 - ETA: 1s - loss: 0.6169 - acc: 0.822 - ETA: 1s - loss: 0.6136 - acc: 0.822 - ETA: 0s - loss: 0.6231 - acc: 0.814 - 7s 10ms/step - loss: 0.6328 - acc: 0.8075 - val_loss: 0.9892 - val_acc: 0.6538\n",
      "Epoch 11/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.5693 - acc: 0.875 - ETA: 5s - loss: 0.6689 - acc: 0.789 - ETA: 4s - loss: 0.6314 - acc: 0.807 - ETA: 4s - loss: 0.6408 - acc: 0.812 - ETA: 3s - loss: 0.6257 - acc: 0.821 - ETA: 3s - loss: 0.6128 - acc: 0.820 - ETA: 2s - loss: 0.5929 - acc: 0.823 - ETA: 1s - loss: 0.5916 - acc: 0.822 - ETA: 1s - loss: 0.6084 - acc: 0.812 - ETA: 0s - loss: 0.6029 - acc: 0.820 - 7s 11ms/step - loss: 0.6071 - acc: 0.8147 - val_loss: 0.9774 - val_acc: 0.6538\n",
      "Epoch 12/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.6563 - acc: 0.781 - ETA: 5s - loss: 0.6072 - acc: 0.789 - ETA: 5s - loss: 0.6060 - acc: 0.791 - ETA: 4s - loss: 0.6117 - acc: 0.796 - ETA: 3s - loss: 0.6012 - acc: 0.809 - ETA: 3s - loss: 0.5970 - acc: 0.802 - ETA: 2s - loss: 0.5830 - acc: 0.817 - ETA: 1s - loss: 0.5684 - acc: 0.832 - ETA: 1s - loss: 0.5827 - acc: 0.828 - ETA: 0s - loss: 0.5820 - acc: 0.829 - 8s 11ms/step - loss: 0.5757 - acc: 0.8333 - val_loss: 0.9743 - val_acc: 0.6538\n",
      "Epoch 13/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.5190 - acc: 0.890 - ETA: 5s - loss: 0.5523 - acc: 0.843 - ETA: 5s - loss: 0.5246 - acc: 0.854 - ETA: 4s - loss: 0.5116 - acc: 0.875 - ETA: 3s - loss: 0.5012 - acc: 0.871 - ETA: 3s - loss: 0.5071 - acc: 0.869 - ETA: 2s - loss: 0.5187 - acc: 0.861 - ETA: 1s - loss: 0.5193 - acc: 0.857 - ETA: 1s - loss: 0.5210 - acc: 0.857 - ETA: 0s - loss: 0.5292 - acc: 0.853 - 8s 11ms/step - loss: 0.5241 - acc: 0.8534 - val_loss: 0.9683 - val_acc: 0.6635\n",
      "Epoch 14/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.4598 - acc: 0.875 - ETA: 5s - loss: 0.5784 - acc: 0.796 - ETA: 5s - loss: 0.5607 - acc: 0.807 - ETA: 4s - loss: 0.5354 - acc: 0.820 - ETA: 3s - loss: 0.5300 - acc: 0.834 - ETA: 3s - loss: 0.5242 - acc: 0.838 - ETA: 2s - loss: 0.5154 - acc: 0.846 - ETA: 1s - loss: 0.5122 - acc: 0.849 - ETA: 1s - loss: 0.5029 - acc: 0.857 - ETA: 0s - loss: 0.5054 - acc: 0.856 - 8s 11ms/step - loss: 0.5023 - acc: 0.8578 - val_loss: 0.9647 - val_acc: 0.6635\n",
      "Epoch 15/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.5499 - acc: 0.828 - ETA: 6s - loss: 0.5028 - acc: 0.859 - ETA: 5s - loss: 0.4846 - acc: 0.864 - ETA: 4s - loss: 0.4765 - acc: 0.867 - ETA: 4s - loss: 0.4633 - acc: 0.881 - ETA: 3s - loss: 0.4620 - acc: 0.877 - ETA: 2s - loss: 0.4641 - acc: 0.877 - ETA: 2s - loss: 0.4684 - acc: 0.863 - ETA: 1s - loss: 0.4802 - acc: 0.857 - ETA: 0s - loss: 0.4838 - acc: 0.856 - 8s 12ms/step - loss: 0.4847 - acc: 0.8563 - val_loss: 0.9552 - val_acc: 0.6635\n",
      "Epoch 16/50\n",
      "696/696 [==============================] - ETA: 7s - loss: 0.4316 - acc: 0.859 - ETA: 6s - loss: 0.5081 - acc: 0.843 - ETA: 5s - loss: 0.5109 - acc: 0.843 - ETA: 4s - loss: 0.4924 - acc: 0.847 - ETA: 4s - loss: 0.4547 - acc: 0.865 - ETA: 3s - loss: 0.4575 - acc: 0.867 - ETA: 2s - loss: 0.4623 - acc: 0.859 - ETA: 2s - loss: 0.4711 - acc: 0.857 - ETA: 1s - loss: 0.4670 - acc: 0.857 - ETA: 0s - loss: 0.4632 - acc: 0.859 - 8s 12ms/step - loss: 0.4596 - acc: 0.8592 - val_loss: 0.9516 - val_acc: 0.6538\n",
      "Epoch 17/50\n",
      "696/696 [==============================] - ETA: 7s - loss: 0.5279 - acc: 0.828 - ETA: 6s - loss: 0.4949 - acc: 0.828 - ETA: 5s - loss: 0.4796 - acc: 0.854 - ETA: 4s - loss: 0.4955 - acc: 0.855 - ETA: 4s - loss: 0.4648 - acc: 0.868 - ETA: 3s - loss: 0.4379 - acc: 0.880 - ETA: 2s - loss: 0.4500 - acc: 0.868 - ETA: 2s - loss: 0.4493 - acc: 0.869 - ETA: 1s - loss: 0.4409 - acc: 0.873 - ETA: 0s - loss: 0.4532 - acc: 0.862 - 8s 12ms/step - loss: 0.4553 - acc: 0.8621 - val_loss: 0.9504 - val_acc: 0.6827\n",
      "Epoch 18/50\n",
      "696/696 [==============================] - ETA: 7s - loss: 0.3267 - acc: 0.921 - ETA: 6s - loss: 0.3823 - acc: 0.890 - ETA: 6s - loss: 0.3926 - acc: 0.885 - ETA: 5s - loss: 0.4503 - acc: 0.855 - ETA: 4s - loss: 0.4444 - acc: 0.856 - ETA: 3s - loss: 0.4359 - acc: 0.862 - ETA: 2s - loss: 0.4409 - acc: 0.859 - ETA: 2s - loss: 0.4293 - acc: 0.867 - ETA: 1s - loss: 0.4158 - acc: 0.869 - ETA: 0s - loss: 0.4251 - acc: 0.867 - 9s 13ms/step - loss: 0.4330 - acc: 0.8664 - val_loss: 0.9414 - val_acc: 0.6731\n",
      "Epoch 19/50\n",
      "696/696 [==============================] - ETA: 7s - loss: 0.4382 - acc: 0.828 - ETA: 6s - loss: 0.4062 - acc: 0.867 - ETA: 6s - loss: 0.4206 - acc: 0.875 - ETA: 5s - loss: 0.4414 - acc: 0.867 - ETA: 4s - loss: 0.4353 - acc: 0.875 - ETA: 3s - loss: 0.4438 - acc: 0.869 - ETA: 2s - loss: 0.4406 - acc: 0.870 - ETA: 2s - loss: 0.4334 - acc: 0.869 - ETA: 1s - loss: 0.4369 - acc: 0.866 - ETA: 0s - loss: 0.4282 - acc: 0.870 - 9s 13ms/step - loss: 0.4304 - acc: 0.8693 - val_loss: 0.9483 - val_acc: 0.6827\n",
      "Epoch 20/50\n",
      "696/696 [==============================] - ETA: 7s - loss: 0.4743 - acc: 0.843 - ETA: 7s - loss: 0.5024 - acc: 0.828 - ETA: 6s - loss: 0.4565 - acc: 0.849 - ETA: 5s - loss: 0.4552 - acc: 0.847 - ETA: 4s - loss: 0.4259 - acc: 0.862 - ETA: 3s - loss: 0.4105 - acc: 0.867 - ETA: 3s - loss: 0.4034 - acc: 0.877 - ETA: 2s - loss: 0.4063 - acc: 0.869 - ETA: 1s - loss: 0.4005 - acc: 0.868 - ETA: 0s - loss: 0.3973 - acc: 0.870 - 9s 13ms/step - loss: 0.4017 - acc: 0.8707 - val_loss: 0.9209 - val_acc: 0.6923\n",
      "Epoch 21/50\n",
      "696/696 [==============================] - ETA: 8s - loss: 0.3366 - acc: 0.906 - ETA: 7s - loss: 0.3197 - acc: 0.914 - ETA: 6s - loss: 0.3928 - acc: 0.864 - ETA: 5s - loss: 0.4196 - acc: 0.839 - ETA: 5s - loss: 0.4012 - acc: 0.859 - ETA: 4s - loss: 0.4216 - acc: 0.846 - ETA: 3s - loss: 0.4116 - acc: 0.859 - ETA: 2s - loss: 0.4068 - acc: 0.859 - ETA: 1s - loss: 0.4127 - acc: 0.857 - ETA: 0s - loss: 0.4126 - acc: 0.859 - 10s 14ms/step - loss: 0.4102 - acc: 0.8578 - val_loss: 0.9717 - val_acc: 0.6538\n",
      "Epoch 22/50\n",
      "696/696 [==============================] - ETA: 8s - loss: 0.4757 - acc: 0.859 - ETA: 7s - loss: 0.4528 - acc: 0.851 - ETA: 6s - loss: 0.4013 - acc: 0.875 - ETA: 5s - loss: 0.4165 - acc: 0.875 - ETA: 5s - loss: 0.4095 - acc: 0.871 - ETA: 4s - loss: 0.4118 - acc: 0.867 - ETA: 3s - loss: 0.4063 - acc: 0.870 - ETA: 2s - loss: 0.3871 - acc: 0.877 - ETA: 1s - loss: 0.3800 - acc: 0.880 - ETA: 0s - loss: 0.3839 - acc: 0.873 - 10s 15ms/step - loss: 0.3924 - acc: 0.8693 - val_loss: 0.9177 - val_acc: 0.6538\n",
      "Epoch 23/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.4362 - acc: 0.812 - ETA: 8s - loss: 0.4181 - acc: 0.843 - ETA: 7s - loss: 0.4098 - acc: 0.854 - ETA: 6s - loss: 0.4433 - acc: 0.832 - ETA: 5s - loss: 0.4269 - acc: 0.850 - ETA: 4s - loss: 0.4112 - acc: 0.862 - ETA: 3s - loss: 0.3954 - acc: 0.872 - ETA: 2s - loss: 0.4031 - acc: 0.867 - ETA: 1s - loss: 0.3956 - acc: 0.869 - ETA: 0s - loss: 0.3878 - acc: 0.873 - 11s 16ms/step - loss: 0.3866 - acc: 0.8736 - val_loss: 0.9697 - val_acc: 0.6154\n",
      "Epoch 24/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.3229 - acc: 0.890 - ETA: 8s - loss: 0.3658 - acc: 0.859 - ETA: 7s - loss: 0.3376 - acc: 0.885 - ETA: 6s - loss: 0.3373 - acc: 0.890 - ETA: 5s - loss: 0.3548 - acc: 0.881 - ETA: 4s - loss: 0.3623 - acc: 0.877 - ETA: 3s - loss: 0.3600 - acc: 0.879 - ETA: 2s - loss: 0.3555 - acc: 0.884 - ETA: 1s - loss: 0.3632 - acc: 0.878 - ETA: 0s - loss: 0.3653 - acc: 0.879 - 11s 16ms/step - loss: 0.3641 - acc: 0.8807 - val_loss: 0.9142 - val_acc: 0.6827\n",
      "Epoch 25/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.3400 - acc: 0.890 - ETA: 8s - loss: 0.4138 - acc: 0.867 - ETA: 7s - loss: 0.4567 - acc: 0.843 - ETA: 6s - loss: 0.4128 - acc: 0.859 - ETA: 5s - loss: 0.4147 - acc: 0.850 - ETA: 4s - loss: 0.3996 - acc: 0.859 - ETA: 3s - loss: 0.3818 - acc: 0.863 - ETA: 2s - loss: 0.3809 - acc: 0.865 - ETA: 1s - loss: 0.3810 - acc: 0.868 - ETA: 0s - loss: 0.3717 - acc: 0.871 - 11s 16ms/step - loss: 0.3715 - acc: 0.8736 - val_loss: 0.9469 - val_acc: 0.6250\n",
      "Epoch 26/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.3712 - acc: 0.843 - ETA: 8s - loss: 0.3908 - acc: 0.859 - ETA: 7s - loss: 0.3763 - acc: 0.869 - ETA: 6s - loss: 0.3731 - acc: 0.875 - ETA: 5s - loss: 0.3672 - acc: 0.871 - ETA: 4s - loss: 0.3664 - acc: 0.872 - ETA: 3s - loss: 0.3731 - acc: 0.866 - ETA: 2s - loss: 0.3639 - acc: 0.873 - ETA: 1s - loss: 0.3460 - acc: 0.881 - ETA: 0s - loss: 0.3542 - acc: 0.878 - 11s 16ms/step - loss: 0.3611 - acc: 0.8750 - val_loss: 0.9203 - val_acc: 0.6827\n",
      "Epoch 27/50\n",
      "696/696 [==============================] - ETA: 9s - loss: 0.2207 - acc: 0.921 - ETA: 8s - loss: 0.2987 - acc: 0.898 - ETA: 7s - loss: 0.3452 - acc: 0.880 - ETA: 6s - loss: 0.3949 - acc: 0.871 - ETA: 5s - loss: 0.3770 - acc: 0.875 - ETA: 4s - loss: 0.3785 - acc: 0.877 - ETA: 4s - loss: 0.3803 - acc: 0.875 - ETA: 3s - loss: 0.3884 - acc: 0.869 - ETA: 2s - loss: 0.3713 - acc: 0.875 - ETA: 1s - loss: 0.3576 - acc: 0.876 - 14s 19ms/step - loss: 0.3525 - acc: 0.8779 - val_loss: 0.9467 - val_acc: 0.6346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3811 - acc: 0.82 - ETA: 12s - loss: 0.3043 - acc: 0.88 - ETA: 10s - loss: 0.3208 - acc: 0.88 - ETA: 9s - loss: 0.3367 - acc: 0.8789 - ETA: 8s - loss: 0.3480 - acc: 0.868 - ETA: 6s - loss: 0.3532 - acc: 0.877 - ETA: 5s - loss: 0.3436 - acc: 0.881 - ETA: 3s - loss: 0.3417 - acc: 0.884 - ETA: 2s - loss: 0.3375 - acc: 0.883 - ETA: 1s - loss: 0.3417 - acc: 0.879 - 16s 23ms/step - loss: 0.3377 - acc: 0.8793 - val_loss: 0.9190 - val_acc: 0.6923\n",
      "Epoch 29/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2925 - acc: 0.92 - ETA: 12s - loss: 0.3351 - acc: 0.89 - ETA: 10s - loss: 0.2951 - acc: 0.90 - ETA: 9s - loss: 0.3349 - acc: 0.8750 - ETA: 8s - loss: 0.3635 - acc: 0.868 - ETA: 6s - loss: 0.3491 - acc: 0.872 - ETA: 5s - loss: 0.3493 - acc: 0.872 - ETA: 3s - loss: 0.3545 - acc: 0.861 - ETA: 2s - loss: 0.3506 - acc: 0.868 - ETA: 1s - loss: 0.3442 - acc: 0.870 - 16s 23ms/step - loss: 0.3405 - acc: 0.8764 - val_loss: 0.9448 - val_acc: 0.6442\n",
      "Epoch 30/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2851 - acc: 0.92 - ETA: 12s - loss: 0.3447 - acc: 0.86 - ETA: 10s - loss: 0.4313 - acc: 0.82 - ETA: 9s - loss: 0.4057 - acc: 0.8359 - ETA: 8s - loss: 0.3994 - acc: 0.831 - ETA: 6s - loss: 0.3706 - acc: 0.846 - ETA: 5s - loss: 0.3609 - acc: 0.859 - ETA: 3s - loss: 0.3595 - acc: 0.865 - ETA: 2s - loss: 0.3403 - acc: 0.873 - ETA: 1s - loss: 0.3336 - acc: 0.878 - 16s 23ms/step - loss: 0.3354 - acc: 0.8764 - val_loss: 0.9299 - val_acc: 0.6635\n",
      "Epoch 31/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3538 - acc: 0.87 - ETA: 12s - loss: 0.3780 - acc: 0.86 - ETA: 10s - loss: 0.3675 - acc: 0.86 - ETA: 9s - loss: 0.3745 - acc: 0.8633 - ETA: 8s - loss: 0.3561 - acc: 0.868 - ETA: 6s - loss: 0.3448 - acc: 0.872 - ETA: 5s - loss: 0.3379 - acc: 0.879 - ETA: 3s - loss: 0.3370 - acc: 0.878 - ETA: 2s - loss: 0.3290 - acc: 0.880 - ETA: 1s - loss: 0.3233 - acc: 0.881 - 16s 23ms/step - loss: 0.3313 - acc: 0.8793 - val_loss: 0.9279 - val_acc: 0.6731\n",
      "Epoch 32/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.4911 - acc: 0.75 - ETA: 12s - loss: 0.4509 - acc: 0.78 - ETA: 10s - loss: 0.4000 - acc: 0.82 - ETA: 9s - loss: 0.3871 - acc: 0.8281 - ETA: 8s - loss: 0.3755 - acc: 0.840 - ETA: 6s - loss: 0.3562 - acc: 0.856 - ETA: 5s - loss: 0.3517 - acc: 0.866 - ETA: 3s - loss: 0.3508 - acc: 0.871 - ETA: 2s - loss: 0.3379 - acc: 0.878 - ETA: 1s - loss: 0.3402 - acc: 0.878 - 16s 23ms/step - loss: 0.3330 - acc: 0.8779 - val_loss: 0.9099 - val_acc: 0.6731\n",
      "Epoch 33/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.4177 - acc: 0.81 - ETA: 12s - loss: 0.3413 - acc: 0.85 - ETA: 10s - loss: 0.3131 - acc: 0.87 - ETA: 9s - loss: 0.3015 - acc: 0.8789 - ETA: 8s - loss: 0.3220 - acc: 0.871 - ETA: 6s - loss: 0.3379 - acc: 0.867 - ETA: 5s - loss: 0.3326 - acc: 0.870 - ETA: 3s - loss: 0.3267 - acc: 0.873 - ETA: 2s - loss: 0.3216 - acc: 0.880 - ETA: 1s - loss: 0.3241 - acc: 0.878 - 16s 23ms/step - loss: 0.3218 - acc: 0.8793 - val_loss: 0.9360 - val_acc: 0.6538\n",
      "Epoch 34/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3037 - acc: 0.89 - ETA: 12s - loss: 0.2368 - acc: 0.91 - ETA: 10s - loss: 0.2841 - acc: 0.89 - ETA: 9s - loss: 0.3157 - acc: 0.8750 - ETA: 8s - loss: 0.3162 - acc: 0.887 - ETA: 6s - loss: 0.3157 - acc: 0.880 - ETA: 5s - loss: 0.3188 - acc: 0.881 - ETA: 3s - loss: 0.3246 - acc: 0.875 - ETA: 2s - loss: 0.3220 - acc: 0.880 - ETA: 1s - loss: 0.3207 - acc: 0.878 - 16s 23ms/step - loss: 0.3181 - acc: 0.8807 - val_loss: 0.9339 - val_acc: 0.6635\n",
      "Epoch 35/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3218 - acc: 0.92 - ETA: 12s - loss: 0.2808 - acc: 0.90 - ETA: 10s - loss: 0.2903 - acc: 0.89 - ETA: 9s - loss: 0.2904 - acc: 0.8867 - ETA: 8s - loss: 0.2962 - acc: 0.887 - ETA: 6s - loss: 0.2914 - acc: 0.890 - ETA: 5s - loss: 0.2923 - acc: 0.888 - ETA: 3s - loss: 0.2954 - acc: 0.888 - ETA: 2s - loss: 0.2967 - acc: 0.888 - ETA: 1s - loss: 0.3089 - acc: 0.885 - 16s 23ms/step - loss: 0.3176 - acc: 0.8793 - val_loss: 0.9200 - val_acc: 0.6731\n",
      "Epoch 36/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2282 - acc: 0.95 - ETA: 12s - loss: 0.3666 - acc: 0.85 - ETA: 10s - loss: 0.3658 - acc: 0.85 - ETA: 9s - loss: 0.3455 - acc: 0.8672 - ETA: 8s - loss: 0.3277 - acc: 0.871 - ETA: 6s - loss: 0.3327 - acc: 0.872 - ETA: 5s - loss: 0.3298 - acc: 0.875 - ETA: 3s - loss: 0.3410 - acc: 0.869 - ETA: 2s - loss: 0.3305 - acc: 0.869 - ETA: 1s - loss: 0.3210 - acc: 0.875 - 16s 23ms/step - loss: 0.3218 - acc: 0.8779 - val_loss: 0.9487 - val_acc: 0.6058\n",
      "Epoch 37/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2752 - acc: 0.89 - ETA: 12s - loss: 0.2366 - acc: 0.91 - ETA: 10s - loss: 0.2801 - acc: 0.88 - ETA: 9s - loss: 0.2859 - acc: 0.8828 - ETA: 8s - loss: 0.3038 - acc: 0.881 - ETA: 6s - loss: 0.2906 - acc: 0.885 - ETA: 5s - loss: 0.2849 - acc: 0.890 - ETA: 3s - loss: 0.3056 - acc: 0.873 - ETA: 2s - loss: 0.3030 - acc: 0.876 - ETA: 1s - loss: 0.3037 - acc: 0.881 - 16s 23ms/step - loss: 0.3091 - acc: 0.8793 - val_loss: 0.9317 - val_acc: 0.6635\n",
      "Epoch 38/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2879 - acc: 0.89 - ETA: 12s - loss: 0.2917 - acc: 0.88 - ETA: 10s - loss: 0.3220 - acc: 0.85 - ETA: 9s - loss: 0.3037 - acc: 0.8750 - ETA: 8s - loss: 0.2955 - acc: 0.875 - ETA: 6s - loss: 0.2973 - acc: 0.875 - ETA: 5s - loss: 0.2900 - acc: 0.877 - ETA: 3s - loss: 0.2937 - acc: 0.877 - ETA: 2s - loss: 0.3004 - acc: 0.876 - ETA: 1s - loss: 0.3059 - acc: 0.878 - 16s 23ms/step - loss: 0.3119 - acc: 0.8807 - val_loss: 0.9497 - val_acc: 0.6442\n",
      "Epoch 39/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2290 - acc: 0.90 - ETA: 12s - loss: 0.2372 - acc: 0.89 - ETA: 10s - loss: 0.3322 - acc: 0.85 - ETA: 9s - loss: 0.3247 - acc: 0.8672 - ETA: 8s - loss: 0.3020 - acc: 0.875 - ETA: 6s - loss: 0.2941 - acc: 0.880 - ETA: 5s - loss: 0.2974 - acc: 0.883 - ETA: 3s - loss: 0.3060 - acc: 0.880 - ETA: 2s - loss: 0.3014 - acc: 0.883 - ETA: 1s - loss: 0.3101 - acc: 0.876 - 16s 23ms/step - loss: 0.3008 - acc: 0.8807 - val_loss: 0.9276 - val_acc: 0.6635\n",
      "Epoch 40/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2121 - acc: 0.95 - ETA: 12s - loss: 0.2490 - acc: 0.92 - ETA: 10s - loss: 0.2952 - acc: 0.90 - ETA: 9s - loss: 0.2792 - acc: 0.9023 - ETA: 8s - loss: 0.3030 - acc: 0.881 - ETA: 6s - loss: 0.2969 - acc: 0.885 - ETA: 5s - loss: 0.3152 - acc: 0.872 - ETA: 3s - loss: 0.3067 - acc: 0.875 - ETA: 2s - loss: 0.3026 - acc: 0.881 - ETA: 1s - loss: 0.3015 - acc: 0.879 - 16s 23ms/step - loss: 0.2998 - acc: 0.8807 - val_loss: 0.9586 - val_acc: 0.6058\n",
      "Epoch 41/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2910 - acc: 0.90 - ETA: 12s - loss: 0.3034 - acc: 0.89 - ETA: 10s - loss: 0.2998 - acc: 0.87 - ETA: 9s - loss: 0.2876 - acc: 0.8828 - ETA: 8s - loss: 0.2885 - acc: 0.881 - ETA: 6s - loss: 0.3092 - acc: 0.872 - ETA: 5s - loss: 0.2984 - acc: 0.881 - ETA: 3s - loss: 0.2995 - acc: 0.884 - ETA: 2s - loss: 0.3048 - acc: 0.880 - ETA: 1s - loss: 0.3043 - acc: 0.876 - 16s 23ms/step - loss: 0.3002 - acc: 0.8807 - val_loss: 0.9542 - val_acc: 0.6346\n",
      "Epoch 42/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3007 - acc: 0.87 - ETA: 12s - loss: 0.2655 - acc: 0.89 - ETA: 10s - loss: 0.2606 - acc: 0.90 - ETA: 9s - loss: 0.2796 - acc: 0.8945 - ETA: 8s - loss: 0.2790 - acc: 0.896 - ETA: 6s - loss: 0.2579 - acc: 0.908 - ETA: 5s - loss: 0.2782 - acc: 0.901 - ETA: 3s - loss: 0.2804 - acc: 0.898 - ETA: 2s - loss: 0.2738 - acc: 0.901 - ETA: 1s - loss: 0.2902 - acc: 0.890 - 16s 23ms/step - loss: 0.3004 - acc: 0.8822 - val_loss: 0.9174 - val_acc: 0.6731\n",
      "Epoch 43/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2714 - acc: 0.87 - ETA: 12s - loss: 0.2518 - acc: 0.87 - ETA: 10s - loss: 0.3207 - acc: 0.86 - ETA: 9s - loss: 0.3107 - acc: 0.8711 - ETA: 8s - loss: 0.3268 - acc: 0.862 - ETA: 6s - loss: 0.3348 - acc: 0.856 - ETA: 5s - loss: 0.3226 - acc: 0.861 - ETA: 3s - loss: 0.3047 - acc: 0.873 - ETA: 2s - loss: 0.3015 - acc: 0.876 - ETA: 1s - loss: 0.3007 - acc: 0.878 - 16s 23ms/step - loss: 0.2975 - acc: 0.8807 - val_loss: 0.9403 - val_acc: 0.6442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3514 - acc: 0.87 - ETA: 12s - loss: 0.3627 - acc: 0.86 - ETA: 10s - loss: 0.3388 - acc: 0.87 - ETA: 9s - loss: 0.3086 - acc: 0.8828 - ETA: 8s - loss: 0.2972 - acc: 0.884 - ETA: 6s - loss: 0.2828 - acc: 0.890 - ETA: 5s - loss: 0.2688 - acc: 0.895 - ETA: 3s - loss: 0.2819 - acc: 0.888 - ETA: 2s - loss: 0.2847 - acc: 0.885 - ETA: 1s - loss: 0.2912 - acc: 0.882 - 16s 23ms/step - loss: 0.2924 - acc: 0.8807 - val_loss: 0.9216 - val_acc: 0.6635\n",
      "Epoch 45/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.3070 - acc: 0.85 - ETA: 12s - loss: 0.2903 - acc: 0.85 - ETA: 10s - loss: 0.3043 - acc: 0.86 - ETA: 9s - loss: 0.3231 - acc: 0.8594 - ETA: 8s - loss: 0.3277 - acc: 0.856 - ETA: 6s - loss: 0.3100 - acc: 0.867 - ETA: 5s - loss: 0.2886 - acc: 0.881 - ETA: 3s - loss: 0.2934 - acc: 0.878 - ETA: 2s - loss: 0.3011 - acc: 0.876 - ETA: 1s - loss: 0.2935 - acc: 0.881 - 16s 23ms/step - loss: 0.2917 - acc: 0.8807 - val_loss: 0.9949 - val_acc: 0.6058\n",
      "Epoch 46/50\n",
      "696/696 [==============================] - ETA: 13s - loss: 0.2813 - acc: 0.89 - ETA: 12s - loss: 0.2893 - acc: 0.87 - ETA: 10s - loss: 0.2527 - acc: 0.89 - ETA: 9s - loss: 0.2496 - acc: 0.8945 - ETA: 8s - loss: 0.2374 - acc: 0.900 - ETA: 6s - loss: 0.2573 - acc: 0.890 - ETA: 5s - loss: 0.2596 - acc: 0.892 - ETA: 3s - loss: 0.2685 - acc: 0.890 - ETA: 2s - loss: 0.2781 - acc: 0.888 - ETA: 1s - loss: 0.2930 - acc: 0.881 - 73s 104ms/step - loss: 0.2994 - acc: 0.8779 - val_loss: 0.9378 - val_acc: 0.6827\n",
      "Epoch 47/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.3235 - acc: 0.859 - ETA: 4s - loss: 0.2780 - acc: 0.898 - ETA: 4s - loss: 0.3016 - acc: 0.895 - ETA: 3s - loss: 0.2645 - acc: 0.910 - ETA: 3s - loss: 0.2692 - acc: 0.900 - ETA: 2s - loss: 0.2855 - acc: 0.890 - ETA: 1s - loss: 0.2908 - acc: 0.888 - ETA: 1s - loss: 0.2824 - acc: 0.890 - ETA: 0s - loss: 0.2907 - acc: 0.888 - ETA: 0s - loss: 0.2876 - acc: 0.885 - 6s 8ms/step - loss: 0.2893 - acc: 0.8822 - val_loss: 0.9096 - val_acc: 0.6827\n",
      "Epoch 48/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.3359 - acc: 0.859 - ETA: 4s - loss: 0.2680 - acc: 0.882 - ETA: 4s - loss: 0.2498 - acc: 0.890 - ETA: 3s - loss: 0.2645 - acc: 0.886 - ETA: 3s - loss: 0.2888 - acc: 0.871 - ETA: 2s - loss: 0.3005 - acc: 0.862 - ETA: 2s - loss: 0.3111 - acc: 0.859 - ETA: 1s - loss: 0.2915 - acc: 0.873 - ETA: 1s - loss: 0.2934 - acc: 0.876 - ETA: 0s - loss: 0.2910 - acc: 0.881 - 6s 9ms/step - loss: 0.2860 - acc: 0.8822 - val_loss: 0.9385 - val_acc: 0.6442\n",
      "Epoch 49/50\n",
      "696/696 [==============================] - ETA: 5s - loss: 0.3512 - acc: 0.875 - ETA: 5s - loss: 0.3644 - acc: 0.851 - ETA: 4s - loss: 0.3863 - acc: 0.838 - ETA: 4s - loss: 0.3419 - acc: 0.863 - ETA: 3s - loss: 0.2996 - acc: 0.878 - ETA: 2s - loss: 0.2917 - acc: 0.875 - ETA: 2s - loss: 0.2893 - acc: 0.877 - ETA: 1s - loss: 0.2784 - acc: 0.882 - ETA: 1s - loss: 0.2771 - acc: 0.885 - ETA: 0s - loss: 0.2827 - acc: 0.882 - 7s 10ms/step - loss: 0.2851 - acc: 0.8822 - val_loss: 0.9469 - val_acc: 0.6538\n",
      "Epoch 50/50\n",
      "696/696 [==============================] - ETA: 6s - loss: 0.2179 - acc: 0.921 - ETA: 5s - loss: 0.3345 - acc: 0.851 - ETA: 5s - loss: 0.2800 - acc: 0.885 - ETA: 4s - loss: 0.2931 - acc: 0.875 - ETA: 3s - loss: 0.2651 - acc: 0.884 - ETA: 3s - loss: 0.2626 - acc: 0.888 - ETA: 2s - loss: 0.2740 - acc: 0.883 - ETA: 1s - loss: 0.2781 - acc: 0.880 - ETA: 1s - loss: 0.2845 - acc: 0.880 - ETA: 0s - loss: 0.2909 - acc: 0.873 - 8s 11ms/step - loss: 0.2862 - acc: 0.8779 - val_loss: 0.9231 - val_acc: 0.6731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2758ca22320>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(lr=0.001) # , beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# optimizer = Adam(lr=0.008)\n",
    "# reducer_lr = ReduceLROnPlateau(factor = 0.00005, patience = 1, min_lr = 1e-6, verbose = 1)\n",
    "\n",
    "my_input = Input(shape=(max_size, ))\n",
    "\n",
    "embedding = Embedding(input_dim=len(vocab)+1,\n",
    "                      output_dim=300,\n",
    "                      weights=[embeddings],\n",
    "                      trainable=False,\n",
    "                      input_length=max_size\n",
    "                      ) (my_input)\n",
    "\n",
    "conv1 = Conv1D(filters=100,\n",
    "               kernel_size=3,\n",
    "               activation = 'relu',\n",
    "#                kernel_regularizer=l2(0.0001),\n",
    "#                bias_regularizer=l2(0.0001)\n",
    "               ) (embedding)\n",
    "\n",
    "conv2 = Conv1D(filters=100,\n",
    "               kernel_size=4,\n",
    "               activation = 'relu',\n",
    "#                kernel_regularizer=l2(0.0001),\n",
    "#                bias_regularizer=l2(0.0001)\n",
    "               ) (embedding)\n",
    "\n",
    "pool1 = GlobalMaxPooling1D()(conv1)\n",
    "pool2 = GlobalMaxPooling1D()(conv2)\n",
    "\n",
    "concat = Concatenate()([pool1, pool2])\n",
    "drop = Dropout(rate=0.5) (concat)\n",
    "my_output = Dense(units=5, activation='softmax')(drop)\n",
    "\n",
    "model_CNN = Model(my_input, my_output)\n",
    "model_CNN.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_CNN.summary()\n",
    "\n",
    "# go through epochs as long as accuracy on validation set increases\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=20,\n",
    "                               mode='min')\n",
    "\n",
    "# make sure that the model corresponding to the best epoch is saved\n",
    "checkpointer = ModelCheckpoint(filepath='cnn_text_categorization.hdf5',\n",
    "                               monitor='val_loss',\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "\n",
    "model_CNN.fit(X_tra_CNN,\n",
    "          y_tra_CNN,\n",
    "          epochs=50,\n",
    "          batch_size=64,\n",
    "          validation_data=(X_val_CNN, y_val_CNN),\n",
    "#           validation_split=0.05,\n",
    "          callbacks=[early_stopping, checkpointer]#, reducer_lr]\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "del model_CNN\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_CNN = model_CNN.predict(X_tra_CNN)\n",
    "y_pred_val_CNN = model_CNN.predict(X_val_CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN text log loss:\n",
      "0.2684406981762581\n",
      "0.9230987612827448\n"
     ]
    }
   ],
   "source": [
    "print('CNN text log loss:')\n",
    "print(log_loss(y_tra_CNN,y_pred_train_CNN))\n",
    "print(log_loss(y_val_CNN, y_pred_val_CNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeyWord MLPs\n",
    "- tf-idfs with tokens and n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all document tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "serialized_docs_train_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/train_all/'\n",
    "filenames = os.listdir(serialized_docs_train_dir)\n",
    "\n",
    "train_data = []\n",
    "y_train_tfidf = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_train_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        train_data.append(doc['x_list'])               \n",
    "        y_train_tfidf.append(doc['y'])\n",
    "        \n",
    "serialized_docs_test_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/test_all/'\n",
    "filenames = os.listdir(serialized_docs_test_dir)\n",
    "\n",
    "test_data = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_test_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        test_data.append(doc['x_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the tokens so they can be give to tf-idf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []\n",
    "y_train_doc = []\n",
    "\n",
    "for idx, doc in enumerate(train_data):\n",
    "    if len(doc) != 0:\n",
    "        train_text.append(' '.join(doc))\n",
    "        y_train_doc.append(y_train_tfidf[idx])\n",
    "y_train_doc = np.array(y_train_doc)\n",
    "\n",
    "test_text = []\n",
    "no_text_idx = []\n",
    "text_idx = []\n",
    "for idx, doc in enumerate(test_data):\n",
    "    if len(doc) != 0:          \n",
    "        test_text.append(' '.join(doc))\n",
    "        text_idx.append(idx)\n",
    "    else:        \n",
    "        no_text_idx.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tf-df features for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = np.concatenate((train_text, test_text))\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    decode_error='ignore',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.7,\n",
    "    min_df=0.02,\n",
    "    lowercase=False,\n",
    "    max_features=5000)\n",
    "word_vectorizer.fit(all_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    decode_error='ignore',\n",
    "    analyzer='char',\n",
    "    max_df=0.5,\n",
    "    min_df=0.2,\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=3000,\n",
    "    lowercase=False)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f1693f50ffce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my_train_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_hosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel, LsiModel, LdaMulticore\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "df_train = []\n",
    "y_train_df = []\n",
    "df_test = []\n",
    "y_test_df = []\n",
    "\n",
    "for i in range(len(train_hosts)):\n",
    "    if len(train_data[i])>0:\n",
    "        df_train.append(train_data[i])\n",
    "        y_train_df.append(y_train[i])\n",
    "\n",
    "for i in range(len(test_hosts)):\n",
    "    if len(test_data[i])>0:\n",
    "        df_test.append(train_data[i])\n",
    "        y_test_df.append(y_train[i])\n",
    "\n",
    "comb = df_train.copy()\n",
    "comb.extend(df_test)\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(comb)\n",
    "\n",
    "id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "id2word.compactify()\n",
    "\n",
    "corpus_train = [id2word.doc2bow(text) for text in df_train]\n",
    "\n",
    "tfidf_train = TfidfModel(corpus_train, ) \n",
    "print('Train: ', tfidf_train)\n",
    "\n",
    "corpus_tfidf_train = tfidf[corpus_train]\n",
    "\n",
    "corpus_test = [id2word.doc2bow(text) for text in df_test]\n",
    "\n",
    "tfidf_test = TfidfModel(corpus_test, ) \n",
    "print('Test: ', tfidf_test)\n",
    "\n",
    "corpus_tfidf_test = tfidf[corpus_test]\n",
    "\n",
    "lda = LdaMulticore(\n",
    "                   corpus=corpus_tfidf_train,\n",
    "                   num_topics=500,\n",
    "                   id2word=id2word,\n",
    "                   chunksize=100,\n",
    "                   workers=7, # Num. Processing Cores - 1\n",
    "                   passes=50,\n",
    "                   eval_every = 1,\n",
    "                   per_word_topics=True)\n",
    "\n",
    "# pickle_out = open('lda_model' + '.pickle_' + str(0),\"wb\")\n",
    "# pickle.dump(lda, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "X_train_lda = np.zeros((len(corpus_train), 500))\n",
    "for i in range(len(corpus_train)):\n",
    "    top_topics = lda.get_document_topics(corpus_train[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(500)]\n",
    "    X_train_lda[i] = topic_vec\n",
    "    \n",
    "    \n",
    "X_test_lda = np.zeros((len(corpus_test), 500))\n",
    "for i in range(len(corpus_test)):\n",
    "    top_topics = lda.get_document_topics(corpus_test[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(500)]\n",
    "    X_test_lda[i] = topic_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "all_text = np.concatenate((train_data, test_data))\n",
    "result = get_bm25_weights(all_text, n_jobs=-1)\n",
    "\n",
    "x_train = result[:800]\n",
    "x_test = result[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA on tf-idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000, random_state=4321)\n",
    "train_features_svd = svd.fit_transform(train_features)\n",
    "test_features_svd = svd.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LSA_1000_train\n",
    "# path_train_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/train_LSA'\n",
    "# with open(path_train_tfidf, 'rb') as file:\n",
    "#         train_features_svd = pickle.load(file)\n",
    "        \n",
    "# #LSA_100_test    \n",
    "# path_test_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/test_LSA'\n",
    "# with open(path_test_tfidf, 'rb') as file:\n",
    "#         test_features_svd = pickle.load(file)\n",
    "        \n",
    "#tfidf_8000_train\n",
    "path_train_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/train_tf_idf'\n",
    "with open(path_train_tfidf, 'rb') as file:\n",
    "        train_features = pickle.load(file)\n",
    "        \n",
    "#tfidf_8000_test    \n",
    "path_test_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/test_tf_idf'\n",
    "with open(path_test_tfidf, 'rb') as file:\n",
    "        test_features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y = [np.argmax(t) for t in y_train_doc]\n",
    "x_indicies = list(range(0, 676))\n",
    "\n",
    "X_tra, X_val, y_tra_class, y_val_class = train_test_split(x_indicies, y, train_size=0.80, random_state=2, stratify=y)\n",
    "\n",
    "# y_tra = np.take(y_train_doc, X_tra, axis=0)\n",
    "# y_val = np.take(y_train_doc, X_val, axis=0) #X_val[70:]\n",
    "\n",
    "# # tmp_v = X_val[:70]\n",
    "# # temp_x = train_features.tocsr()[tmp_v,:]\n",
    "# # temp_y = np.take(y_train_doc, tmp_v, axis=0)\n",
    "\n",
    "# X_tra = train_features.tocsr()[X_tra,:]\n",
    "# X_val = train_features.tocsr()[X_val,:] #[X_val[70:],:]\n",
    "\n",
    "#TSVD tf-idf\n",
    "y_tra = np.take(y_train_doc, X_tra, axis=0)\n",
    "y_val = np.take(y_train_doc, X_val, axis=0)\n",
    "\n",
    "# tmp_v = X_val[:70]\n",
    "# temp_x = np.take(train_features_svd, tmp_v, axis=0)\n",
    "# temp_y = np.take(y_train_doc, tmp_v, axis=0)\n",
    "\n",
    "X_tra = np.take(train_features_svd, X_tra, axis=0)\n",
    "X_val = np.take(train_features_svd, X_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9868421052631579\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_va_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-db35b6ba2228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tra_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_va_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_va_class' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xgb = XGBClassifier(objective ='multi:softprob', learning_rate = 0.01,\n",
    "                     max_depth = 5, alpha = 10, n_estimators = 100,\n",
    "                     eval_metris='mlogloss',\n",
    "                     reg_lambda=0.8\n",
    "                     )\n",
    "\n",
    "xgb.fit(X_tra, y_tra_class)\n",
    "y_pred_train = xgb.predict_proba(X_tra)\n",
    "y_pred_val = xgb.predict_proba(X_val)        \n",
    "\n",
    "print(xgb.score(X_tra, y_tra_class))\n",
    "print(xgb.score(X_val, y_va_class))\n",
    "\n",
    "print('Loss')\n",
    "print(log_loss(y_tra_class, y_pred_train))\n",
    "print(log_loss(y_val_class, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.45      0.36      0.40        14\n",
      "           2       0.57      0.57      0.57        14\n",
      "           3       0.67      0.92      0.77        13\n",
      "           4       0.35      0.40      0.38        15\n",
      "\n",
      "   micro avg       0.56      0.56      0.56        68\n",
      "   macro avg       0.58      0.57      0.56        68\n",
      "weighted avg       0.57      0.56      0.55        68\n",
      "\n",
      "Multi-Class Log-loss:  1.4588326480800693\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_class, np.argmax(y_pred, axis=1)))\n",
    "print('Multi-Class Log-loss: ',log_loss(y_val_class, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ll\n",
      "0.8506856713731347\n",
      "1.0800381862364583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from statistics import mean \n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seeds = [random.randint(1, 20000) for _ in range(2)]\n",
    "res_train = []\n",
    "res_val = []\n",
    "x_indicies = list(range(0, 676))\n",
    "\n",
    "for idx in range(len(seeds)):  \n",
    "    \n",
    "    X_tra, X_val, y_tra_class, y_val_class = train_test_split(x_indicies, y, train_size=0.90, random_state=33, stratify=y)\n",
    "    \n",
    "    #TSVD tf-idf\n",
    "    y_tra_l = np.take(y_train_doc, X_tra, axis=0)\n",
    "    y_val_l = np.take(y_train_doc, X_val, axis=0)\n",
    "\n",
    "    X_tra = np.take(train_features_svd, X_tra, axis=0)\n",
    "    X_val = np.take(train_features_svd, X_val, axis=0)\n",
    "\n",
    "    svc = SVC(probability=True, C=1.1, kernel='rbf', gamma=0.035, random_state=5) #C=1.3, gamma=0.4\n",
    "    svc.fit(X_tra, y_tra_class)\n",
    "\n",
    "    y_pred_train = svc.predict_proba(X_tra)\n",
    "    y_pred_val = svc.predict_proba(X_val)\n",
    "    \n",
    "    res_train.append(log_loss(y_tra_l, y_pred_train))\n",
    "    res_val.append(log_loss(y_val_l, y_pred_val))\n",
    "\n",
    "print('Mean ll')\n",
    "print(mean(res_train))\n",
    "print(mean(res_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN Mlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(\n",
    "    input_size,\n",
    "    optimizer,    \n",
    "    classes=5,  \n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    hidden_layers=1,\n",
    "    units = 600,\n",
    "    dropout_rate = 0.5,\n",
    "    funnel = True,\n",
    "    hidden_activation='relu',\n",
    "    output_activation='softmax'\n",
    "):\n",
    "  \n",
    "    # Define the seed1for numpy and Tensorflow to have reproducible experiments.\n",
    "    np.random.seed(1402) \n",
    "    set_random_seed(2)\n",
    "       \n",
    "    input = Input(\n",
    "        shape=(input_size,),\n",
    "        name='Input'\n",
    "    )\n",
    "    x = input\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "    print(x.shape)\n",
    "    # Define the hidden layers.\n",
    "    for i in range(hidden_layers):\n",
    "        if funnel:\n",
    "            layer_units=units // (i+1)\n",
    "        else: \n",
    "            layer_units=units\n",
    "        x = Dense(\n",
    "           units=layer_units,\n",
    "           kernel_initializer='glorot_uniform',\n",
    "           activation=hidden_activation,\n",
    "           name='Hidden-{0:d}'.format(i + 1)\n",
    "        )(x)\n",
    "     #   x = BatchNormalization()(x)\n",
    "     #   x = BatchNormalization(x)\n",
    "        #Dropout\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "        \n",
    "    # Define the output layer.    \n",
    "    output = Dense(\n",
    "        units=classes,\n",
    "        kernel_initializer='uniform',\n",
    "        activation=output_activation,\n",
    "        name='Output'\n",
    "    )(x)\n",
    "    # Define the model and train it.\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "      \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_crossentropy', 'accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 676)\n",
      "Train on 540 samples, validate on 136 samples\n",
      "Epoch 1/200\n",
      "540/540 [==============================] - ETA: 3s - loss: 1.6038 - categorical_crossentropy: 1.6038 - acc: 0.23 - 1s 2ms/step - loss: 1.6074 - categorical_crossentropy: 1.6074 - acc: 0.2389 - val_loss: 1.6049 - val_categorical_crossentropy: 1.6049 - val_acc: 0.3603\n",
      "Epoch 2/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.6082 - categorical_crossentropy: 1.6082 - acc: 0.24 - 0s 131us/step - loss: 1.6080 - categorical_crossentropy: 1.6080 - acc: 0.2352 - val_loss: 1.6004 - val_categorical_crossentropy: 1.6004 - val_acc: 0.3529\n",
      "Epoch 3/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.6030 - categorical_crossentropy: 1.6030 - acc: 0.21 - 0s 157us/step - loss: 1.5986 - categorical_crossentropy: 1.5986 - acc: 0.2556 - val_loss: 1.5961 - val_categorical_crossentropy: 1.5961 - val_acc: 0.3676\n",
      "Epoch 4/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5983 - categorical_crossentropy: 1.5983 - acc: 0.24 - ETA: 0s - loss: 1.5963 - categorical_crossentropy: 1.5963 - acc: 0.27 - 0s 225us/step - loss: 1.5962 - categorical_crossentropy: 1.5962 - acc: 0.2741 - val_loss: 1.5916 - val_categorical_crossentropy: 1.5916 - val_acc: 0.3603\n",
      "Epoch 5/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5962 - categorical_crossentropy: 1.5962 - acc: 0.28 - ETA: 0s - loss: 1.5899 - categorical_crossentropy: 1.5899 - acc: 0.30 - 0s 222us/step - loss: 1.5896 - categorical_crossentropy: 1.5896 - acc: 0.3056 - val_loss: 1.5870 - val_categorical_crossentropy: 1.5870 - val_acc: 0.3676\n",
      "Epoch 6/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5962 - categorical_crossentropy: 1.5962 - acc: 0.25 - ETA: 0s - loss: 1.5872 - categorical_crossentropy: 1.5872 - acc: 0.30 - 0s 277us/step - loss: 1.5877 - categorical_crossentropy: 1.5877 - acc: 0.3130 - val_loss: 1.5822 - val_categorical_crossentropy: 1.5822 - val_acc: 0.3824\n",
      "Epoch 7/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5798 - categorical_crossentropy: 1.5798 - acc: 0.32 - ETA: 0s - loss: 1.5793 - categorical_crossentropy: 1.5793 - acc: 0.33 - 0s 235us/step - loss: 1.5796 - categorical_crossentropy: 1.5796 - acc: 0.3315 - val_loss: 1.5771 - val_categorical_crossentropy: 1.5771 - val_acc: 0.3750\n",
      "Epoch 8/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5674 - categorical_crossentropy: 1.5674 - acc: 0.42 - ETA: 0s - loss: 1.5704 - categorical_crossentropy: 1.5704 - acc: 0.36 - 0s 249us/step - loss: 1.5725 - categorical_crossentropy: 1.5725 - acc: 0.3611 - val_loss: 1.5715 - val_categorical_crossentropy: 1.5715 - val_acc: 0.3824\n",
      "Epoch 9/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5667 - categorical_crossentropy: 1.5667 - acc: 0.35 - ETA: 0s - loss: 1.5660 - categorical_crossentropy: 1.5660 - acc: 0.39 - 0s 225us/step - loss: 1.5663 - categorical_crossentropy: 1.5663 - acc: 0.3889 - val_loss: 1.5654 - val_categorical_crossentropy: 1.5654 - val_acc: 0.3824\n",
      "Epoch 10/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5629 - categorical_crossentropy: 1.5629 - acc: 0.32 - ETA: 0s - loss: 1.5640 - categorical_crossentropy: 1.5640 - acc: 0.37 - 0s 215us/step - loss: 1.5639 - categorical_crossentropy: 1.5639 - acc: 0.3796 - val_loss: 1.5588 - val_categorical_crossentropy: 1.5588 - val_acc: 0.3824\n",
      "Epoch 11/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5581 - categorical_crossentropy: 1.5581 - acc: 0.38 - ETA: 0s - loss: 1.5531 - categorical_crossentropy: 1.5531 - acc: 0.40 - 0s 244us/step - loss: 1.5542 - categorical_crossentropy: 1.5542 - acc: 0.3963 - val_loss: 1.5516 - val_categorical_crossentropy: 1.5516 - val_acc: 0.4044\n",
      "Epoch 12/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5504 - categorical_crossentropy: 1.5504 - acc: 0.35 - ETA: 0s - loss: 1.5482 - categorical_crossentropy: 1.5482 - acc: 0.38 - 0s 218us/step - loss: 1.5482 - categorical_crossentropy: 1.5482 - acc: 0.3778 - val_loss: 1.5439 - val_categorical_crossentropy: 1.5439 - val_acc: 0.4265\n",
      "Epoch 13/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5455 - categorical_crossentropy: 1.5455 - acc: 0.45 - ETA: 0s - loss: 1.5420 - categorical_crossentropy: 1.5420 - acc: 0.45 - 0s 231us/step - loss: 1.5426 - categorical_crossentropy: 1.5426 - acc: 0.4537 - val_loss: 1.5353 - val_categorical_crossentropy: 1.5353 - val_acc: 0.4779\n",
      "Epoch 14/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5232 - categorical_crossentropy: 1.5232 - acc: 0.50 - ETA: 0s - loss: 1.5317 - categorical_crossentropy: 1.5317 - acc: 0.48 - 0s 227us/step - loss: 1.5331 - categorical_crossentropy: 1.5331 - acc: 0.4796 - val_loss: 1.5262 - val_categorical_crossentropy: 1.5262 - val_acc: 0.5221\n",
      "Epoch 15/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5368 - categorical_crossentropy: 1.5368 - acc: 0.46 - ETA: 0s - loss: 1.5205 - categorical_crossentropy: 1.5205 - acc: 0.52 - 0s 211us/step - loss: 1.5198 - categorical_crossentropy: 1.5198 - acc: 0.5204 - val_loss: 1.5162 - val_categorical_crossentropy: 1.5162 - val_acc: 0.5588\n",
      "Epoch 16/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5130 - categorical_crossentropy: 1.5130 - acc: 0.53 - ETA: 0s - loss: 1.5132 - categorical_crossentropy: 1.5132 - acc: 0.49 - 0s 210us/step - loss: 1.5146 - categorical_crossentropy: 1.5146 - acc: 0.4907 - val_loss: 1.5050 - val_categorical_crossentropy: 1.5050 - val_acc: 0.5809\n",
      "Epoch 17/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.5030 - categorical_crossentropy: 1.5030 - acc: 0.47 - ETA: 0s - loss: 1.4968 - categorical_crossentropy: 1.4968 - acc: 0.52 - 0s 215us/step - loss: 1.4970 - categorical_crossentropy: 1.4970 - acc: 0.5278 - val_loss: 1.4930 - val_categorical_crossentropy: 1.4930 - val_acc: 0.5809\n",
      "Epoch 18/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4949 - categorical_crossentropy: 1.4949 - acc: 0.50 - ETA: 0s - loss: 1.4825 - categorical_crossentropy: 1.4825 - acc: 0.56 - 0s 211us/step - loss: 1.4825 - categorical_crossentropy: 1.4825 - acc: 0.5630 - val_loss: 1.4799 - val_categorical_crossentropy: 1.4799 - val_acc: 0.5809\n",
      "Epoch 19/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4841 - categorical_crossentropy: 1.4841 - acc: 0.55 - ETA: 0s - loss: 1.4733 - categorical_crossentropy: 1.4733 - acc: 0.58 - 0s 262us/step - loss: 1.4721 - categorical_crossentropy: 1.4721 - acc: 0.5889 - val_loss: 1.4651 - val_categorical_crossentropy: 1.4651 - val_acc: 0.5882\n",
      "Epoch 20/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4745 - categorical_crossentropy: 1.4745 - acc: 0.55 - ETA: 0s - loss: 1.4542 - categorical_crossentropy: 1.4542 - acc: 0.57 - 0s 222us/step - loss: 1.4529 - categorical_crossentropy: 1.4529 - acc: 0.5722 - val_loss: 1.4491 - val_categorical_crossentropy: 1.4491 - val_acc: 0.5956\n",
      "Epoch 21/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4356 - categorical_crossentropy: 1.4356 - acc: 0.60 - ETA: 0s - loss: 1.4414 - categorical_crossentropy: 1.4414 - acc: 0.59 - 0s 286us/step - loss: 1.4394 - categorical_crossentropy: 1.4394 - acc: 0.6037 - val_loss: 1.4310 - val_categorical_crossentropy: 1.4310 - val_acc: 0.5956\n",
      "Epoch 22/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4272 - categorical_crossentropy: 1.4272 - acc: 0.67 - ETA: 0s - loss: 1.4207 - categorical_crossentropy: 1.4207 - acc: 0.63 - 0s 232us/step - loss: 1.4191 - categorical_crossentropy: 1.4191 - acc: 0.6352 - val_loss: 1.4116 - val_categorical_crossentropy: 1.4116 - val_acc: 0.5956\n",
      "Epoch 23/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.4053 - categorical_crossentropy: 1.4053 - acc: 0.57 - ETA: 0s - loss: 1.3868 - categorical_crossentropy: 1.3868 - acc: 0.64 - 0s 229us/step - loss: 1.3885 - categorical_crossentropy: 1.3885 - acc: 0.6463 - val_loss: 1.3908 - val_categorical_crossentropy: 1.3908 - val_acc: 0.6103\n",
      "Epoch 24/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.3938 - categorical_crossentropy: 1.3938 - acc: 0.63 - ETA: 0s - loss: 1.3679 - categorical_crossentropy: 1.3679 - acc: 0.67 - 0s 212us/step - loss: 1.3636 - categorical_crossentropy: 1.3636 - acc: 0.6796 - val_loss: 1.3689 - val_categorical_crossentropy: 1.3689 - val_acc: 0.6103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.3604 - categorical_crossentropy: 1.3604 - acc: 0.64 - ETA: 0s - loss: 1.3413 - categorical_crossentropy: 1.3413 - acc: 0.65 - 0s 211us/step - loss: 1.3419 - categorical_crossentropy: 1.3419 - acc: 0.6463 - val_loss: 1.3451 - val_categorical_crossentropy: 1.3451 - val_acc: 0.6324\n",
      "Epoch 26/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.3307 - categorical_crossentropy: 1.3307 - acc: 0.62 - ETA: 0s - loss: 1.3107 - categorical_crossentropy: 1.3107 - acc: 0.65 - 0s 253us/step - loss: 1.3089 - categorical_crossentropy: 1.3089 - acc: 0.6648 - val_loss: 1.3204 - val_categorical_crossentropy: 1.3204 - val_acc: 0.6176\n",
      "Epoch 27/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.3175 - categorical_crossentropy: 1.3175 - acc: 0.70 - ETA: 0s - loss: 1.2882 - categorical_crossentropy: 1.2882 - acc: 0.66 - 0s 251us/step - loss: 1.2869 - categorical_crossentropy: 1.2869 - acc: 0.6611 - val_loss: 1.2946 - val_categorical_crossentropy: 1.2946 - val_acc: 0.6250\n",
      "Epoch 28/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.2269 - categorical_crossentropy: 1.2269 - acc: 0.69 - ETA: 0s - loss: 1.2364 - categorical_crossentropy: 1.2364 - acc: 0.67 - 0s 249us/step - loss: 1.2481 - categorical_crossentropy: 1.2481 - acc: 0.6611 - val_loss: 1.2677 - val_categorical_crossentropy: 1.2677 - val_acc: 0.6471\n",
      "Epoch 29/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.2274 - categorical_crossentropy: 1.2274 - acc: 0.71 - ETA: 0s - loss: 1.2160 - categorical_crossentropy: 1.2160 - acc: 0.68 - 0s 238us/step - loss: 1.2173 - categorical_crossentropy: 1.2173 - acc: 0.6833 - val_loss: 1.2403 - val_categorical_crossentropy: 1.2403 - val_acc: 0.6250\n",
      "Epoch 30/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1663 - categorical_crossentropy: 1.1663 - acc: 0.68 - ETA: 0s - loss: 1.1577 - categorical_crossentropy: 1.1577 - acc: 0.71 - 0s 268us/step - loss: 1.1662 - categorical_crossentropy: 1.1662 - acc: 0.6907 - val_loss: 1.2127 - val_categorical_crossentropy: 1.2127 - val_acc: 0.6176\n",
      "Epoch 31/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1791 - categorical_crossentropy: 1.1791 - acc: 0.67 - ETA: 0s - loss: 1.1381 - categorical_crossentropy: 1.1381 - acc: 0.70 - 0s 227us/step - loss: 1.1368 - categorical_crossentropy: 1.1368 - acc: 0.7000 - val_loss: 1.1844 - val_categorical_crossentropy: 1.1844 - val_acc: 0.6324\n",
      "Epoch 32/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1138 - categorical_crossentropy: 1.1138 - acc: 0.72 - ETA: 0s - loss: 1.1058 - categorical_crossentropy: 1.1058 - acc: 0.69 - 0s 238us/step - loss: 1.1049 - categorical_crossentropy: 1.1049 - acc: 0.6926 - val_loss: 1.1563 - val_categorical_crossentropy: 1.1563 - val_acc: 0.6691\n",
      "Epoch 33/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1086 - categorical_crossentropy: 1.1086 - acc: 0.69 - ETA: 0s - loss: 1.0814 - categorical_crossentropy: 1.0814 - acc: 0.70 - 0s 233us/step - loss: 1.0789 - categorical_crossentropy: 1.0789 - acc: 0.7111 - val_loss: 1.1291 - val_categorical_crossentropy: 1.1291 - val_acc: 0.6912\n",
      "Epoch 34/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 1.1378 - categorical_crossentropy: 1.1378 - acc: 0.62 - ETA: 0s - loss: 1.0565 - categorical_crossentropy: 1.0565 - acc: 0.67 - 0s 240us/step - loss: 1.0428 - categorical_crossentropy: 1.0428 - acc: 0.7000 - val_loss: 1.1026 - val_categorical_crossentropy: 1.1026 - val_acc: 0.6985\n",
      "Epoch 35/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.9909 - categorical_crossentropy: 0.9909 - acc: 0.73 - ETA: 0s - loss: 0.9927 - categorical_crossentropy: 0.9927 - acc: 0.72 - 0s 268us/step - loss: 0.9924 - categorical_crossentropy: 0.9924 - acc: 0.7241 - val_loss: 1.0767 - val_categorical_crossentropy: 1.0767 - val_acc: 0.6985\n",
      "Epoch 36/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.9621 - categorical_crossentropy: 0.9621 - acc: 0.76 - ETA: 0s - loss: 0.9562 - categorical_crossentropy: 0.9562 - acc: 0.74 - 0s 235us/step - loss: 0.9444 - categorical_crossentropy: 0.9444 - acc: 0.7481 - val_loss: 1.0524 - val_categorical_crossentropy: 1.0524 - val_acc: 0.7059\n",
      "Epoch 37/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.9602 - categorical_crossentropy: 0.9602 - acc: 0.73 - ETA: 0s - loss: 0.9260 - categorical_crossentropy: 0.9260 - acc: 0.76 - 0s 222us/step - loss: 0.9247 - categorical_crossentropy: 0.9247 - acc: 0.7667 - val_loss: 1.0292 - val_categorical_crossentropy: 1.0292 - val_acc: 0.7059\n",
      "Epoch 38/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8959 - categorical_crossentropy: 0.8959 - acc: 0.78 - ETA: 0s - loss: 0.8804 - categorical_crossentropy: 0.8804 - acc: 0.78 - 0s 259us/step - loss: 0.8816 - categorical_crossentropy: 0.8816 - acc: 0.7889 - val_loss: 1.0079 - val_categorical_crossentropy: 1.0079 - val_acc: 0.7059\n",
      "Epoch 39/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8472 - categorical_crossentropy: 0.8472 - acc: 0.78 - ETA: 0s - loss: 0.8420 - categorical_crossentropy: 0.8420 - acc: 0.77 - 0s 255us/step - loss: 0.8444 - categorical_crossentropy: 0.8444 - acc: 0.7704 - val_loss: 0.9878 - val_categorical_crossentropy: 0.9878 - val_acc: 0.7059\n",
      "Epoch 40/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8040 - categorical_crossentropy: 0.8040 - acc: 0.78 - ETA: 0s - loss: 0.8129 - categorical_crossentropy: 0.8129 - acc: 0.79 - 0s 212us/step - loss: 0.8080 - categorical_crossentropy: 0.8080 - acc: 0.7889 - val_loss: 0.9688 - val_categorical_crossentropy: 0.9688 - val_acc: 0.7132\n",
      "Epoch 41/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.8002 - categorical_crossentropy: 0.8002 - acc: 0.78 - ETA: 0s - loss: 0.7884 - categorical_crossentropy: 0.7884 - acc: 0.80 - 0s 246us/step - loss: 0.7810 - categorical_crossentropy: 0.7810 - acc: 0.8111 - val_loss: 0.9512 - val_categorical_crossentropy: 0.9512 - val_acc: 0.7206\n",
      "Epoch 42/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.7581 - categorical_crossentropy: 0.7581 - acc: 0.78 - ETA: 0s - loss: 0.7623 - categorical_crossentropy: 0.7623 - acc: 0.81 - 0s 290us/step - loss: 0.7539 - categorical_crossentropy: 0.7539 - acc: 0.8130 - val_loss: 0.9351 - val_categorical_crossentropy: 0.9351 - val_acc: 0.7206\n",
      "Epoch 43/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.7576 - categorical_crossentropy: 0.7576 - acc: 0.83 - ETA: 0s - loss: 0.7239 - categorical_crossentropy: 0.7239 - acc: 0.81 - 0s 214us/step - loss: 0.7251 - categorical_crossentropy: 0.7251 - acc: 0.8056 - val_loss: 0.9210 - val_categorical_crossentropy: 0.9210 - val_acc: 0.7206\n",
      "Epoch 44/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.7209 - categorical_crossentropy: 0.7209 - acc: 0.78 - ETA: 0s - loss: 0.7008 - categorical_crossentropy: 0.7008 - acc: 0.81 - 0s 236us/step - loss: 0.6958 - categorical_crossentropy: 0.6958 - acc: 0.8222 - val_loss: 0.9082 - val_categorical_crossentropy: 0.9082 - val_acc: 0.7279\n",
      "Epoch 45/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6650 - categorical_crossentropy: 0.6650 - acc: 0.83 - ETA: 0s - loss: 0.6517 - categorical_crossentropy: 0.6517 - acc: 0.83 - 0s 255us/step - loss: 0.6797 - categorical_crossentropy: 0.6797 - acc: 0.8241 - val_loss: 0.8955 - val_categorical_crossentropy: 0.8955 - val_acc: 0.7279\n",
      "Epoch 46/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6460 - categorical_crossentropy: 0.6460 - acc: 0.84 - ETA: 0s - loss: 0.6501 - categorical_crossentropy: 0.6501 - acc: 0.82 - 0s 227us/step - loss: 0.6449 - categorical_crossentropy: 0.6449 - acc: 0.8333 - val_loss: 0.8834 - val_categorical_crossentropy: 0.8834 - val_acc: 0.7353\n",
      "Epoch 47/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6121 - categorical_crossentropy: 0.6121 - acc: 0.82 - ETA: 0s - loss: 0.5953 - categorical_crossentropy: 0.5953 - acc: 0.84 - 0s 214us/step - loss: 0.5986 - categorical_crossentropy: 0.5986 - acc: 0.8463 - val_loss: 0.8719 - val_categorical_crossentropy: 0.8719 - val_acc: 0.7279\n",
      "Epoch 48/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6437 - categorical_crossentropy: 0.6437 - acc: 0.84 - ETA: 0s - loss: 0.5847 - categorical_crossentropy: 0.5847 - acc: 0.87 - 0s 212us/step - loss: 0.5935 - categorical_crossentropy: 0.5935 - acc: 0.8704 - val_loss: 0.8613 - val_categorical_crossentropy: 0.8613 - val_acc: 0.7279\n",
      "Epoch 49/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.6180 - categorical_crossentropy: 0.6180 - acc: 0.82 - ETA: 0s - loss: 0.5485 - categorical_crossentropy: 0.5485 - acc: 0.86 - 0s 242us/step - loss: 0.5480 - categorical_crossentropy: 0.5480 - acc: 0.8722 - val_loss: 0.8512 - val_categorical_crossentropy: 0.8512 - val_acc: 0.7353\n",
      "Epoch 50/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.5452 - categorical_crossentropy: 0.5452 - acc: 0.85 - ETA: 0s - loss: 0.5304 - categorical_crossentropy: 0.5304 - acc: 0.87 - 0s 205us/step - loss: 0.5281 - categorical_crossentropy: 0.5281 - acc: 0.8796 - val_loss: 0.8430 - val_categorical_crossentropy: 0.8430 - val_acc: 0.7279\n",
      "Epoch 51/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.5222 - categorical_crossentropy: 0.5222 - acc: 0.89 - ETA: 0s - loss: 0.5159 - categorical_crossentropy: 0.5159 - acc: 0.89 - 0s 220us/step - loss: 0.5133 - categorical_crossentropy: 0.5133 - acc: 0.8944 - val_loss: 0.8358 - val_categorical_crossentropy: 0.8358 - val_acc: 0.7206\n",
      "Epoch 52/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4851 - categorical_crossentropy: 0.4851 - acc: 0.92 - ETA: 0s - loss: 0.4879 - categorical_crossentropy: 0.4879 - acc: 0.90 - 0s 273us/step - loss: 0.4904 - categorical_crossentropy: 0.4904 - acc: 0.9019 - val_loss: 0.8286 - val_categorical_crossentropy: 0.8286 - val_acc: 0.7206\n",
      "Epoch 53/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4522 - categorical_crossentropy: 0.4522 - acc: 0.89 - ETA: 0s - loss: 0.4381 - categorical_crossentropy: 0.4381 - acc: 0.91 - 0s 253us/step - loss: 0.4401 - categorical_crossentropy: 0.4401 - acc: 0.9111 - val_loss: 0.8226 - val_categorical_crossentropy: 0.8226 - val_acc: 0.7206\n",
      "Epoch 54/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4576 - categorical_crossentropy: 0.4576 - acc: 0.88 - ETA: 0s - loss: 0.4492 - categorical_crossentropy: 0.4492 - acc: 0.90 - 0s 233us/step - loss: 0.4524 - categorical_crossentropy: 0.4524 - acc: 0.9056 - val_loss: 0.8161 - val_categorical_crossentropy: 0.8161 - val_acc: 0.7279\n",
      "Epoch 55/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4527 - categorical_crossentropy: 0.4527 - acc: 0.91 - ETA: 0s - loss: 0.4227 - categorical_crossentropy: 0.4227 - acc: 0.91 - 0s 225us/step - loss: 0.4288 - categorical_crossentropy: 0.4288 - acc: 0.9167 - val_loss: 0.8108 - val_categorical_crossentropy: 0.8108 - val_acc: 0.7279\n",
      "Epoch 56/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.4174 - categorical_crossentropy: 0.4174 - acc: 0.90 - ETA: 0s - loss: 0.3984 - categorical_crossentropy: 0.3984 - acc: 0.93 - 0s 268us/step - loss: 0.3867 - categorical_crossentropy: 0.3867 - acc: 0.9296 - val_loss: 0.8059 - val_categorical_crossentropy: 0.8059 - val_acc: 0.7353\n",
      "Epoch 57/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3480 - categorical_crossentropy: 0.3480 - acc: 0.92 - ETA: 0s - loss: 0.3830 - categorical_crossentropy: 0.3830 - acc: 0.92 - 0s 240us/step - loss: 0.3841 - categorical_crossentropy: 0.3841 - acc: 0.9259 - val_loss: 0.8012 - val_categorical_crossentropy: 0.8012 - val_acc: 0.7426\n",
      "Epoch 58/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3918 - categorical_crossentropy: 0.3918 - acc: 0.93 - ETA: 0s - loss: 0.3607 - categorical_crossentropy: 0.3607 - acc: 0.93 - 0s 227us/step - loss: 0.3522 - categorical_crossentropy: 0.3522 - acc: 0.9407 - val_loss: 0.7961 - val_categorical_crossentropy: 0.7961 - val_acc: 0.7426\n",
      "Epoch 59/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3801 - categorical_crossentropy: 0.3801 - acc: 0.94 - ETA: 0s - loss: 0.3615 - categorical_crossentropy: 0.3615 - acc: 0.93 - 0s 226us/step - loss: 0.3628 - categorical_crossentropy: 0.3628 - acc: 0.9315 - val_loss: 0.7927 - val_categorical_crossentropy: 0.7927 - val_acc: 0.7353\n",
      "Epoch 60/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3477 - categorical_crossentropy: 0.3477 - acc: 0.92 - ETA: 0s - loss: 0.3365 - categorical_crossentropy: 0.3365 - acc: 0.93 - 0s 240us/step - loss: 0.3245 - categorical_crossentropy: 0.3245 - acc: 0.9444 - val_loss: 0.7910 - val_categorical_crossentropy: 0.7910 - val_acc: 0.7426\n",
      "Epoch 61/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.3381 - categorical_crossentropy: 0.3381 - acc: 0.92 - ETA: 0s - loss: 0.3239 - categorical_crossentropy: 0.3239 - acc: 0.93 - ETA: 0s - loss: 0.3175 - categorical_crossentropy: 0.3175 - acc: 0.93 - 0s 386us/step - loss: 0.3208 - categorical_crossentropy: 0.3208 - acc: 0.9315 - val_loss: 0.7895 - val_categorical_crossentropy: 0.7895 - val_acc: 0.7353\n",
      "Epoch 62/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2629 - categorical_crossentropy: 0.2629 - acc: 0.96 - ETA: 0s - loss: 0.3089 - categorical_crossentropy: 0.3089 - acc: 0.93 - 0s 259us/step - loss: 0.3070 - categorical_crossentropy: 0.3070 - acc: 0.9352 - val_loss: 0.7865 - val_categorical_crossentropy: 0.7865 - val_acc: 0.7279\n",
      "Epoch 63/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2866 - categorical_crossentropy: 0.2866 - acc: 0.93 - ETA: 0s - loss: 0.2829 - categorical_crossentropy: 0.2829 - acc: 0.94 - 0s 233us/step - loss: 0.2835 - categorical_crossentropy: 0.2835 - acc: 0.9444 - val_loss: 0.7811 - val_categorical_crossentropy: 0.7811 - val_acc: 0.7353\n",
      "Epoch 64/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2913 - categorical_crossentropy: 0.2913 - acc: 0.94 - ETA: 0s - loss: 0.2764 - categorical_crossentropy: 0.2764 - acc: 0.95 - 0s 223us/step - loss: 0.2767 - categorical_crossentropy: 0.2767 - acc: 0.9519 - val_loss: 0.7754 - val_categorical_crossentropy: 0.7754 - val_acc: 0.7353\n",
      "Epoch 65/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2134 - categorical_crossentropy: 0.2134 - acc: 1.00 - ETA: 0s - loss: 0.2378 - categorical_crossentropy: 0.2378 - acc: 0.97 - 0s 251us/step - loss: 0.2458 - categorical_crossentropy: 0.2458 - acc: 0.9741 - val_loss: 0.7720 - val_categorical_crossentropy: 0.7720 - val_acc: 0.7279\n",
      "Epoch 66/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2313 - categorical_crossentropy: 0.2313 - acc: 0.98 - ETA: 0s - loss: 0.2323 - categorical_crossentropy: 0.2323 - acc: 0.96 - 0s 214us/step - loss: 0.2324 - categorical_crossentropy: 0.2324 - acc: 0.9648 - val_loss: 0.7699 - val_categorical_crossentropy: 0.7699 - val_acc: 0.7353\n",
      "Epoch 67/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2378 - categorical_crossentropy: 0.2378 - acc: 0.96 - ETA: 0s - loss: 0.2405 - categorical_crossentropy: 0.2405 - acc: 0.96 - 0s 209us/step - loss: 0.2408 - categorical_crossentropy: 0.2408 - acc: 0.9593 - val_loss: 0.7700 - val_categorical_crossentropy: 0.7700 - val_acc: 0.7353\n",
      "Epoch 68/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1967 - categorical_crossentropy: 0.1967 - acc: 0.96 - ETA: 0s - loss: 0.1968 - categorical_crossentropy: 0.1968 - acc: 0.98 - 0s 223us/step - loss: 0.1964 - categorical_crossentropy: 0.1964 - acc: 0.9815 - val_loss: 0.7720 - val_categorical_crossentropy: 0.7720 - val_acc: 0.7353\n",
      "Epoch 69/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1876 - categorical_crossentropy: 0.1876 - acc: 0.97 - ETA: 0s - loss: 0.2091 - categorical_crossentropy: 0.2091 - acc: 0.96 - 0s 222us/step - loss: 0.2087 - categorical_crossentropy: 0.2087 - acc: 0.9685 - val_loss: 0.7720 - val_categorical_crossentropy: 0.7720 - val_acc: 0.7353\n",
      "Epoch 70/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1971 - categorical_crossentropy: 0.1971 - acc: 0.98 - ETA: 0s - loss: 0.1990 - categorical_crossentropy: 0.1990 - acc: 0.97 - 0s 235us/step - loss: 0.1964 - categorical_crossentropy: 0.1964 - acc: 0.9778 - val_loss: 0.7701 - val_categorical_crossentropy: 0.7701 - val_acc: 0.7279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.2033 - categorical_crossentropy: 0.2033 - acc: 0.96 - ETA: 0s - loss: 0.1971 - categorical_crossentropy: 0.1971 - acc: 0.97 - 0s 225us/step - loss: 0.1939 - categorical_crossentropy: 0.1939 - acc: 0.9741 - val_loss: 0.7657 - val_categorical_crossentropy: 0.7657 - val_acc: 0.7353\n",
      "Epoch 72/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1484 - categorical_crossentropy: 0.1484 - acc: 0.99 - ETA: 0s - loss: 0.1810 - categorical_crossentropy: 0.1810 - acc: 0.98 - 0s 251us/step - loss: 0.1847 - categorical_crossentropy: 0.1847 - acc: 0.9796 - val_loss: 0.7628 - val_categorical_crossentropy: 0.7628 - val_acc: 0.7206\n",
      "Epoch 73/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1740 - categorical_crossentropy: 0.1740 - acc: 0.99 - ETA: 0s - loss: 0.1578 - categorical_crossentropy: 0.1578 - acc: 0.98 - 0s 201us/step - loss: 0.1590 - categorical_crossentropy: 0.1590 - acc: 0.9852 - val_loss: 0.7647 - val_categorical_crossentropy: 0.7647 - val_acc: 0.7206\n",
      "Epoch 74/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1411 - categorical_crossentropy: 0.1411 - acc: 0.97 - ETA: 0s - loss: 0.1498 - categorical_crossentropy: 0.1498 - acc: 0.97 - 0s 225us/step - loss: 0.1523 - categorical_crossentropy: 0.1523 - acc: 0.9759 - val_loss: 0.7652 - val_categorical_crossentropy: 0.7652 - val_acc: 0.7206\n",
      "Epoch 75/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1941 - categorical_crossentropy: 0.1941 - acc: 0.96 - ETA: 0s - loss: 0.1692 - categorical_crossentropy: 0.1692 - acc: 0.96 - 0s 283us/step - loss: 0.1664 - categorical_crossentropy: 0.1664 - acc: 0.9685 - val_loss: 0.7630 - val_categorical_crossentropy: 0.7630 - val_acc: 0.7206\n",
      "Epoch 76/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1401 - categorical_crossentropy: 0.1401 - acc: 0.98 - ETA: 0s - loss: 0.1605 - categorical_crossentropy: 0.1605 - acc: 0.96 - 0s 271us/step - loss: 0.1605 - categorical_crossentropy: 0.1605 - acc: 0.9704 - val_loss: 0.7634 - val_categorical_crossentropy: 0.7634 - val_acc: 0.7132\n",
      "Epoch 77/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1512 - categorical_crossentropy: 0.1512 - acc: 0.97 - ETA: 0s - loss: 0.1378 - categorical_crossentropy: 0.1378 - acc: 0.98 - 0s 235us/step - loss: 0.1382 - categorical_crossentropy: 0.1382 - acc: 0.9815 - val_loss: 0.7651 - val_categorical_crossentropy: 0.7651 - val_acc: 0.7132\n",
      "Epoch 78/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1174 - categorical_crossentropy: 0.1174 - acc: 1.00 - ETA: 0s - loss: 0.1280 - categorical_crossentropy: 0.1280 - acc: 0.99 - 0s 227us/step - loss: 0.1301 - categorical_crossentropy: 0.1301 - acc: 0.9926 - val_loss: 0.7681 - val_categorical_crossentropy: 0.7681 - val_acc: 0.7206\n",
      "Epoch 79/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1402 - categorical_crossentropy: 0.1402 - acc: 0.98 - ETA: 0s - loss: 0.1271 - categorical_crossentropy: 0.1271 - acc: 0.99 - 0s 225us/step - loss: 0.1267 - categorical_crossentropy: 0.1267 - acc: 0.9926 - val_loss: 0.7680 - val_categorical_crossentropy: 0.7680 - val_acc: 0.7206\n",
      "Epoch 80/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1049 - categorical_crossentropy: 0.1049 - acc: 0.99 - ETA: 0s - loss: 0.1054 - categorical_crossentropy: 0.1054 - acc: 0.99 - 0s 253us/step - loss: 0.1052 - categorical_crossentropy: 0.1052 - acc: 0.9926 - val_loss: 0.7674 - val_categorical_crossentropy: 0.7674 - val_acc: 0.7206\n",
      "Epoch 81/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1215 - categorical_crossentropy: 0.1215 - acc: 0.99 - ETA: 0s - loss: 0.1110 - categorical_crossentropy: 0.1110 - acc: 0.99 - 0s 250us/step - loss: 0.1131 - categorical_crossentropy: 0.1131 - acc: 0.9926 - val_loss: 0.7666 - val_categorical_crossentropy: 0.7666 - val_acc: 0.7353\n",
      "Epoch 82/200\n",
      "540/540 [==============================] - ETA: 0s - loss: 0.1104 - categorical_crossentropy: 0.1104 - acc: 0.99 - ETA: 0s - loss: 0.1148 - categorical_crossentropy: 0.1148 - acc: 0.98 - 0s 224us/step - loss: 0.1148 - categorical_crossentropy: 0.1148 - acc: 0.9889 - val_loss: 0.7666 - val_categorical_crossentropy: 0.7666 - val_acc: 0.7426\n",
      "Finished training.\n",
      "------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 676)               0         \n",
      "_________________________________________________________________\n",
      "Hidden-1 (Dense)             (None, 1000)              677000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "Hidden-2 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 1,180,005\n",
      "Trainable params: 1,180,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 5\n",
    "epochs = 200\n",
    "optimizer = Adam(lr=0.0002)\n",
    "\n",
    "model_MLP = MLP_model(\n",
    "    optimizer=optimizer,\n",
    "    input_size=X_tra.shape[1],\n",
    "    dropout_rate = 0.7,\n",
    "    hidden_layers=2,\n",
    "    units=1000,\n",
    "    funnel=True\n",
    ")\n",
    "\n",
    "# Keras Callbacks\n",
    "reducer_lr = ReduceLROnPlateau(factor = 0.00002, patience = 10, min_lr = 1e-6, verbose = -1)\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_categorical_crossentropy', mode='min', patience = 10) # Change 4 to 8 in the final run\n",
    "model_file_name = 'tfidf_mlp'\n",
    "check_pointer = keras.callbacks.ModelCheckpoint(model_file_name, monitor='val_categorical_crossentropy', mode='min', verbose = -1, save_best_only = True)  \n",
    "callbacks_list = [early_stopper, reducer_lr, check_pointer]#\n",
    "\n",
    "model_MLP.fit(x=X_tra,\n",
    "              y=y_tra,          \n",
    "              validation_data=(X_val, y_val), \n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              batch_size=batch_size,\n",
    "              callbacks = callbacks_list)\n",
    "\n",
    "# y_test_pred = model_MLP.predict(test_features.tocsr(), batch_size=batch_size)\n",
    "print('Finished training.')\n",
    "print('------------------')    \n",
    "model_MLP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41084"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "del model_MLP\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6817416127205693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "temp_pred = model_MLP.predict(temp_x, batch_size=batch_size)\n",
    "print(log_loss(temp_y, temp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x214fc8be9b0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "test_classes = {0:'athlitismos', 1:'diaskedasi-psyxagogia', 2:'eidiseis-mme',\n",
    "       3:'katastimata-agores', 4:'pliroforiki-diadiktyo'}\n",
    "results_val = [test_classes[i] for i in np.argmax(temp_pred,axis=1)]\n",
    "true_val = [test_classes[i] for i in np.argmax(temp_y,axis=1)]\n",
    "cm = confusion_matrix(true_val,results_val)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_T = model_MLP.predict(X_tra)\n",
    "y_pred_val_T = model_MLP.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_T = model_MLP.predict(test_features.tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from math import log, e\n",
    "import pandas as pd\n",
    "\n",
    "import timeit\n",
    "\n",
    "entropy([0.5,0.5], base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 123 t: 109 g: 56\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "t = 0\n",
    "g = 0\n",
    "y_final_pred = np.zeros((200,5))\n",
    "for idx in range(len(y_pred_test_G)):\n",
    "    if idx in no_text_idx:\n",
    "        y_final_pred[idx] = y_pred_test_G[idx]\n",
    "    else:\n",
    "        graph = y_pred_test_G[idx]\n",
    "        index_text = text_idx.index(idx)\n",
    "        text = y_pred_test_T[index_text]\n",
    "                \n",
    "        if np.argmax(graph)==np.argmax(text):\n",
    "            if entropy(graph)<entropy(text):\n",
    "                y_final_pred[idx] = graph\n",
    "            else:\n",
    "                y_final_pred[idx] = text\n",
    "                t = t+1\n",
    "        else:\n",
    "            y_final_pred[idx] = 0.5*graph + 0.7*text\n",
    "            g += 1\n",
    "    if np.argmax(y_pred_test_T[index_text])==np.argmax(y_pred_test_G[idx]):\n",
    "        c+=1\n",
    "\n",
    "print('c:',c,'t:',t,'g:',g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_G = pd.read_csv('C:/Users/User/Desktop/DS_challenge/notebooks/sample_submission_22.csv').set_index('Host')\n",
    "y_pred_test_G = np.array(y_pred_test_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {'athlitismos':0, 'diaskedasi-psyxagogia':1, 'eidiseis-mme':2, 'katastimata-agores':3, 'pliroforiki-diadiktyo':4}\n",
    "# Write predictions to a file\n",
    "with open('sample_submission_last_1.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = list(class_labels.keys())\n",
    "    lst.insert(0, \"Host\")\n",
    "    writer.writerow(lst)\n",
    "    for i,test_host in enumerate(test_hosts):\n",
    "        lst = y_final_pred[i,:].tolist()\n",
    "        lst.insert(0, test_host)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval of most similar document using the Weisfeiler-Lehman subtree kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GraKeL graphs for train and test documents.\n",
    "We use the tokenized documents that we serialized in the start of the notebook.\n",
    "\n",
    "**We process only nodes that have text!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word co-occurrence networks\n",
      " for train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [00:46, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word co-occurrence networks\n",
      " for test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:15, 12.55it/s]\n"
     ]
    }
   ],
   "source": [
    "y_train_grakel = []\n",
    "y_train_grakel_idx = []\n",
    "print(\"Creating word co-occurrence networks\\n for train data\")\n",
    "word_networks = list()\n",
    "for idx, doc in tqdm(enumerate(train_data)):\n",
    "    node_labels = dict()\n",
    "    tokens_to_ids = dict()\n",
    "    for token in doc:\n",
    "        if token not in tokens_to_ids:\n",
    "            tokens_to_ids[token] = len(tokens_to_ids)\n",
    "            node_labels[tokens_to_ids[token]] = token\n",
    "    \n",
    "    edges = list()\n",
    "    for i in range(len(doc)-1):\n",
    "        edges.append((tokens_to_ids[doc[i]], tokens_to_ids[doc[i+1]]))\n",
    "         \n",
    "    try:        \n",
    "        word_networks.append(Graph(edges, node_labels=node_labels))\n",
    "        y_train_grakel.append(y_train[idx])\n",
    "        y_train_grakel_idx.append(idx)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Creating word co-occurrence networks\\n for test data\")\n",
    "word_networks_test = list()\n",
    "test_ids = []\n",
    "for idx, doc in tqdm(enumerate(test_data)):\n",
    "    node_labels = dict()\n",
    "    tokens_to_ids = dict()\n",
    "    for token in doc:\n",
    "        if token not in tokens_to_ids:\n",
    "            tokens_to_ids[token] = len(tokens_to_ids)\n",
    "            node_labels[tokens_to_ids[token]] = token\n",
    "    \n",
    "    edges = list()\n",
    "    for i in range(len(doc)-1):\n",
    "        edges.append((tokens_to_ids[doc[i]], tokens_to_ids[doc[i+1]]))\n",
    "         \n",
    "    try:        \n",
    "        word_networks_test.append(Graph(edges, node_labels=node_labels))\n",
    "        test_ids.append(idx)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sums the similarities of the top N (20 in our case) closest documents for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct solution:\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    \n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def get_categories_prob(K, y_train_grakel, idx):\n",
    "    K[idx, 0] = 0\n",
    "    similarity_values = np.take(K[:, 0], np.argsort(K[:,0])[-20:]) # 1x10\n",
    "    similarity_category = np.take(y_train_grakel, np.argsort(K[:,0])[-20:], axis=0) # 10x5+\n",
    "    \n",
    "    for idx, similarity in enumerate(similarity_values):\n",
    "        \n",
    "        similarity_category[idx] = similarity_category[idx] * similarity\n",
    "\n",
    "    similarity_category = np.array(np.sum(similarity_category, axis=0))\n",
    "       \n",
    "    category_prob = softmax(similarity_category)\n",
    "    \n",
    "    return similarity_category, category_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gk = WeisfeilerLehman(niter=2, normalize=True, base_kernel=VertexHistogram)\n",
    "#gk = GraphKernel(kernel = [{\"name\": \"weisfeiler_lehman\", \"niter\": 5}, {\"name\": \"subtree_wl\"}], Nystroem=20)\n",
    "\n",
    "# Train documents\n",
    "serialized_docs_train_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/train/'\n",
    "results_train = []\n",
    "for idx, train_graph in tqdm(enumerate(word_networks)):\n",
    "    gk.fit([train_graph])\n",
    "    K = gk.transform(word_networks)    \n",
    "    cat_sum, cat_prob = get_categories_prob(K, y_train_grakel, idx)    \n",
    "        \n",
    "    res = {\n",
    "        'doc_id': y_train_grakel_idx[idx],\n",
    "        'y': y_train_grakel[idx],\n",
    "        'cat_sum': cat_sum,\n",
    "        'cat_prob': cat_prob\n",
    "    }\n",
    "    print()\n",
    "    print(res)\n",
    "        \n",
    "    pickle_out = open(serialized_docs_train_dir + 'dict.pickle_' + str(idx),\"wb\")\n",
    "    pickle.dump(res, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    results_train.append(res)\n",
    "    \n",
    "# Test documents\n",
    "serialized_docs_test_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/test/'\n",
    "results_test = []\n",
    "for idx, test_graph in tqdm(enumerate(word_networks_test)):\n",
    "    gk.fit([test_graph])\n",
    "    K = gk.transform(word_networks)\n",
    "    \n",
    "    cat_sum, cat_prob = get_categories_prob(K, y_train_grakel)\n",
    "    res = {\n",
    "        'doc_id': test_ids[idx],\n",
    "        'true': y_test[test_ids[idx]],\n",
    "        'cat_sum': cat_sum,\n",
    "        'cat_prob': cat_prob\n",
    "    }\n",
    "    \n",
    "    pickle_out = open(serialized_docs_test_dir + 'dict.pickle_' + str(idx),\"wb\")\n",
    "    pickle.dump(res, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    results_test.append(res)\n",
    "\n",
    "# MEMORY OVERLOAD\n",
    "# from grakel import GraphKernel\n",
    "# gk = WeisfeilerLehman(niter=2, normalize=True, base_kernel=VertexHistogram, n_jobs=-1)\n",
    "# #gk = GraphKernel(kernel = [{\"name\": \"weisfeiler_lehman\", \"niter\": 5}, {\"name\": \"subtree_wl\"}], Nystroem=20)\n",
    "\n",
    "\n",
    "# Κ = gk.fit_transform(word_networks)\n",
    "# K = gk.transform(word_networks_test)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the WL results calculates above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "serialized_docs_test_WL_results_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/train/'\n",
    "filenames = os.listdir(serialized_docs_test_WL_results_dir)\n",
    "\n",
    "train_results = []\n",
    "for file in filenames:\n",
    "    with open(serialized_docs_test_WL_results_dir + file, 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        train_results.append(doc)              \n",
    "\n",
    "serialized_docs_test_WL_results_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/test/'\n",
    "filenames = os.listdir(serialized_docs_test_WL_results_dir)\n",
    "\n",
    "test_results = []\n",
    "for file in filenames:\n",
    "    with open(serialized_docs_test_WL_results_dir + file, 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        test_results.append(doc)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = []\n",
    "y_train_class = []\n",
    "y_train_dum = []\n",
    "train_probs = []\n",
    "for doc in train_results:\n",
    "    x_train.append(doc['cat_sum'])\n",
    "    train_probs.append(doc['cat_prob'])\n",
    "    y_train_class.append(np.where(doc['y']==1)[0][0])\n",
    "    y_train_dum.append(doc['y'])\n",
    "    \n",
    "x_test = np.zeros((200,5))\n",
    "x_test_prob = np.zeros((200,5))\n",
    "for doc in test_results:\n",
    "    x_test[doc['doc_id']] = doc['cat_sum']\n",
    "    x_test_prob[doc['doc_id']] = doc['cat_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a75707e88087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'log_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_train_class, train_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6a80df7aa36a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_indicies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m675\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_tra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_indicies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "x_indicies = list(range(0, 675))\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_indicies, y_train_class, train_size=0.90, random_state=3, stratify=y_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra_WL = np.take(x_train, X_tra, axis=0)\n",
    "X_val_WL = np.take(x_train, X_val, axis=0)\n",
    "\n",
    "y_tra_WL = np.take(y_train_dum ,X_tra, axis=0)\n",
    "y_val_WL = np.take(y_train_dum ,X_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0038, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=2, shrinking=True, tol=0.001,\n",
       "  verbose=False)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(probability=True, C=1, kernel='rbf', gamma=0.0038, random_state=2\n",
    "         )\n",
    "svc.fit(X_tra_WL, y_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_WL = svc.predict_proba(X_tra_WL)\n",
    "y_pred_val_WL = svc.predict_proba(X_val_WL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WL res:\n",
      "0.19232843395578017\n",
      "0.18366625487638547\n"
     ]
    }
   ],
   "source": [
    "print('WL res:')\n",
    "print(log_loss(y_tra_WL, y_pred_train_WL))\n",
    "print(log_loss(y_val_WL, y_pred_val_WL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_WL = svc.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 132 t: 0 g: 0\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "t = 0\n",
    "g = 0\n",
    "y_final_pred = np.zeros((200,5))\n",
    "for idx in range(len(y_pred_test_G)):\n",
    "    if idx in no_text_idx:\n",
    "        y_final_pred[idx] = np.array(y_pred_test_G.iloc[idx])\n",
    "    else:\n",
    "        graph = np.array(y_pred_test_G.iloc[idx])\n",
    "        index_text = text_idx.index(idx)\n",
    "        text = y_test_pred[index_text]\n",
    "#         y_final_pred[idx] = text\n",
    "        if np.argmax(graph)==np.argmax(text):\n",
    "            if max(graph)>max(text):\n",
    "                y_final_pred[idx] = graph\n",
    "                g+=1\n",
    "            else:\n",
    "                y_final_pred[idx] = text\n",
    "                t+=1\n",
    "        else:\n",
    "            y_final_pred[idx] = 0.5*graph + 0.5*text\n",
    "        \n",
    "        \n",
    "    if np.argmax(y_test_pred[index_text])==np.argmax(np.array(y_pred_test_G.iloc[idx])):\n",
    "        c+=1\n",
    "\n",
    "print('c:',c,'t:',t,'g:',g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "all_text = np.concatenate((train_data, test_data))\n",
    "result = get_bm25_weights(all_text, n_jobs=-1)\n",
    "\n",
    "x_train = result[:800]\n",
    "x_test = result[800:]\n",
    "\n",
    "X_train_bm25 = np.zeros((800, 1000))\n",
    "for i in range(800):\n",
    "    X_train_bm25[i] = x_train[i]\n",
    "\n",
    "X_test_bm25 = np.zeros((200, 1000))\n",
    "for i in range(200):\n",
    "    X_test_bm25[i] = x_test[i]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_bm25_scaled = min_max_scaler.fit_transform(X_train_bm25)\n",
    "X_train_bm25 = pd.DataFrame(X_train_bm25_scaled)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_test_bm25_scaled = min_max_scaler.fit_transform(X_test_bm25)\n",
    "X_test_bm25 = pd.DataFrame(X_test_bm25_scaled)\n",
    "\n",
    "classes = {0:'athlitismos', 1:'diaskedasi-psyxagogia', 2:'eidiseis-mme',\n",
    "       3:'katastimata-agores', 4:'pliroforiki-diadiktyo'}\n",
    "\n",
    "y_train_cl = [classes[y_train_class[i]] for i in range(800)]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(X_train_bm25, y_train_cl, train_size=0.7,\n",
    "                                              random_state=245354, stratify=y_train_cl)\n",
    "\n",
    "# C = 4\n",
    "# gamma = 0.1\n",
    "\n",
    "# svc = SVC(probability=True, C=C, kernel='rbf', gamma=gamma, random_state=7)\n",
    "# svc.fit(X_tra, y_tra)\n",
    "# y_pred_train = svc.predict_proba(X_tra)\n",
    "# y_pred_val = svc.predict_proba(X_val)\n",
    "\n",
    "mcw = 10\n",
    "md = 100\n",
    "g = 0\n",
    "\n",
    "xgb = XGBClassifier(min_child_weight = mcw, gamma = g, max_depth = md)\n",
    "xgb.fit(X_tra, y_tra)\n",
    "y_pred_train = xgb.predict_proba(X_tra)\n",
    "y_pred_val = xgb.predict_proba(X_val)\n",
    "\n",
    "\n",
    "# if ((abs(svc.score(X_tra, y_tra) - svc.score(X_val, y_val))) <= 0.06 ) \\\n",
    "#     & (log_loss(y_tra, y_pred_train)<0.97):\n",
    "print('C = {} , gamma = {} '.format(C,gamma))\n",
    "print(svc.score(X_tra, y_tra))\n",
    "print(svc.score(X_val, y_val))\n",
    "print('Loss')\n",
    "print(log_loss(y_tra, y_pred_train))\n",
    "print(log_loss(y_val, y_pred_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
