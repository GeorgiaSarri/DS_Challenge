{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os \n",
    "import gc\n",
    "\n",
    "from random import randint\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tensorflow import set_random_seed\n",
    "from keras.models import load_model\n",
    "import sys\n",
    "sys.path.insert(0,'C:/Users/User/Desktop/DS_challenge/')\n",
    "from GraphEmbedding.ge import DeepWalk\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from statistics import mean \n",
    "import random\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import Dropout,Dense,Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras as k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r'C:\\Users\\User\\Desktop\\DS_challenge\\data\\\\'\n",
    "TRAIN_FILE = dir_path + \"train.csv\"\n",
    "TEST_FILE = dir_path + \"test.csv\"\n",
    "GRAPH_FILE = dir_path + \"edgelist.txt\"\n",
    "domains_path = 'C:/Users/User/Desktop/DS_challenge/data/data_science_challenge_2019/domains/'\n",
    "\n",
    "class_labels = {'athlitismos':0, 'diaskedasi-psyxagogia':1, 'eidiseis-mme':2, 'katastimata-agores':3, 'pliroforiki-diadiktyo':4}\n",
    "\n",
    "train_hosts = list()\n",
    "train_labels = list()\n",
    "with open(TRAIN_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        train_hosts.append(l[0])\n",
    "        train_labels.append(l[1][:-1])\n",
    "\n",
    "y_train_class = []\n",
    "y_train_dum = np.zeros((len(train_hosts), len(class_labels)))\n",
    "for i, train_label in enumerate(train_labels):\n",
    "    y_train_dum[i,class_labels[train_label]] = 1\n",
    "    y_train_class.append(class_labels[train_label])\n",
    "    \n",
    "test_hosts = list()\n",
    "with open(TEST_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        test_hosts.append(l[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_w2v_100_path = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/graph_emb/test_emb.pickle'\n",
    "with open(serialized_w2v_100_path, 'rb') as file:\n",
    "        X_train_G = pickle.load(file)\n",
    "        \n",
    "serialized_w2v_100_path = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/graph_emb/train_emb.pickle'\n",
    "with open(serialized_w2v_100_path, 'rb') as file:\n",
    "        X_test_G = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_docs_train_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/train_all/'\n",
    "filenames = os.listdir(serialized_docs_train_dir)\n",
    "\n",
    "train_data = []\n",
    "y_train_tfidf = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_train_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        train_data.append(doc['x_list'])               \n",
    "        y_train_tfidf.append(doc['y'])\n",
    "        \n",
    "serialized_docs_test_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/test_all/'\n",
    "filenames = os.listdir(serialized_docs_test_dir)\n",
    "\n",
    "test_data = []\n",
    "for idx, file in enumerate(filenames):\n",
    "    with open(serialized_docs_test_dir + 'dict.pickle_' + str(idx), 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        test_data.append(doc['x_list'])\n",
    "        \n",
    "train_text = []\n",
    "y_train_doc = []\n",
    "\n",
    "train_text_idx = []\n",
    "train_no_text_idx = []\n",
    "train_text_index_map = {}\n",
    "counter = 0\n",
    "for idx, doc in enumerate(train_data):\n",
    "    if len(doc) != 0:\n",
    "        train_text.append(' '.join(doc))\n",
    "        y_train_doc.append(y_train_tfidf[idx])\n",
    "        train_text_idx.append(idx)    \n",
    "        train_text_index_map[idx] = counter\n",
    "        counter+=1\n",
    "    else:\n",
    "        train_no_text_idx.append(idx)\n",
    "        \n",
    "y_train_doc = np.array(y_train_doc)\n",
    "\n",
    "test_text = []\n",
    "test_text_idx = []\n",
    "test_no_text_idx = []\n",
    "test_text_index_map = {}\n",
    "counter = 0\n",
    "for idx, doc in enumerate(test_data):\n",
    "    if len(doc) != 0:          \n",
    "        test_text.append(' '.join(doc))\n",
    "        test_text_idx.append(idx)\n",
    "        test_text_index_map[idx] = counter\n",
    "        counter+=1\n",
    "    else:        \n",
    "        test_no_text_idx.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF for MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = np.concatenate((train_text, test_text))\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    decode_error='ignore',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 2),\n",
    "#     max_df=0.7,\n",
    "#     min_df=0.05,\n",
    "    lowercase=False,\n",
    "    max_features=5000)\n",
    "word_vectorizer.fit(all_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    decode_error='ignore',\n",
    "    analyzer='char',\n",
    "    max_df=0.5,\n",
    "    min_df=0.2,\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=3000,\n",
    "    lowercase=False)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSA on TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000, random_state=2)\n",
    "train_features_svd = svd.fit_transform(train_features)\n",
    "test_features_svd = svd.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Save pickles\n",
    "features_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/'\n",
    "filename = 'train_tf_idf'\n",
    "pickle_out = open(features_dir + filename,\"wb\")\n",
    "pickle.dump(train_features, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "filename = 'test_tf_idf'\n",
    "pickle_out = open(features_dir + filename,\"wb\")\n",
    "pickle.dump(test_features, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "filename = 'train_LSA'\n",
    "pickle_out = open(features_dir + filename,\"wb\")\n",
    "pickle.dump(train_features_svd, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "filename = 'test_LSA'\n",
    "pickle_out = open(features_dir + filename,\"wb\")\n",
    "pickle.dump(test_features_svd, pickle_out)\n",
    "pickle_out.close()\n",
    "'''\n",
    "\n",
    "#tfidf_8000_train\n",
    "path_train_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/train_tf_idf'\n",
    "with open(path_train_tfidf, 'rb') as file:\n",
    "        train_features = pickle.load(file)\n",
    "        \n",
    "#tfidf_8000_test    \n",
    "path_test_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/test_tf_idf'\n",
    "with open(path_test_tfidf, 'rb') as file:\n",
    "        test_features = pickle.load(file)\n",
    "        \n",
    "# #LSA_1000_train\n",
    "# path_train_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/train_LSA'\n",
    "# with open(path_train_tfidf, 'rb') as file:\n",
    "#         train_features_svd = pickle.load(file)\n",
    "        \n",
    "# #LSA_100_test    \n",
    "# path_test_tfidf = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/gen_features/test_LSA'\n",
    "# with open(path_test_tfidf, 'rb') as file:\n",
    "#         test_features_svd = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000, random_state=2)\n",
    "train_features_svd = svd.fit_transform(train_features)\n",
    "test_features_svd = svd.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676, 676)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_svd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP for LSA and tf-idf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(\n",
    "    input_size,\n",
    "    optimizer,    \n",
    "    classes=5,  \n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    hidden_layers=1,\n",
    "    units = 600,\n",
    "    dropout_rate = 0.5,\n",
    "    funnel = True,\n",
    "    hidden_activation='relu',\n",
    "    output_activation='softmax'\n",
    "):\n",
    "  \n",
    "    # Define the seed for numpy and Tensorflow to have reproducible experiments.\n",
    "    np.random.seed(23) \n",
    "    set_random_seed(452)\n",
    "       \n",
    "    input = Input(\n",
    "        shape=(input_size,),\n",
    "        name='Input'\n",
    "    )\n",
    "    x = input\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "    print(x.shape)\n",
    "    # Define the hidden layers.\n",
    "    for i in range(hidden_layers):\n",
    "        if funnel:\n",
    "            layer_units=units // (i+1)\n",
    "        else: \n",
    "            layer_units=units\n",
    "        x = Dense(\n",
    "           units=layer_units,\n",
    "           kernel_initializer='glorot_uniform',\n",
    "           activation=hidden_activation,\n",
    "           name='Hidden-{0:d}'.format(i + 1)\n",
    "        )(x)\n",
    "     #   x = BatchNormalization()(x)\n",
    "     #   x = BatchNormalization(x)\n",
    "        #Dropout\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "        \n",
    "    # Define the output layer.    \n",
    "    output = Dense(\n",
    "        units=classes,\n",
    "        kernel_initializer='uniform',\n",
    "        activation=output_activation,\n",
    "        name='Output'\n",
    "    )(x)\n",
    "    # Define the model and train it.\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "      \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_crossentropy', 'accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to ensmble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 5\n",
    "epochs = 200\n",
    "optimizer = Adam(lr=0.00023)\n",
    "\n",
    "# model_MLP = MLP_model(\n",
    "#     optimizer=optimizer,\n",
    "#     input_size=X_tra.shape[1],\n",
    "#     dropout_rate = 0.7,\n",
    "#     hidden_layers=3,\n",
    "#     units=1000,\n",
    "#     funnel=True\n",
    "# )\n",
    "\n",
    "# Keras Callbacks\n",
    "reducer_lr = ReduceLROnPlateau(factor = 0.00002, patience = 3, min_lr = 1e-6, verbose = -1)\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_categorical_crossentropy', mode='min', patience = 20) # Change 4 to 8 in the final run\n",
    "model_file_name = 'tfidf_mlp'\n",
    "check_pointer = keras.callbacks.ModelCheckpoint(model_file_name, monitor='val_categorical_crossentropy', mode='min', verbose = -1, save_best_only = True)  \n",
    "callbacks_list = [early_stopper, reducer_lr, check_pointer]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573 103\n",
      "(?, 8000)\n",
      "Train on 573 samples, validate on 103 samples\n",
      "Epoch 1/200\n",
      "573/573 [==============================] - ETA: 6s - loss: 1.6097 - categorical_crossentropy: 1.6097 - acc: 0.21 - ETA: 2s - loss: 1.6083 - categorical_crossentropy: 1.6083 - acc: 0.22 - ETA: 0s - loss: 1.6104 - categorical_crossentropy: 1.6104 - acc: 0.21 - ETA: 0s - loss: 1.6095 - categorical_crossentropy: 1.6095 - acc: 0.21 - 2s 4ms/step - loss: 1.6117 - categorical_crossentropy: 1.6117 - acc: 0.2094 - val_loss: 1.6068 - val_categorical_crossentropy: 1.6068 - val_acc: 0.2524\n",
      "Epoch 2/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.6083 - categorical_crossentropy: 1.6083 - acc: 0.20 - ETA: 0s - loss: 1.6135 - categorical_crossentropy: 1.6135 - acc: 0.19 - ETA: 0s - loss: 1.6132 - categorical_crossentropy: 1.6132 - acc: 0.17 - ETA: 0s - loss: 1.6123 - categorical_crossentropy: 1.6123 - acc: 0.17 - 0s 594us/step - loss: 1.6128 - categorical_crossentropy: 1.6128 - acc: 0.1745 - val_loss: 1.6056 - val_categorical_crossentropy: 1.6056 - val_acc: 0.2621\n",
      "Epoch 3/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.6203 - categorical_crossentropy: 1.6203 - acc: 0.20 - ETA: 0s - loss: 1.6175 - categorical_crossentropy: 1.6175 - acc: 0.18 - ETA: 0s - loss: 1.6143 - categorical_crossentropy: 1.6143 - acc: 0.19 - ETA: 0s - loss: 1.6117 - categorical_crossentropy: 1.6117 - acc: 0.19 - 0s 755us/step - loss: 1.6115 - categorical_crossentropy: 1.6115 - acc: 0.2007 - val_loss: 1.6037 - val_categorical_crossentropy: 1.6037 - val_acc: 0.2524\n",
      "Epoch 4/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5991 - categorical_crossentropy: 1.5991 - acc: 0.25 - ETA: 0s - loss: 1.6012 - categorical_crossentropy: 1.6012 - acc: 0.21 - ETA: 0s - loss: 1.5975 - categorical_crossentropy: 1.5975 - acc: 0.22 - ETA: 0s - loss: 1.6024 - categorical_crossentropy: 1.6024 - acc: 0.21 - 1s 879us/step - loss: 1.6010 - categorical_crossentropy: 1.6010 - acc: 0.2129 - val_loss: 1.6018 - val_categorical_crossentropy: 1.6018 - val_acc: 0.2330\n",
      "Epoch 5/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5963 - categorical_crossentropy: 1.5963 - acc: 0.21 - ETA: 0s - loss: 1.6059 - categorical_crossentropy: 1.6059 - acc: 0.19 - ETA: 0s - loss: 1.6054 - categorical_crossentropy: 1.6054 - acc: 0.19 - ETA: 0s - loss: 1.6036 - categorical_crossentropy: 1.6036 - acc: 0.19 - 0s 795us/step - loss: 1.6042 - categorical_crossentropy: 1.6042 - acc: 0.1972 - val_loss: 1.5994 - val_categorical_crossentropy: 1.5994 - val_acc: 0.2524\n",
      "Epoch 6/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.6131 - categorical_crossentropy: 1.6131 - acc: 0.27 - ETA: 0s - loss: 1.6038 - categorical_crossentropy: 1.6038 - acc: 0.26 - ETA: 0s - loss: 1.6023 - categorical_crossentropy: 1.6023 - acc: 0.26 - ETA: 0s - loss: 1.6031 - categorical_crossentropy: 1.6031 - acc: 0.24 - 0s 860us/step - loss: 1.6057 - categorical_crossentropy: 1.6057 - acc: 0.2391 - val_loss: 1.5967 - val_categorical_crossentropy: 1.5967 - val_acc: 0.2524\n",
      "Epoch 7/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5920 - categorical_crossentropy: 1.5920 - acc: 0.23 - ETA: 0s - loss: 1.6016 - categorical_crossentropy: 1.6016 - acc: 0.22 - ETA: 0s - loss: 1.6031 - categorical_crossentropy: 1.6031 - acc: 0.22 - ETA: 0s - loss: 1.6008 - categorical_crossentropy: 1.6008 - acc: 0.22 - 0s 863us/step - loss: 1.6023 - categorical_crossentropy: 1.6023 - acc: 0.2251 - val_loss: 1.5939 - val_categorical_crossentropy: 1.5939 - val_acc: 0.2621\n",
      "Epoch 8/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.6069 - categorical_crossentropy: 1.6069 - acc: 0.24 - ETA: 0s - loss: 1.6097 - categorical_crossentropy: 1.6097 - acc: 0.21 - ETA: 0s - loss: 1.6062 - categorical_crossentropy: 1.6062 - acc: 0.22 - ETA: 0s - loss: 1.6011 - categorical_crossentropy: 1.6011 - acc: 0.22 - 1s 875us/step - loss: 1.6013 - categorical_crossentropy: 1.6013 - acc: 0.2269 - val_loss: 1.5906 - val_categorical_crossentropy: 1.5906 - val_acc: 0.2621\n",
      "Epoch 9/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5751 - categorical_crossentropy: 1.5751 - acc: 0.28 - ETA: 0s - loss: 1.5874 - categorical_crossentropy: 1.5874 - acc: 0.26 - ETA: 0s - loss: 1.5927 - categorical_crossentropy: 1.5927 - acc: 0.23 - ETA: 0s - loss: 1.5944 - categorical_crossentropy: 1.5944 - acc: 0.23 - 0s 846us/step - loss: 1.5936 - categorical_crossentropy: 1.5936 - acc: 0.2426 - val_loss: 1.5876 - val_categorical_crossentropy: 1.5876 - val_acc: 0.2718\n",
      "Epoch 10/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.6026 - categorical_crossentropy: 1.6026 - acc: 0.18 - ETA: 0s - loss: 1.6013 - categorical_crossentropy: 1.6013 - acc: 0.20 - ETA: 0s - loss: 1.5909 - categorical_crossentropy: 1.5909 - acc: 0.21 - ETA: 0s - loss: 1.5948 - categorical_crossentropy: 1.5948 - acc: 0.19 - 0s 830us/step - loss: 1.5890 - categorical_crossentropy: 1.5890 - acc: 0.2164 - val_loss: 1.5841 - val_categorical_crossentropy: 1.5841 - val_acc: 0.2816\n",
      "Epoch 11/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5693 - categorical_crossentropy: 1.5693 - acc: 0.28 - ETA: 0s - loss: 1.5736 - categorical_crossentropy: 1.5736 - acc: 0.24 - ETA: 0s - loss: 1.5725 - categorical_crossentropy: 1.5725 - acc: 0.25 - ETA: 0s - loss: 1.5769 - categorical_crossentropy: 1.5769 - acc: 0.24 - 1s 894us/step - loss: 1.5783 - categorical_crossentropy: 1.5783 - acc: 0.2426 - val_loss: 1.5797 - val_categorical_crossentropy: 1.5797 - val_acc: 0.2913\n",
      "Epoch 12/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5560 - categorical_crossentropy: 1.5560 - acc: 0.26 - ETA: 0s - loss: 1.5772 - categorical_crossentropy: 1.5772 - acc: 0.25 - ETA: 0s - loss: 1.5772 - categorical_crossentropy: 1.5772 - acc: 0.25 - ETA: 0s - loss: 1.5729 - categorical_crossentropy: 1.5729 - acc: 0.25 - 0s 870us/step - loss: 1.5761 - categorical_crossentropy: 1.5761 - acc: 0.2583 - val_loss: 1.5750 - val_categorical_crossentropy: 1.5750 - val_acc: 0.2913\n",
      "Epoch 13/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5738 - categorical_crossentropy: 1.5738 - acc: 0.24 - ETA: 0s - loss: 1.5759 - categorical_crossentropy: 1.5759 - acc: 0.21 - ETA: 0s - loss: 1.5799 - categorical_crossentropy: 1.5799 - acc: 0.19 - ETA: 0s - loss: 1.5769 - categorical_crossentropy: 1.5769 - acc: 0.21 - 0s 855us/step - loss: 1.5759 - categorical_crossentropy: 1.5759 - acc: 0.2182 - val_loss: 1.5694 - val_categorical_crossentropy: 1.5694 - val_acc: 0.2718\n",
      "Epoch 14/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5496 - categorical_crossentropy: 1.5496 - acc: 0.26 - ETA: 0s - loss: 1.5648 - categorical_crossentropy: 1.5648 - acc: 0.24 - ETA: 0s - loss: 1.5577 - categorical_crossentropy: 1.5577 - acc: 0.23 - ETA: 0s - loss: 1.5654 - categorical_crossentropy: 1.5654 - acc: 0.23 - 0s 848us/step - loss: 1.5598 - categorical_crossentropy: 1.5598 - acc: 0.2356 - val_loss: 1.5632 - val_categorical_crossentropy: 1.5632 - val_acc: 0.2816\n",
      "Epoch 15/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5575 - categorical_crossentropy: 1.5575 - acc: 0.20 - ETA: 0s - loss: 1.5544 - categorical_crossentropy: 1.5544 - acc: 0.21 - ETA: 0s - loss: 1.5701 - categorical_crossentropy: 1.5701 - acc: 0.22 - ETA: 0s - loss: 1.5627 - categorical_crossentropy: 1.5627 - acc: 0.22 - 1s 895us/step - loss: 1.5581 - categorical_crossentropy: 1.5581 - acc: 0.2356 - val_loss: 1.5568 - val_categorical_crossentropy: 1.5568 - val_acc: 0.2718\n",
      "Epoch 16/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5378 - categorical_crossentropy: 1.5378 - acc: 0.28 - ETA: 0s - loss: 1.5423 - categorical_crossentropy: 1.5423 - acc: 0.24 - ETA: 0s - loss: 1.5257 - categorical_crossentropy: 1.5257 - acc: 0.26 - ETA: 0s - loss: 1.5402 - categorical_crossentropy: 1.5402 - acc: 0.25 - 1s 903us/step - loss: 1.5441 - categorical_crossentropy: 1.5441 - acc: 0.2443 - val_loss: 1.5504 - val_categorical_crossentropy: 1.5504 - val_acc: 0.2913\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573/573 [==============================] - ETA: 0s - loss: 1.5051 - categorical_crossentropy: 1.5051 - acc: 0.32 - ETA: 0s - loss: 1.5157 - categorical_crossentropy: 1.5157 - acc: 0.31 - ETA: 0s - loss: 1.5310 - categorical_crossentropy: 1.5310 - acc: 0.29 - ETA: 0s - loss: 1.5433 - categorical_crossentropy: 1.5433 - acc: 0.28 - 0s 857us/step - loss: 1.5513 - categorical_crossentropy: 1.5513 - acc: 0.2688 - val_loss: 1.5435 - val_categorical_crossentropy: 1.5435 - val_acc: 0.3204\n",
      "Epoch 18/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5318 - categorical_crossentropy: 1.5318 - acc: 0.23 - ETA: 0s - loss: 1.5182 - categorical_crossentropy: 1.5182 - acc: 0.25 - ETA: 0s - loss: 1.5290 - categorical_crossentropy: 1.5290 - acc: 0.25 - ETA: 0s - loss: 1.5440 - categorical_crossentropy: 1.5440 - acc: 0.24 - 1s 874us/step - loss: 1.5413 - categorical_crossentropy: 1.5413 - acc: 0.2531 - val_loss: 1.5363 - val_categorical_crossentropy: 1.5363 - val_acc: 0.3398\n",
      "Epoch 19/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5156 - categorical_crossentropy: 1.5156 - acc: 0.28 - ETA: 0s - loss: 1.5316 - categorical_crossentropy: 1.5316 - acc: 0.26 - ETA: 0s - loss: 1.5444 - categorical_crossentropy: 1.5444 - acc: 0.25 - ETA: 0s - loss: 1.5409 - categorical_crossentropy: 1.5409 - acc: 0.24 - 1s 888us/step - loss: 1.5364 - categorical_crossentropy: 1.5364 - acc: 0.2461 - val_loss: 1.5283 - val_categorical_crossentropy: 1.5283 - val_acc: 0.3883\n",
      "Epoch 20/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5603 - categorical_crossentropy: 1.5603 - acc: 0.21 - ETA: 0s - loss: 1.5516 - categorical_crossentropy: 1.5516 - acc: 0.24 - ETA: 0s - loss: 1.5548 - categorical_crossentropy: 1.5548 - acc: 0.25 - ETA: 0s - loss: 1.5503 - categorical_crossentropy: 1.5503 - acc: 0.25 - 0s 714us/step - loss: 1.5416 - categorical_crossentropy: 1.5416 - acc: 0.2723 - val_loss: 1.5195 - val_categorical_crossentropy: 1.5195 - val_acc: 0.3883\n",
      "Epoch 21/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5238 - categorical_crossentropy: 1.5238 - acc: 0.28 - ETA: 0s - loss: 1.5472 - categorical_crossentropy: 1.5472 - acc: 0.25 - ETA: 0s - loss: 1.5299 - categorical_crossentropy: 1.5299 - acc: 0.27 - ETA: 0s - loss: 1.5284 - categorical_crossentropy: 1.5284 - acc: 0.26 - 0s 834us/step - loss: 1.5256 - categorical_crossentropy: 1.5256 - acc: 0.2600 - val_loss: 1.5098 - val_categorical_crossentropy: 1.5098 - val_acc: 0.4175\n",
      "Epoch 22/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5159 - categorical_crossentropy: 1.5159 - acc: 0.28 - ETA: 0s - loss: 1.5106 - categorical_crossentropy: 1.5106 - acc: 0.29 - ETA: 0s - loss: 1.5104 - categorical_crossentropy: 1.5104 - acc: 0.29 - ETA: 0s - loss: 1.5033 - categorical_crossentropy: 1.5033 - acc: 0.30 - 0s 686us/step - loss: 1.5029 - categorical_crossentropy: 1.5029 - acc: 0.3124 - val_loss: 1.4996 - val_categorical_crossentropy: 1.4996 - val_acc: 0.4369\n",
      "Epoch 23/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5146 - categorical_crossentropy: 1.5146 - acc: 0.33 - ETA: 0s - loss: 1.5163 - categorical_crossentropy: 1.5163 - acc: 0.32 - ETA: 0s - loss: 1.5192 - categorical_crossentropy: 1.5192 - acc: 0.30 - ETA: 0s - loss: 1.5038 - categorical_crossentropy: 1.5038 - acc: 0.30 - 0s 813us/step - loss: 1.5093 - categorical_crossentropy: 1.5093 - acc: 0.2949 - val_loss: 1.4887 - val_categorical_crossentropy: 1.4887 - val_acc: 0.4466\n",
      "Epoch 24/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5235 - categorical_crossentropy: 1.5235 - acc: 0.28 - ETA: 0s - loss: 1.4989 - categorical_crossentropy: 1.4989 - acc: 0.31 - ETA: 0s - loss: 1.4905 - categorical_crossentropy: 1.4905 - acc: 0.32 - ETA: 0s - loss: 1.4926 - categorical_crossentropy: 1.4926 - acc: 0.32 - 0s 682us/step - loss: 1.4972 - categorical_crossentropy: 1.4972 - acc: 0.3072 - val_loss: 1.4770 - val_categorical_crossentropy: 1.4770 - val_acc: 0.4854\n",
      "Epoch 25/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.4612 - categorical_crossentropy: 1.4612 - acc: 0.32 - ETA: 0s - loss: 1.4523 - categorical_crossentropy: 1.4523 - acc: 0.35 - ETA: 0s - loss: 1.4638 - categorical_crossentropy: 1.4638 - acc: 0.34 - ETA: 0s - loss: 1.4787 - categorical_crossentropy: 1.4787 - acc: 0.33 - 0s 717us/step - loss: 1.4777 - categorical_crossentropy: 1.4777 - acc: 0.3386 - val_loss: 1.4644 - val_categorical_crossentropy: 1.4644 - val_acc: 0.4854\n",
      "Epoch 26/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.5029 - categorical_crossentropy: 1.5029 - acc: 0.31 - ETA: 0s - loss: 1.4902 - categorical_crossentropy: 1.4902 - acc: 0.35 - ETA: 0s - loss: 1.4811 - categorical_crossentropy: 1.4811 - acc: 0.34 - ETA: 0s - loss: 1.4812 - categorical_crossentropy: 1.4812 - acc: 0.33 - 0s 862us/step - loss: 1.4797 - categorical_crossentropy: 1.4797 - acc: 0.3316 - val_loss: 1.4499 - val_categorical_crossentropy: 1.4499 - val_acc: 0.4951\n",
      "Epoch 27/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.4475 - categorical_crossentropy: 1.4475 - acc: 0.34 - ETA: 0s - loss: 1.4732 - categorical_crossentropy: 1.4732 - acc: 0.37 - ETA: 0s - loss: 1.4767 - categorical_crossentropy: 1.4767 - acc: 0.34 - ETA: 0s - loss: 1.4786 - categorical_crossentropy: 1.4786 - acc: 0.34 - 0s 837us/step - loss: 1.4734 - categorical_crossentropy: 1.4734 - acc: 0.3543 - val_loss: 1.4331 - val_categorical_crossentropy: 1.4331 - val_acc: 0.4951\n",
      "Epoch 28/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.4779 - categorical_crossentropy: 1.4779 - acc: 0.30 - ETA: 0s - loss: 1.4723 - categorical_crossentropy: 1.4723 - acc: 0.31 - ETA: 0s - loss: 1.4598 - categorical_crossentropy: 1.4598 - acc: 0.34 - ETA: 0s - loss: 1.4525 - categorical_crossentropy: 1.4525 - acc: 0.36 - 1s 910us/step - loss: 1.4569 - categorical_crossentropy: 1.4569 - acc: 0.3647 - val_loss: 1.4151 - val_categorical_crossentropy: 1.4151 - val_acc: 0.5243\n",
      "Epoch 29/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.4301 - categorical_crossentropy: 1.4301 - acc: 0.35 - ETA: 0s - loss: 1.4152 - categorical_crossentropy: 1.4152 - acc: 0.38 - ETA: 0s - loss: 1.4194 - categorical_crossentropy: 1.4194 - acc: 0.39 - ETA: 0s - loss: 1.4230 - categorical_crossentropy: 1.4230 - acc: 0.38 - 0s 769us/step - loss: 1.4212 - categorical_crossentropy: 1.4212 - acc: 0.3892 - val_loss: 1.3939 - val_categorical_crossentropy: 1.3939 - val_acc: 0.5534\n",
      "Epoch 30/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.4031 - categorical_crossentropy: 1.4031 - acc: 0.45 - ETA: 0s - loss: 1.4438 - categorical_crossentropy: 1.4438 - acc: 0.41 - ETA: 0s - loss: 1.4185 - categorical_crossentropy: 1.4185 - acc: 0.44 - ETA: 0s - loss: 1.4272 - categorical_crossentropy: 1.4272 - acc: 0.43 - 1s 907us/step - loss: 1.4257 - categorical_crossentropy: 1.4257 - acc: 0.4346 - val_loss: 1.3689 - val_categorical_crossentropy: 1.3689 - val_acc: 0.5728\n",
      "Epoch 31/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.3676 - categorical_crossentropy: 1.3676 - acc: 0.47 - ETA: 0s - loss: 1.4040 - categorical_crossentropy: 1.4040 - acc: 0.46 - ETA: 0s - loss: 1.3978 - categorical_crossentropy: 1.3978 - acc: 0.46 - ETA: 0s - loss: 1.4028 - categorical_crossentropy: 1.4028 - acc: 0.42 - 0s 851us/step - loss: 1.4048 - categorical_crossentropy: 1.4048 - acc: 0.4328 - val_loss: 1.3416 - val_categorical_crossentropy: 1.3416 - val_acc: 0.6019\n",
      "Epoch 32/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.3796 - categorical_crossentropy: 1.3796 - acc: 0.53 - ETA: 0s - loss: 1.3852 - categorical_crossentropy: 1.3852 - acc: 0.48 - ETA: 0s - loss: 1.3913 - categorical_crossentropy: 1.3913 - acc: 0.46 - ETA: 0s - loss: 1.3900 - categorical_crossentropy: 1.3900 - acc: 0.47 - 0s 820us/step - loss: 1.3834 - categorical_crossentropy: 1.3834 - acc: 0.4729 - val_loss: 1.3124 - val_categorical_crossentropy: 1.3124 - val_acc: 0.6117\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573/573 [==============================] - ETA: 0s - loss: 1.4230 - categorical_crossentropy: 1.4230 - acc: 0.41 - ETA: 0s - loss: 1.3762 - categorical_crossentropy: 1.3762 - acc: 0.46 - ETA: 0s - loss: 1.3829 - categorical_crossentropy: 1.3829 - acc: 0.45 - ETA: 0s - loss: 1.3802 - categorical_crossentropy: 1.3802 - acc: 0.44 - 0s 863us/step - loss: 1.3785 - categorical_crossentropy: 1.3785 - acc: 0.4433 - val_loss: 1.2810 - val_categorical_crossentropy: 1.2810 - val_acc: 0.6117\n",
      "Epoch 34/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.3051 - categorical_crossentropy: 1.3051 - acc: 0.52 - ETA: 0s - loss: 1.3056 - categorical_crossentropy: 1.3056 - acc: 0.52 - ETA: 0s - loss: 1.3351 - categorical_crossentropy: 1.3351 - acc: 0.47 - ETA: 0s - loss: 1.3457 - categorical_crossentropy: 1.3457 - acc: 0.45 - 1s 895us/step - loss: 1.3478 - categorical_crossentropy: 1.3478 - acc: 0.4433 - val_loss: 1.2494 - val_categorical_crossentropy: 1.2494 - val_acc: 0.6117\n",
      "Epoch 35/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.2777 - categorical_crossentropy: 1.2777 - acc: 0.50 - ETA: 0s - loss: 1.2943 - categorical_crossentropy: 1.2943 - acc: 0.50 - ETA: 0s - loss: 1.2809 - categorical_crossentropy: 1.2809 - acc: 0.51 - ETA: 0s - loss: 1.2897 - categorical_crossentropy: 1.2897 - acc: 0.51 - 1s 898us/step - loss: 1.2941 - categorical_crossentropy: 1.2941 - acc: 0.5061 - val_loss: 1.2144 - val_categorical_crossentropy: 1.2144 - val_acc: 0.6019\n",
      "Epoch 36/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.3301 - categorical_crossentropy: 1.3301 - acc: 0.43 - ETA: 0s - loss: 1.3196 - categorical_crossentropy: 1.3196 - acc: 0.46 - ETA: 0s - loss: 1.2878 - categorical_crossentropy: 1.2878 - acc: 0.47 - ETA: 0s - loss: 1.2850 - categorical_crossentropy: 1.2850 - acc: 0.48 - 0s 868us/step - loss: 1.2719 - categorical_crossentropy: 1.2719 - acc: 0.4974 - val_loss: 1.1782 - val_categorical_crossentropy: 1.1782 - val_acc: 0.6019\n",
      "Epoch 37/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.2834 - categorical_crossentropy: 1.2834 - acc: 0.49 - ETA: 0s - loss: 1.2352 - categorical_crossentropy: 1.2352 - acc: 0.54 - ETA: 0s - loss: 1.2539 - categorical_crossentropy: 1.2539 - acc: 0.51 - ETA: 0s - loss: 1.2553 - categorical_crossentropy: 1.2553 - acc: 0.51 - 1s 931us/step - loss: 1.2449 - categorical_crossentropy: 1.2449 - acc: 0.5218 - val_loss: 1.1401 - val_categorical_crossentropy: 1.1401 - val_acc: 0.6019\n",
      "Epoch 38/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.2625 - categorical_crossentropy: 1.2625 - acc: 0.48 - ETA: 0s - loss: 1.2248 - categorical_crossentropy: 1.2248 - acc: 0.54 - ETA: 0s - loss: 1.2014 - categorical_crossentropy: 1.2014 - acc: 0.56 - ETA: 0s - loss: 1.2140 - categorical_crossentropy: 1.2140 - acc: 0.54 - 0s 864us/step - loss: 1.2044 - categorical_crossentropy: 1.2044 - acc: 0.5515 - val_loss: 1.1030 - val_categorical_crossentropy: 1.1030 - val_acc: 0.6117\n",
      "Epoch 39/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.1666 - categorical_crossentropy: 1.1666 - acc: 0.50 - ETA: 0s - loss: 1.1801 - categorical_crossentropy: 1.1801 - acc: 0.54 - ETA: 0s - loss: 1.1954 - categorical_crossentropy: 1.1954 - acc: 0.52 - ETA: 0s - loss: 1.1882 - categorical_crossentropy: 1.1882 - acc: 0.52 - 0s 860us/step - loss: 1.1894 - categorical_crossentropy: 1.1894 - acc: 0.5236 - val_loss: 1.0735 - val_categorical_crossentropy: 1.0735 - val_acc: 0.6019\n",
      "Epoch 40/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.1478 - categorical_crossentropy: 1.1478 - acc: 0.56 - ETA: 0s - loss: 1.1158 - categorical_crossentropy: 1.1158 - acc: 0.55 - ETA: 0s - loss: 1.1841 - categorical_crossentropy: 1.1841 - acc: 0.52 - ETA: 0s - loss: 1.1672 - categorical_crossentropy: 1.1672 - acc: 0.53 - 1s 916us/step - loss: 1.1683 - categorical_crossentropy: 1.1683 - acc: 0.5323 - val_loss: 1.0486 - val_categorical_crossentropy: 1.0486 - val_acc: 0.6019\n",
      "Epoch 41/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.1772 - categorical_crossentropy: 1.1772 - acc: 0.57 - ETA: 0s - loss: 1.1882 - categorical_crossentropy: 1.1882 - acc: 0.57 - ETA: 0s - loss: 1.1834 - categorical_crossentropy: 1.1834 - acc: 0.55 - ETA: 0s - loss: 1.1696 - categorical_crossentropy: 1.1696 - acc: 0.55 - 0s 860us/step - loss: 1.1611 - categorical_crossentropy: 1.1611 - acc: 0.5567 - val_loss: 1.0250 - val_categorical_crossentropy: 1.0250 - val_acc: 0.6117\n",
      "Epoch 42/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.1683 - categorical_crossentropy: 1.1683 - acc: 0.57 - ETA: 0s - loss: 1.1243 - categorical_crossentropy: 1.1243 - acc: 0.58 - ETA: 0s - loss: 1.1132 - categorical_crossentropy: 1.1132 - acc: 0.58 - ETA: 0s - loss: 1.1039 - categorical_crossentropy: 1.1039 - acc: 0.57 - 1s 907us/step - loss: 1.1108 - categorical_crossentropy: 1.1108 - acc: 0.5689 - val_loss: 1.0106 - val_categorical_crossentropy: 1.0106 - val_acc: 0.6311\n",
      "Epoch 43/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.0675 - categorical_crossentropy: 1.0675 - acc: 0.59 - ETA: 0s - loss: 1.0376 - categorical_crossentropy: 1.0376 - acc: 0.61 - ETA: 0s - loss: 1.0768 - categorical_crossentropy: 1.0768 - acc: 0.58 - ETA: 0s - loss: 1.0748 - categorical_crossentropy: 1.0748 - acc: 0.58 - 0s 851us/step - loss: 1.0720 - categorical_crossentropy: 1.0720 - acc: 0.5864 - val_loss: 0.9960 - val_categorical_crossentropy: 0.9960 - val_acc: 0.6408\n",
      "Epoch 44/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.9901 - categorical_crossentropy: 0.9901 - acc: 0.56 - ETA: 0s - loss: 1.0228 - categorical_crossentropy: 1.0228 - acc: 0.58 - ETA: 0s - loss: 1.0614 - categorical_crossentropy: 1.0614 - acc: 0.55 - ETA: 0s - loss: 1.0734 - categorical_crossentropy: 1.0734 - acc: 0.55 - 1s 912us/step - loss: 1.0621 - categorical_crossentropy: 1.0621 - acc: 0.5602 - val_loss: 0.9776 - val_categorical_crossentropy: 0.9776 - val_acc: 0.6311\n",
      "Epoch 45/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.0702 - categorical_crossentropy: 1.0702 - acc: 0.56 - ETA: 0s - loss: 1.0655 - categorical_crossentropy: 1.0655 - acc: 0.57 - ETA: 0s - loss: 1.0821 - categorical_crossentropy: 1.0821 - acc: 0.58 - ETA: 0s - loss: 1.0483 - categorical_crossentropy: 1.0483 - acc: 0.60 - 0s 853us/step - loss: 1.0540 - categorical_crossentropy: 1.0540 - acc: 0.5986 - val_loss: 0.9616 - val_categorical_crossentropy: 0.9616 - val_acc: 0.6505\n",
      "Epoch 46/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.0974 - categorical_crossentropy: 1.0974 - acc: 0.57 - ETA: 0s - loss: 1.0495 - categorical_crossentropy: 1.0495 - acc: 0.55 - ETA: 0s - loss: 1.0684 - categorical_crossentropy: 1.0684 - acc: 0.53 - ETA: 0s - loss: 1.0544 - categorical_crossentropy: 1.0544 - acc: 0.56 - 1s 901us/step - loss: 1.0334 - categorical_crossentropy: 1.0334 - acc: 0.5759 - val_loss: 0.9500 - val_categorical_crossentropy: 0.9500 - val_acc: 0.6602\n",
      "Epoch 47/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 1.1091 - categorical_crossentropy: 1.1091 - acc: 0.54 - ETA: 0s - loss: 1.0162 - categorical_crossentropy: 1.0162 - acc: 0.60 - ETA: 0s - loss: 1.0121 - categorical_crossentropy: 1.0121 - acc: 0.60 - ETA: 0s - loss: 1.0069 - categorical_crossentropy: 1.0069 - acc: 0.60 - 1s 879us/step - loss: 1.0262 - categorical_crossentropy: 1.0262 - acc: 0.5899 - val_loss: 0.9452 - val_categorical_crossentropy: 0.9452 - val_acc: 0.6505\n",
      "Epoch 48/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.9529 - categorical_crossentropy: 0.9529 - acc: 0.59 - ETA: 0s - loss: 0.9848 - categorical_crossentropy: 0.9848 - acc: 0.57 - ETA: 0s - loss: 1.0018 - categorical_crossentropy: 1.0018 - acc: 0.57 - ETA: 0s - loss: 0.9916 - categorical_crossentropy: 0.9916 - acc: 0.58 - 1s 903us/step - loss: 0.9801 - categorical_crossentropy: 0.9801 - acc: 0.6003 - val_loss: 0.9379 - val_categorical_crossentropy: 0.9379 - val_acc: 0.6602\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573/573 [==============================] - ETA: 0s - loss: 0.9297 - categorical_crossentropy: 0.9297 - acc: 0.61 - ETA: 0s - loss: 0.9049 - categorical_crossentropy: 0.9049 - acc: 0.66 - ETA: 0s - loss: 0.9468 - categorical_crossentropy: 0.9468 - acc: 0.61 - ETA: 0s - loss: 0.9455 - categorical_crossentropy: 0.9455 - acc: 0.63 - 1s 877us/step - loss: 0.9466 - categorical_crossentropy: 0.9466 - acc: 0.6300 - val_loss: 0.9272 - val_categorical_crossentropy: 0.9272 - val_acc: 0.6505\n",
      "Epoch 50/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.8585 - categorical_crossentropy: 0.8585 - acc: 0.70 - ETA: 0s - loss: 0.8818 - categorical_crossentropy: 0.8818 - acc: 0.65 - ETA: 0s - loss: 0.9089 - categorical_crossentropy: 0.9089 - acc: 0.61 - ETA: 0s - loss: 0.9087 - categorical_crossentropy: 0.9087 - acc: 0.62 - 1s 889us/step - loss: 0.9080 - categorical_crossentropy: 0.9080 - acc: 0.6283 - val_loss: 0.9140 - val_categorical_crossentropy: 0.9140 - val_acc: 0.6602\n",
      "Epoch 51/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.8512 - categorical_crossentropy: 0.8512 - acc: 0.67 - ETA: 0s - loss: 0.9061 - categorical_crossentropy: 0.9061 - acc: 0.65 - ETA: 0s - loss: 0.8954 - categorical_crossentropy: 0.8954 - acc: 0.66 - ETA: 0s - loss: 0.8943 - categorical_crossentropy: 0.8943 - acc: 0.66 - 1s 933us/step - loss: 0.8905 - categorical_crossentropy: 0.8905 - acc: 0.6719 - val_loss: 0.9073 - val_categorical_crossentropy: 0.9073 - val_acc: 0.6408\n",
      "Epoch 52/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7129 - categorical_crossentropy: 0.7129 - acc: 0.76 - ETA: 0s - loss: 0.8423 - categorical_crossentropy: 0.8423 - acc: 0.69 - ETA: 0s - loss: 0.8519 - categorical_crossentropy: 0.8519 - acc: 0.66 - ETA: 0s - loss: 0.9174 - categorical_crossentropy: 0.9174 - acc: 0.64 - 1s 879us/step - loss: 0.9120 - categorical_crossentropy: 0.9120 - acc: 0.6475 - val_loss: 0.8950 - val_categorical_crossentropy: 0.8950 - val_acc: 0.6505\n",
      "Epoch 53/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.9663 - categorical_crossentropy: 0.9663 - acc: 0.60 - ETA: 0s - loss: 0.9042 - categorical_crossentropy: 0.9042 - acc: 0.61 - ETA: 0s - loss: 0.8646 - categorical_crossentropy: 0.8646 - acc: 0.64 - ETA: 0s - loss: 0.8863 - categorical_crossentropy: 0.8863 - acc: 0.63 - 0s 872us/step - loss: 0.8918 - categorical_crossentropy: 0.8918 - acc: 0.6283 - val_loss: 0.8842 - val_categorical_crossentropy: 0.8842 - val_acc: 0.6602\n",
      "Epoch 54/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7791 - categorical_crossentropy: 0.7791 - acc: 0.73 - ETA: 0s - loss: 0.8699 - categorical_crossentropy: 0.8699 - acc: 0.65 - ETA: 0s - loss: 0.9194 - categorical_crossentropy: 0.9194 - acc: 0.64 - ETA: 0s - loss: 0.9004 - categorical_crossentropy: 0.9004 - acc: 0.64 - 1s 891us/step - loss: 0.8924 - categorical_crossentropy: 0.8924 - acc: 0.6440 - val_loss: 0.8735 - val_categorical_crossentropy: 0.8735 - val_acc: 0.6699\n",
      "Epoch 55/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7741 - categorical_crossentropy: 0.7741 - acc: 0.67 - ETA: 0s - loss: 0.8614 - categorical_crossentropy: 0.8614 - acc: 0.67 - ETA: 0s - loss: 0.8979 - categorical_crossentropy: 0.8979 - acc: 0.65 - ETA: 0s - loss: 0.8894 - categorical_crossentropy: 0.8894 - acc: 0.63 - 1s 883us/step - loss: 0.8800 - categorical_crossentropy: 0.8800 - acc: 0.6370 - val_loss: 0.8722 - val_categorical_crossentropy: 0.8722 - val_acc: 0.6602\n",
      "Epoch 56/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.8484 - categorical_crossentropy: 0.8484 - acc: 0.61 - ETA: 0s - loss: 0.8380 - categorical_crossentropy: 0.8380 - acc: 0.62 - ETA: 0s - loss: 0.8222 - categorical_crossentropy: 0.8222 - acc: 0.64 - ETA: 0s - loss: 0.8320 - categorical_crossentropy: 0.8320 - acc: 0.65 - 1s 891us/step - loss: 0.8274 - categorical_crossentropy: 0.8274 - acc: 0.6649 - val_loss: 0.8721 - val_categorical_crossentropy: 0.8721 - val_acc: 0.6602\n",
      "Epoch 57/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7964 - categorical_crossentropy: 0.7964 - acc: 0.70 - ETA: 0s - loss: 0.8272 - categorical_crossentropy: 0.8272 - acc: 0.66 - ETA: 0s - loss: 0.7926 - categorical_crossentropy: 0.7926 - acc: 0.69 - ETA: 0s - loss: 0.8060 - categorical_crossentropy: 0.8060 - acc: 0.68 - 1s 917us/step - loss: 0.8084 - categorical_crossentropy: 0.8084 - acc: 0.6859 - val_loss: 0.8664 - val_categorical_crossentropy: 0.8664 - val_acc: 0.6796\n",
      "Epoch 58/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.8169 - categorical_crossentropy: 0.8169 - acc: 0.64 - ETA: 0s - loss: 0.8273 - categorical_crossentropy: 0.8273 - acc: 0.65 - ETA: 0s - loss: 0.7911 - categorical_crossentropy: 0.7911 - acc: 0.68 - ETA: 0s - loss: 0.7987 - categorical_crossentropy: 0.7987 - acc: 0.69 - 1s 902us/step - loss: 0.7961 - categorical_crossentropy: 0.7961 - acc: 0.6894 - val_loss: 0.8525 - val_categorical_crossentropy: 0.8525 - val_acc: 0.6699\n",
      "Epoch 59/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7544 - categorical_crossentropy: 0.7544 - acc: 0.65 - ETA: 0s - loss: 0.7218 - categorical_crossentropy: 0.7218 - acc: 0.69 - ETA: 0s - loss: 0.7620 - categorical_crossentropy: 0.7620 - acc: 0.69 - ETA: 0s - loss: 0.7576 - categorical_crossentropy: 0.7576 - acc: 0.70 - 1s 875us/step - loss: 0.7703 - categorical_crossentropy: 0.7703 - acc: 0.6981 - val_loss: 0.8453 - val_categorical_crossentropy: 0.8453 - val_acc: 0.6796\n",
      "Epoch 60/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.6989 - categorical_crossentropy: 0.6989 - acc: 0.71 - ETA: 0s - loss: 0.7227 - categorical_crossentropy: 0.7227 - acc: 0.74 - ETA: 0s - loss: 0.7535 - categorical_crossentropy: 0.7535 - acc: 0.71 - ETA: 0s - loss: 0.7509 - categorical_crossentropy: 0.7509 - acc: 0.71 - 0s 851us/step - loss: 0.7468 - categorical_crossentropy: 0.7468 - acc: 0.7086 - val_loss: 0.8396 - val_categorical_crossentropy: 0.8396 - val_acc: 0.6796\n",
      "Epoch 61/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.8156 - categorical_crossentropy: 0.8156 - acc: 0.71 - ETA: 0s - loss: 0.7945 - categorical_crossentropy: 0.7945 - acc: 0.68 - ETA: 0s - loss: 0.7776 - categorical_crossentropy: 0.7776 - acc: 0.69 - ETA: 0s - loss: 0.7545 - categorical_crossentropy: 0.7545 - acc: 0.69 - 0s 761us/step - loss: 0.7498 - categorical_crossentropy: 0.7498 - acc: 0.6946 - val_loss: 0.8419 - val_categorical_crossentropy: 0.8419 - val_acc: 0.6796\n",
      "Epoch 62/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7770 - categorical_crossentropy: 0.7770 - acc: 0.71 - ETA: 0s - loss: 0.7915 - categorical_crossentropy: 0.7915 - acc: 0.68 - ETA: 0s - loss: 0.7709 - categorical_crossentropy: 0.7709 - acc: 0.70 - ETA: 0s - loss: 0.7499 - categorical_crossentropy: 0.7499 - acc: 0.70 - 1s 943us/step - loss: 0.7541 - categorical_crossentropy: 0.7541 - acc: 0.7068 - val_loss: 0.8364 - val_categorical_crossentropy: 0.8364 - val_acc: 0.6796\n",
      "Epoch 63/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7257 - categorical_crossentropy: 0.7257 - acc: 0.77 - ETA: 0s - loss: 0.7241 - categorical_crossentropy: 0.7241 - acc: 0.73 - ETA: 0s - loss: 0.7026 - categorical_crossentropy: 0.7026 - acc: 0.73 - ETA: 0s - loss: 0.7300 - categorical_crossentropy: 0.7300 - acc: 0.72 - 0s 862us/step - loss: 0.7240 - categorical_crossentropy: 0.7240 - acc: 0.7277 - val_loss: 0.8305 - val_categorical_crossentropy: 0.8305 - val_acc: 0.7087\n",
      "Epoch 64/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7099 - categorical_crossentropy: 0.7099 - acc: 0.75 - ETA: 0s - loss: 0.6398 - categorical_crossentropy: 0.6398 - acc: 0.77 - ETA: 0s - loss: 0.6398 - categorical_crossentropy: 0.6398 - acc: 0.77 - ETA: 0s - loss: 0.6822 - categorical_crossentropy: 0.6822 - acc: 0.74 - 1s 919us/step - loss: 0.6980 - categorical_crossentropy: 0.6980 - acc: 0.7347 - val_loss: 0.8301 - val_categorical_crossentropy: 0.8301 - val_acc: 0.6990\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573/573 [==============================] - ETA: 0s - loss: 0.6582 - categorical_crossentropy: 0.6582 - acc: 0.72 - ETA: 0s - loss: 0.6743 - categorical_crossentropy: 0.6743 - acc: 0.71 - ETA: 0s - loss: 0.6932 - categorical_crossentropy: 0.6932 - acc: 0.70 - ETA: 0s - loss: 0.6865 - categorical_crossentropy: 0.6865 - acc: 0.70 - 0s 853us/step - loss: 0.6839 - categorical_crossentropy: 0.6839 - acc: 0.7155 - val_loss: 0.8430 - val_categorical_crossentropy: 0.8430 - val_acc: 0.6699\n",
      "Epoch 66/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.7450 - categorical_crossentropy: 0.7450 - acc: 0.71 - ETA: 0s - loss: 0.7618 - categorical_crossentropy: 0.7618 - acc: 0.71 - ETA: 0s - loss: 0.7011 - categorical_crossentropy: 0.7011 - acc: 0.73 - ETA: 0s - loss: 0.6907 - categorical_crossentropy: 0.6907 - acc: 0.74 - 0s 840us/step - loss: 0.6837 - categorical_crossentropy: 0.6837 - acc: 0.7417 - val_loss: 0.8342 - val_categorical_crossentropy: 0.8342 - val_acc: 0.6796\n",
      "Epoch 67/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.6573 - categorical_crossentropy: 0.6573 - acc: 0.74 - ETA: 0s - loss: 0.6145 - categorical_crossentropy: 0.6145 - acc: 0.75 - ETA: 0s - loss: 0.6380 - categorical_crossentropy: 0.6380 - acc: 0.74 - ETA: 0s - loss: 0.6617 - categorical_crossentropy: 0.6617 - acc: 0.73 - 0s 848us/step - loss: 0.6601 - categorical_crossentropy: 0.6601 - acc: 0.7382 - val_loss: 0.8238 - val_categorical_crossentropy: 0.8238 - val_acc: 0.6893\n",
      "Epoch 68/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.6685 - categorical_crossentropy: 0.6685 - acc: 0.70 - ETA: 0s - loss: 0.6227 - categorical_crossentropy: 0.6227 - acc: 0.76 - ETA: 0s - loss: 0.6221 - categorical_crossentropy: 0.6221 - acc: 0.77 - ETA: 0s - loss: 0.6499 - categorical_crossentropy: 0.6499 - acc: 0.75 - 1s 895us/step - loss: 0.6529 - categorical_crossentropy: 0.6529 - acc: 0.7539 - val_loss: 0.8212 - val_categorical_crossentropy: 0.8212 - val_acc: 0.6893\n",
      "Epoch 69/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.6719 - categorical_crossentropy: 0.6719 - acc: 0.68 - ETA: 0s - loss: 0.6431 - categorical_crossentropy: 0.6431 - acc: 0.72 - ETA: 0s - loss: 0.6464 - categorical_crossentropy: 0.6464 - acc: 0.73 - ETA: 0s - loss: 0.6358 - categorical_crossentropy: 0.6358 - acc: 0.74 - 1s 894us/step - loss: 0.6394 - categorical_crossentropy: 0.6394 - acc: 0.7400 - val_loss: 0.8240 - val_categorical_crossentropy: 0.8240 - val_acc: 0.6796\n",
      "Epoch 70/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.6060 - categorical_crossentropy: 0.6060 - acc: 0.82 - ETA: 0s - loss: 0.5860 - categorical_crossentropy: 0.5860 - acc: 0.80 - ETA: 0s - loss: 0.5980 - categorical_crossentropy: 0.5980 - acc: 0.78 - ETA: 0s - loss: 0.6020 - categorical_crossentropy: 0.6020 - acc: 0.77 - 0s 824us/step - loss: 0.6077 - categorical_crossentropy: 0.6077 - acc: 0.7749 - val_loss: 0.8214 - val_categorical_crossentropy: 0.8214 - val_acc: 0.6990\n",
      "Epoch 71/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.4999 - categorical_crossentropy: 0.4999 - acc: 0.85 - ETA: 0s - loss: 0.5483 - categorical_crossentropy: 0.5483 - acc: 0.80 - ETA: 0s - loss: 0.5639 - categorical_crossentropy: 0.5639 - acc: 0.78 - ETA: 0s - loss: 0.6138 - categorical_crossentropy: 0.6138 - acc: 0.77 - 0s 822us/step - loss: 0.6242 - categorical_crossentropy: 0.6242 - acc: 0.7801 - val_loss: 0.8220 - val_categorical_crossentropy: 0.8220 - val_acc: 0.6796\n",
      "Epoch 72/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.5800 - categorical_crossentropy: 0.5800 - acc: 0.75 - ETA: 0s - loss: 0.6137 - categorical_crossentropy: 0.6137 - acc: 0.73 - ETA: 0s - loss: 0.6032 - categorical_crossentropy: 0.6032 - acc: 0.75 - ETA: 0s - loss: 0.5973 - categorical_crossentropy: 0.5973 - acc: 0.75 - 1s 921us/step - loss: 0.6100 - categorical_crossentropy: 0.6100 - acc: 0.7522 - val_loss: 0.8220 - val_categorical_crossentropy: 0.8220 - val_acc: 0.6796\n",
      "Epoch 73/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.6616 - categorical_crossentropy: 0.6616 - acc: 0.75 - ETA: 0s - loss: 0.6453 - categorical_crossentropy: 0.6453 - acc: 0.75 - ETA: 0s - loss: 0.6791 - categorical_crossentropy: 0.6791 - acc: 0.73 - ETA: 0s - loss: 0.6094 - categorical_crossentropy: 0.6094 - acc: 0.77 - 1s 885us/step - loss: 0.6038 - categorical_crossentropy: 0.6038 - acc: 0.7766 - val_loss: 0.8220 - val_categorical_crossentropy: 0.8220 - val_acc: 0.6796\n",
      "Epoch 74/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.4809 - categorical_crossentropy: 0.4809 - acc: 0.79 - ETA: 0s - loss: 0.5880 - categorical_crossentropy: 0.5880 - acc: 0.75 - ETA: 0s - loss: 0.5801 - categorical_crossentropy: 0.5801 - acc: 0.77 - ETA: 0s - loss: 0.5908 - categorical_crossentropy: 0.5908 - acc: 0.76 - 0s 829us/step - loss: 0.5876 - categorical_crossentropy: 0.5876 - acc: 0.7609 - val_loss: 0.8220 - val_categorical_crossentropy: 0.8220 - val_acc: 0.6796\n",
      "Epoch 75/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.5763 - categorical_crossentropy: 0.5763 - acc: 0.78 - ETA: 0s - loss: 0.6108 - categorical_crossentropy: 0.6108 - acc: 0.78 - ETA: 0s - loss: 0.5564 - categorical_crossentropy: 0.5564 - acc: 0.81 - ETA: 0s - loss: 0.5672 - categorical_crossentropy: 0.5672 - acc: 0.80 - 0s 865us/step - loss: 0.5735 - categorical_crossentropy: 0.5735 - acc: 0.7888 - val_loss: 0.8221 - val_categorical_crossentropy: 0.8221 - val_acc: 0.6796\n",
      "Epoch 76/200\n",
      "573/573 [==============================] - ETA: 0s - loss: 0.5771 - categorical_crossentropy: 0.5771 - acc: 0.78 - ETA: 0s - loss: 0.6686 - categorical_crossentropy: 0.6686 - acc: 0.73 - ETA: 0s - loss: 0.6432 - categorical_crossentropy: 0.6432 - acc: 0.74 - ETA: 0s - loss: 0.6384 - categorical_crossentropy: 0.6384 - acc: 0.74 - 0s 855us/step - loss: 0.6292 - categorical_crossentropy: 0.6292 - acc: 0.7504 - val_loss: 0.8221 - val_categorical_crossentropy: 0.8221 - val_acc: 0.6796\n",
      "Epoch 77/200\n",
      "128/573 [=====>........................] - ETA: 0s - loss: 0.6509 - categorical_crossentropy: 0.6509 - acc: 0.73"
     ]
    }
   ],
   "source": [
    "#seeds = [random.randint(1, 20000) for _ in range(1)]\n",
    "\n",
    "results_G = []\n",
    "results_T = []\n",
    "\n",
    "x_indicies = list(range(0, 800))\n",
    "   \n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_indicies, y_train_class, train_size=0.85, random_state=7, stratify=y_train_class)\n",
    "\n",
    "# Select the Samples that have text\n",
    "X_tra_text_idx = [i for i in X_tra if i in train_text_idx]     \n",
    "X_val_text_idx = [i for i in X_val if i in train_text_idx]\n",
    "\n",
    "X_tra_text_idx_map = [train_text_index_map[i] for i in X_tra_text_idx]\n",
    "X_val_text_idx_map = [train_text_index_map[i] for i in X_val_text_idx]\n",
    "\n",
    "print(len(X_tra_text_idx_map), len(X_val_text_idx_map))\n",
    "#Text Samples\n",
    "# y_tra_T = np.take(y_train_dum, X_tra_text_idx, axis=0)\n",
    "# y_val_T = np.take(y_train_dum, X_val_text_idx, axis=0)\n",
    "\n",
    "# X_tra_T = np.take(train_features_svd, X_tra_text_idx_map, axis=0)\n",
    "# X_val_T = np.take(train_features_svd, X_val_text_idx_map, axis=0)\n",
    "\n",
    "y_tra_T = np.take(y_train_dum, X_tra_text_idx, axis=0)\n",
    "y_val_T = np.take(y_train_dum, X_val_text_idx, axis=0) #X_val[70:]\n",
    "\n",
    "X_tra_T = train_features.tocsr()[X_tra_text_idx_map,:]\n",
    "X_val_T = train_features.tocsr()[X_val_text_idx_map,:] #[X_val[70:],:]\n",
    "\n",
    "\n",
    "#Graph Samples\n",
    "X_tra_G = np.take(X_train_G, X_tra, axis=0)\n",
    "X_val_G = np.take(X_train_G, X_val, axis=0)\n",
    "\n",
    "y_tra_G = np.take(y_train_class ,X_tra, axis=0)\n",
    "y_val_G = np.take(y_train_class ,X_val, axis=0)\n",
    "\n",
    "y_val_G_l = np.take(y_train_dum, X_val, axis=0)\n",
    "\n",
    "\n",
    "model_MLP = MLP_model(\n",
    "optimizer=optimizer,\n",
    "input_size=X_tra_T.shape[1],\n",
    "dropout_rate = 0.8,\n",
    "hidden_layers=3,\n",
    "units=1100,\n",
    "funnel=True\n",
    ")\n",
    "\n",
    "model_MLP.fit(x=X_tra_T,\n",
    "              y=y_tra_T,          \n",
    "              validation_data=(X_val_T, y_val_T), \n",
    "              epochs=200,\n",
    "              verbose=1,\n",
    "              batch_size=batch_size,\n",
    "              callbacks = callbacks_list\n",
    "         )    \n",
    "\n",
    "svc = SVC(probability=True, C=1.45, kernel='rbf', gamma=0.035, random_state=2) #C=1.3, gamma=0.4\n",
    "svc.fit(X_tra_G, y_tra_G)\n",
    "\n",
    "\n",
    "y_pred_val_G = svc.predict_proba(X_val_G)    \n",
    "y_pred_val_T = model_MLP.predict(X_val_T)\n",
    "\n",
    "results_G.append(log_loss(y_val_G_l, y_pred_val_G))\n",
    "results_T.append(log_loss(y_val_T, y_pred_val_T))\n",
    "\n",
    "\n",
    "    \n",
    "print('Mean ll')\n",
    "print(mean(results_G))\n",
    "print(mean(results_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     K.clear_session()\n",
    "del model_MLP\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e9e340b0f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_classes = {0:'athlitismos', 1:'diaskedasi-psyxagogia', 2:'eidiseis-mme',\n",
    "       3:'katastimata-agores', 4:'pliroforiki-diadiktyo'}\n",
    "results = [test_classes[i] for i in np.argmax(y_pred_train_G,axis=1)]\n",
    "cm = confusion_matrix(y_tra,results)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_G = svc.predict_proba(X_test_G)\n",
    "y_pred_test_T = model_MLP.predict(test_features.tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 123 t: 87 g: 36\n"
     ]
    }
   ],
   "source": [
    "y_test_final = ensemble_predictions(y_pred_test_G, y_pred_test_T, y_pred_test_WL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(y_pred_G, y_pred_T, y_pred_WL,no_text_idx=test_no_text_idx,text_idx=test_text_idx,combine_method='avg_mix' ):\n",
    "    if combine_method=='avg_mix':\n",
    "        t=0\n",
    "        g=0\n",
    "        c=0\n",
    "        y_final_pred = np.zeros((200,5))\n",
    "        for idx in range(len(y_pred_G)):\n",
    "            if idx in no_text_idx:\n",
    "                y_final_pred[idx] = y_pred_G[idx]\n",
    "            else:\n",
    "                graph = y_pred_G[idx]\n",
    "                index_text = text_idx.index(idx)\n",
    "                text = y_pred_T[index_text]\n",
    "                grakel = y_pred_WL[idx]\n",
    "                if np.argmax(graph)==np.argmax(text):\n",
    "                    if max(graph)>max(text):\n",
    "                        y_final_pred[idx] = graph\n",
    "                        g+=1\n",
    "                    else:\n",
    "                        y_final_pred[idx] = text\n",
    "                        t+=1\n",
    "                else:\n",
    "                    y_final_pred[idx] = 0.4*graph + 0.3*text + 0.3*grakel\n",
    "\n",
    "                if np.argmax(grakel)==np.argmax(y_pred_T[index_text]):\n",
    "                    c+=1\n",
    "    print('c:',c,'t:',t,'g:',g)\n",
    "    return y_final_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {'athlitismos':0, 'diaskedasi-psyxagogia':1, 'eidiseis-mme':2, 'katastimata-agores':3, 'pliroforiki-diadiktyo':4}\n",
    "# Write predictions to a file\n",
    "with open('sample_submission_last_2.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = list(class_labels.keys())\n",
    "    lst.insert(0, \"Host\")\n",
    "    writer.writerow(lst)\n",
    "    for i,test_host in enumerate(test_hosts):\n",
    "        lst = y_test_final[i,:].tolist()\n",
    "        lst.insert(0, test_host)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from gensim.models import Word2Vec\n",
    "sys.path.insert(0,'C:/Users/User/Desktop/DS_challenge/')\n",
    "from GraphEmbedding.ge import DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "serialized_docs_test_WL_results_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/train/'\n",
    "filenames = os.listdir(serialized_docs_test_WL_results_dir)\n",
    "\n",
    "train_results = []\n",
    "for file in filenames:\n",
    "    with open(serialized_docs_test_WL_results_dir + file, 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        train_results.append(doc)              \n",
    "\n",
    "serialized_docs_test_WL_results_dir = 'C:/Users/User/Desktop/DS_challenge/data/serialized_objects/WL_results/test/'\n",
    "filenames = os.listdir(serialized_docs_test_WL_results_dir)\n",
    "\n",
    "test_results = []\n",
    "for file in filenames:\n",
    "    with open(serialized_docs_test_WL_results_dir + file, 'rb') as file:\n",
    "        doc = pickle.load(file)\n",
    "        test_results.append(doc)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = []\n",
    "y_train_class = []\n",
    "y_train_dum = []\n",
    "train_probs = []\n",
    "for doc in train_results:\n",
    "    x_train.append(doc['cat_sum'])\n",
    "    train_probs.append(doc['cat_prob'])\n",
    "    y_train_class.append(np.where(doc['y']==1)[0][0])\n",
    "    y_train_dum.append(doc['y'])\n",
    "    \n",
    "x_test = np.zeros((200,5))\n",
    "x_test_prob = np.zeros((200,5))\n",
    "for doc in test_results:\n",
    "    x_test[doc['doc_id']] = doc['cat_sum']\n",
    "    x_test_prob[doc['doc_id']] = doc['cat_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WL res:\n",
      "0.19232843395578017\n",
      "0.18366625487638547\n"
     ]
    }
   ],
   "source": [
    "x_indicies = list(range(0, 675))\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_indicies, y_train_class, train_size=0.90, random_state=3, stratify=y_train_class)\n",
    "\n",
    "X_tra_WL = np.take(x_train, X_tra, axis=0)\n",
    "X_val_WL = np.take(x_train, X_val, axis=0)\n",
    "\n",
    "y_tra_WL = np.take(y_train_dum ,X_tra, axis=0)\n",
    "y_val_WL = np.take(y_train_dum ,X_val, axis=0)\n",
    "\n",
    "svc = SVC(probability=True, C=1, kernel='rbf', gamma=0.0038, random_state=2\n",
    "         )\n",
    "svc.fit(X_tra_WL, y_tra)\n",
    "\n",
    "y_pred_train_WL = svc.predict_proba(X_tra_WL)\n",
    "y_pred_val_WL = svc.predict_proba(X_val_WL)\n",
    "\n",
    "print('WL res:')\n",
    "print(log_loss(y_tra_WL, y_pred_train_WL))\n",
    "print(log_loss(y_val_WL, y_pred_val_WL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_WL = svc.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
